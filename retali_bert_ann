from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
from collections import OrderedDict
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import japanize_matplotlib
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
import lightgbm as lgb
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA


# ---- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆä¸­é–“å±¤ã‚’è¿”ã™è¨­å®šï¼‰ ----
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased", output_hidden_states=True)

def bert_features(text, layer=8):
    # ãƒ†ã‚­ã‚¹ãƒˆ â†’ Tensor
    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)

    with torch.no_grad():
        outputs = model(**tokens)

    hidden_states = outputs.hidden_states  # å…¨å±¤ (13å€‹)

    # ---- ä½¿ã„ãŸã„å±¤ã‚’é¸æŠ ----
    h = hidden_states[layer]  # shape: (1, seq_len, 768)

    # ---- CLSãƒ™ã‚¯ãƒˆãƒ«ã‚’ç‰¹å¾´é‡ã«ã™ã‚‹ ----
    cls_vec = h[:, 0, :].squeeze().numpy()  # shape: (768,)

    return cls_vec


def bert_features_batch(text_list, layer=8, batch_size=16):

    all_vecs = []

    for i in range(0, len(text_list), batch_size):
        batch_texts = text_list[i:i+batch_size]

        # ã¾ã¨ã‚ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆpadding ã‚ã‚Šï¼‰
        tokens = tokenizer(batch_texts,
                           return_tensors='pt',
                           truncation=True,
                           padding=True,
                           max_length=128)

        with torch.no_grad():
            outputs = model(**tokens)

        hidden_states = outputs.hidden_states
        h = hidden_states[layer]       # shape: (B, seq_len, 768)
        mask = tokens["attention_mask"].unsqueeze(-1)

        # mean pooling
        h_masked = h * mask
        sum_vec = h_masked.sum(dim=1)
        len_tokens = mask.sum(dim=1)
        mean_vec = sum_vec / len_tokens  # shape: (B, 768)

        # numpy ã«å¤‰æ›
        all_vecs.append(mean_vec.numpy())

    # å…¨ãƒãƒƒãƒã‚’çµåˆ
    return np.vstack(all_vecs)


# --- å‰å‡¦ç†ã‚’é–¢æ•°åŒ– ---
def preprocess(retail_df, weather_df):
    # æ—¥ä»˜å‡¦ç†
    weather_df['æ—¥ä»˜'] = pd.to_datetime(weather_df['å¹´æœˆæ—¥.1'], errors='coerce')
    weather_df = weather_df.loc[:, ~weather_df.columns.str.contains(r'\.\d+$')]
    weather_df = weather_df.drop(columns=['æœ€æ·±ç©é›ª(cm)', 'å¹³å‡é›²é‡(10åˆ†æ¯”)', 'å¹´æœˆæ—¥'], errors='ignore')
    weather_df = weather_df.drop(index=[0, 1]).reset_index(drop=True)

    retail_df['æ—¥ä»˜'] = pd.to_datetime(retail_df['day'])
    retail_df = retail_df.drop(columns=['day'])

    merged = pd.merge(retail_df, weather_df, on='æ—¥ä»˜', how='left')

    # åºƒå‘Šçµ±åˆã¨ãƒã‚¤ãƒŠãƒªåŒ–
    ad_cols = ['SNS', 'å£²å ´æ–½ç­–', 'TVæ”¾æ˜ ', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³']
    for col in ad_cols:
        if col not in merged.columns:
            merged[col] = 0
    merged[ad_cols] = merged[ad_cols].fillna(0)
    for col in ad_cols:
        merged[col + '_æœ‰ç„¡'] = (merged[col] != 0).astype(int)
    merged['åºƒå‘Š_æœ‰ç„¡'] = merged[[c + '_æœ‰ç„¡' for c in ad_cols]].max(axis=1)
    merged = merged.drop(columns = ad_cols + [col + '_æœ‰ç„¡' for col in ad_cols])

    # ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆå•†å“ã”ã¨ï¼‰
    for lag in [1, 2, 3]:
        merged[f'åºƒå‘Š_æœ‰ç„¡_lag{lag}'] = merged.groupby('name')['åºƒå‘Š_æœ‰ç„¡'].shift(lag).fillna(0).astype(int)

    # ç§»å‹•å¹³å‡ï¼ˆ1ã¤å‰ã‹ã‚‰ã®3ç‚¹ç§»å‹•å¹³å‡ï¼‰
    merged['å£²ä¸Šæ•°_rollmean_prev3'] = (
        merged.groupby('name')['å£²ä¸Šæ•°']
              .apply(lambda s: s.shift(1).rolling(window=3, min_periods=1).mean())
              .reset_index(level=0, drop=True)
    ).fillna(0)

    # ç‰¹å®šã®æ¬ æå‡¦ç†ï¼ˆå…ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ„å›³ã‚’è¸è¥²ï¼‰
    merged = merged[merged.index != 3]  # å…ƒã‚³ãƒ¼ãƒ‰ã§é™¤å¤–ã—ã¦ã„ãŸè¡Œ
    beef_mask = (merged['name'] == 'ç„™ç…ç‰›è‚‰') & (merged.index >= 16) & (merged.index <= 94)
    for col in ['å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±']:
        if col in merged.columns:
            mean_value = merged.loc[beef_mask, col].mean()
            merged.loc[beef_mask, col] = merged.loc[beef_mask, col].fillna(mean_value)

    # å®¢æ•°ã®æ¬ æåŸ‹ã‚ï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®å€¤ã‚’å‚ç…§ï¼‰
    if 'å®¢æ•°' in merged.columns:
        try:
            costomer_count_250928 = merged.loc[merged['æ—¥ä»˜'] == pd.to_datetime('2025-09-28'), 'å®¢æ•°'].values[0]
            merged['å®¢æ•°'] = merged['å®¢æ•°'].fillna(costomer_count_250928)
        except Exception:
            merged['å®¢æ•°'] = merged['å®¢æ•°'].fillna(merged['å®¢æ•°'].mean())

    # æ•°å€¤åˆ—ã®å¹³å‡è£œå®Œ
    num_cols = ['price', 'å®¢æ•°', 'å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±',
                'å¹³å‡æ°—æ¸©(â„ƒ)', 'é™æ°´é‡ã®åˆè¨ˆ(mm)', 'æ—¥ç…§æ™‚é–“(æ™‚é–“)', 'å¹³å‡é¢¨é€Ÿ(m/s)', 'å¹³å‡è’¸æ°—åœ§(hPa)']
    num_cols = [c for c in num_cols if c in merged.columns]
    num_imputer = SimpleImputer(strategy='mean')
    merged[num_cols] = num_imputer.fit_transform(merged[num_cols])

    # IQRã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¤–ã‚Œå€¤å‡¦ç†ï¼‰
    for col in num_cols:
        Q1 = merged[col].quantile(0.25)
        Q3 = merged[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        merged[col] = merged[col].clip(lower, upper)

    # æ™‚ç³»åˆ—ç‰¹å¾´é‡
    SEASON = {1: "WINTER",2: "WINTER",3: "SPRING",4: "SPRING",5: "SPRING",
              6: "SUMMER",7: "SUMMER",8: "SUMMER",9: "FALL",10: "FALL",11: "FALL",12: "WINTER"}
    merged['æœˆ'] = merged['æ—¥ä»˜'].dt.month
    merged['é€±'] = merged['æ—¥ä»˜'].dt.isocalendar().week
    merged['å¹´'] = merged['æ—¥ä»˜'].dt.year
    merged['yday'] = merged['æ—¥ä»˜'].dt.dayofyear
    merged['season'] = merged['æœˆ'].map(SEASON)

    merged['yday_sin'] = np.sin(2 * np.pi * merged['yday'] / 365)
    merged['yday_cos'] = np.cos(2 * np.pi * merged['yday'] / 365)

    merged = merged.drop(columns=['æœˆ', 'é€±', 'å¹´', 'yday'])

    # season ã®ãƒ€ãƒŸãƒ¼åŒ–
    merged = pd.get_dummies(merged, columns=['season'], drop_first=True, dtype=int)


    ## ğŸš€ ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ (Interaction Features) ã®ç”Ÿæˆ
    lag_ad = 'åºƒå‘Š_æœ‰ç„¡_lag3'
    # 1. å½“åº—åœ¨åº«æ‰‹æŒé€± ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é …
    merged['Interaction_åœ¨åº«æ‰‹æŒé€±_åºƒå‘Š'] = merged['å½“åº—åœ¨åº«æ‰‹æŒé€±'] * merged[lag_ad]
    # 2. åº—é ­åœ¨åº«æ•° ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é …
    merged['Interaction_åº—é ­åœ¨åº«æ•°_åºƒå‘Š'] = merged['åº—é ­åœ¨åº«æ•°'] * merged[lag_ad]
    # 3. ç´å…¥äºˆå®šæ•° ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é … (Optional: ç›¸é–¢ãŒé«˜ã‹ã£ãŸãŸã‚è¿½åŠ )
    merged['Interaction_ç´å…¥äºˆå®šæ•°_åºƒå‘Š'] = merged['ç´å…¥äºˆå®šæ•°'] * merged[lag_ad]
    merged['Interaction_å®¢æ•°_åºƒå‘Š'] = merged['å®¢æ•°'] * merged[lag_ad]
    merged['Interaction_price_åºƒå‘Š'] = merged['price'] * merged[lag_ad]

    # ä½¿ã„ã‚„ã™ã„é †ã«åˆ—ã‚’æ•´ãˆã‚‹ï¼ˆä»»æ„ï¼‰
    merged = merged.reset_index(drop=True)
    return merged


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: numpy -> torch tensor å¤‰æ› ---
def to_tensor_Xy(X_np, y_np):
    X_t = torch.from_numpy(X_np.astype(np.float32))
    y_t = torch.from_numpy(y_np.astype(np.float32)).reshape(-1, 1)
    return X_t, y_t


def training_loop_ann(n_epochs, optimizer, scheduler, model, loss_fn, X_train, X_test, y_train, y_test, patience=500,
                      min_delta=1e-5):
    train_losses = []
    val_losses = []

    # Early Stoppingç”¨ã®å¤‰æ•°ã‚’åˆæœŸåŒ–
    best_loss = float('inf')
    epochs_no_improve = 0

    for epoch in range(1, n_epochs + 1):
        # --- è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ— ---
        model.train()
        y_p_train = model(X_train)
        loss_train = loss_fn(y_p_train, y_train)

        optimizer.zero_grad()
        loss_train.backward()
        optimizer.step()
        scheduler.step()

        # --- æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ— ---
        model.eval()
        with torch.no_grad():
            y_p_test = model(X_test)
            loss_test = loss_fn(y_p_test, y_test)

        # çµæœã®è¨˜éŒ²
        train_losses.append(loss_train.item())
        current_val_loss = loss_test.item()
        val_losses.append(current_val_loss)

        # --- Early Stoppingåˆ¤å®š ---
        if current_val_loss < best_loss - min_delta:
            # æå¤±ãŒæ”¹å–„ã—ãŸå ´åˆ
            best_loss = current_val_loss
            epochs_no_improve = 0
            best_model_state = model.state_dict()
        else:
            # æ”¹å–„ã—ãªã‹ã£ãŸå ´åˆ
            epochs_no_improve += 1

        if epochs_no_improve >= patience:
            print(f"\nEarly Stopping triggered at Epoch {epoch}. Best Loss: {best_loss:.4f}")
            model.load_state_dict(best_model_state)
            break

    # Early Stoppingã§ä¸­æ–­ã•ã‚Œãªã‹ã£ãŸå ´åˆã€æœ€å¾Œã®çŠ¶æ…‹ãŒ 'best' ã¨ãªã‚‹
    if epochs_no_improve < patience:
        # æœ€å¾Œã«è¨˜éŒ²ã•ã‚ŒãŸæå¤±ãŒãƒ™ã‚¹ãƒˆã ã£ãŸã¨ã¿ãªã—ã€ãã®æ™‚ç‚¹ã®é‡ã¿ã‚’ä¿å­˜
        best_model_state = model.state_dict()
        model.load_state_dict(best_model_state)
        print(f"\nTraining finished (max epochs reached). Final Loss: {current_val_loss:.4f}")

    return train_losses, val_losses


def train_model_ann(X_train, X_val, y_train, y_val, input_size, title):
    model = nn.Sequential(OrderedDict([
        ('hidden1', nn.Linear(input_size, 300)),
        ('bn1', nn.BatchNorm1d(300)),
        ('act1', nn.ReLU()),
        ('drop1', nn.Dropout(0.4)),
        ('hidden2', nn.Linear(300, 200)),
        ('bn2', nn.BatchNorm1d(200)),
        ('act2', nn.ReLU()),
        ('drop2', nn.Dropout(0.4)),
        ('hidden3', nn.Linear(200, 100)),
        ('bn3', nn.BatchNorm1d(100)),
        ('act3', nn.ReLU()),
        ('drop3', nn.Dropout(0.4)),
        ('hidden4', nn.Linear(100, 50)),
        ('bn4', nn.BatchNorm1d(50)),
        ('act4', nn.ReLU()),
        ('drop4', nn.Dropout(0.2)),
        ('output', nn.Linear(50, 1))
    ]))

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)
    loss_fn = nn.MSELoss()

    train_losses, val_losses = training_loop_ann(
        n_epochs=8000,
        optimizer=optimizer,
        scheduler=scheduler,
        model=model,
        loss_fn=loss_fn,
        X_train=X_train,
        X_test=X_val,
        y_train=y_train,
        y_test=y_val,
        patience=1000,  # æå¤±ãŒæ”¹å–„ã—ãªã„ã®ã‚’å¾…ã¤ã‚¨ãƒãƒƒã‚¯æ•° (ä¾‹: 1000)
        min_delta=1e-5  # æ”¹å–„ã¨è¦‹ãªã™æœ€å°å¤‰åŒ–é‡
    )


    # ----- å­¦ç¿’æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ -----
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title(f'{title} - Training & Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.show()

    return model


def evaluate_model_ann(model, X_val, y_val, title, sales_max, sales_min):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val).squeeze().numpy()
        y_true = y_val.squeeze().numpy()


    y_true_orig = y_true * (sales_max - sales_min) + sales_min
    y_pred_orig = y_pred * (sales_max - sales_min) + sales_min
    mse = mean_squared_error(y_true_orig, y_pred_orig)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true_orig, y_pred_orig)
    r2 = r2_score(y_true_orig, y_pred_orig)
    ''' # è©•ä¾¡æŒ‡æ•°ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™

    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    '''

    # print(f"\n=== {title} ã®è©•ä¾¡çµæœ ===")
    # print(f"RMSEï¼ˆå¹³å‡äºŒä¹—èª¤å·®ã®å¹³æ–¹æ ¹ï¼‰: {rmse:.4f}")
    # print(f"MAEï¼ˆå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰: {mae:.4f}")
    # print(f"RÂ²ï¼ˆæ±ºå®šä¿‚æ•°ï¼‰: {r2:.4f}\n")



    # === 2. äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ ===
    plt.figure(figsize=(6, 6))
    plt.scatter(y_true, y_pred, alpha=0.6)
    min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='y=x')
    plt.title(f'{title}ï¼šäºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.legend()
    plt.grid(True)
    plt.show()

    return rmse, mae, r2
'''
# --- ANN ã‚’ TimeSeriesSplit ã§ CV å®Ÿè¡Œ ---
def run_ann_cv(X, y, tscv, verbose=True):
    rmse_list, mae_list, r2_list = [], [], []
    input_size = X.shape[1]

    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        X_train_np, X_test_np = X[train_idx], X[test_idx]
        y_train_np, y_test_np = y[train_idx], y[test_idx]

        # torch ãƒ†ãƒ³ã‚½ãƒ«åŒ–
        X_train_t, y_train_t = to_tensor_Xy(X_train_np, y_train_np)
        X_val_t, y_val_t = to_tensor_Xy(X_test_np, y_test_np)

        # å­¦ç¿’ï¼ˆtrain_model_ann ã‚’åˆ©ç”¨ï¼‰
        title = f"ANN Fold {fold}"
        model_ann = train_model_ann(X_train_t, X_val_t, y_train_t, y_val_t, input_size=input_size, title=title)

        # è©•ä¾¡ï¼ˆy_scaler ã® min/max ã‚’æ¸¡ã™ï¼‰
        sales_max = y_scaler.data_max_[0]
        sales_min = y_scaler.data_min_[0]
        rmse, mae, r2 = evaluate_model_ann(model_ann, X_val_t, y_val_t, title, sales_max, sales_min)
        rmse_list.append(rmse); mae_list.append(mae); r2_list.append(r2)

        if verbose:
            print(f"[ANN] Fold {fold} -> RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}")

    print("\n=== ANN CV çµæœ ===")
    print(f"å¹³å‡ RMSE: {np.mean(rmse_list):.4f} (std {np.std(rmse_list):.4f})")
    print(f"å¹³å‡ MAE : {np.mean(mae_list):.4f}")
    print(f"å¹³å‡ R2  : {np.mean(r2_list):.4f}")
    return rmse_list, mae_list, r2_list
'''




def train_lgb_cv(X, y_raw, n_splits=10):
    tscv = TimeSeriesSplit(n_splits=n_splits)
    rmses = []
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        X_train, X_val = X[train_idx], X[val_idx]
        y_val_raw = y_raw[val_idx]  # è©•ä¾¡ç”¨ã«ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

        y_train = np.log1p(y_raw[train_idx])
        y_val = np.log1p(y_val_raw)

        scaler = StandardScaler()
        X_train_s = scaler.fit_transform(X_train)
        X_val_s = scaler.transform(X_val)

        train_data = lgb.Dataset(X_train_s, label=y_train.ravel())
        val_data = lgb.Dataset(X_val_s, label=y_val.ravel(), reference=train_data)

        params = {
            "objective": "regression",
            "metric": "rmse",
            "learning_rate": 0.05,
            "num_leaves": 31,
            "min_data_in_leaf":50,
            "feature_fraction":0.8,
            "bagging_fraction":0.8,
            "bagging_freq":5,
            "verbosity": -1
        }

        evals_result = {}
        bst = lgb.train(params, train_data, num_boost_round=5000,
                        callbacks=[lgb.record_evaluation(evals_result),
                                   lgb.early_stopping(stopping_rounds=100, verbose=False)],
                        valid_sets=[val_data])

        y_pred_log = bst.predict(X_val_s, num_iteration=bst.best_iteration)
        y_pred = np.expm1(y_pred_log)  # inverse of log1p
        mse = mean_squared_error(y_val_raw, y_pred)
        rmse = np.sqrt(mse)
        '''if fold == n_splits - 1:
            plt.figure(figsize=(10, 5))

            # 'valid_0' (æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿) ã® 'rmse' ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            metric = 'rmse'
            lgb.plot_metric(evals_result, metric=metric, ax=plt.gca())

            plt.title(f'LightGBM Validation RMSE during Boosting (Fold {fold + 1})')
            plt.xlabel('Boosting Rounds')
            plt.ylabel(f'Validation {metric.upper()}')
            plt.grid(True)
            plt.show()'''

        print(f"Fold {fold + 1} RMSE: {rmse:.4f}")  # Fold ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¡¨ç¤ºã‚’ä¿®æ­£
        rmses.append(rmse)

    print("CV mean RMSE:", np.mean(rmses))
    return rmses





# ===== PIã‚’åˆ©ç”¨ã—ã¦ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å¯è¦–åŒ– =====
# --- PFI ã®ãŸã‚ã®é–¢æ•° (scikit-learn ã® permutation_importance ã‚’åˆ©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ãŒã€ä»Šå›ã¯æ‰‹å‹•ã§å®Ÿè£…) ---
def calculate_permutation_importance(model, X_val_t, y_val_t, feature_names, sales_max, sales_min,
                                     loss_fn=mean_squared_error):
    model.eval()

    # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾— (ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—)
    with torch.no_grad():
        y_pred_base = model(X_val_t).squeeze().numpy()
        y_true = y_val_t.squeeze().numpy()

    # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã™
    y_true_orig = y_true * (sales_max - sales_min) + sales_min
    y_pred_base_orig = y_pred_base * (sales_max - sales_min) + sales_min
    base_score = loss_fn(y_true_orig, y_pred_base_orig)

    importance = {}
    X_val_np = X_val_t.numpy()

    # 2. å„ç‰¹å¾´é‡ã‚’é †ç•ªã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    for i, name in enumerate(feature_names):
        # å…ƒã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
        X_shuffled_np = X_val_np.copy()

        # iç•ªç›®ã®ç‰¹å¾´é‡ã ã‘ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        X_shuffled_np[:, i] = np.random.permutation(X_shuffled_np[:, i])

        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã§äºˆæ¸¬
        X_shuffled_t = torch.from_numpy(X_shuffled_np.astype(np.float32))

        with torch.no_grad():
            y_pred_shuffled = model(X_shuffled_t).squeeze().numpy()

        # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã—ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        y_pred_shuffled_orig = y_pred_shuffled * (sales_max - sales_min) + sales_min
        shuffled_score = loss_fn(y_true_orig, y_pred_shuffled_orig)

        # é‡è¦åº¦: (ã‚·ãƒ£ãƒƒãƒ•ãƒ«å¾Œã®ã‚¹ã‚³ã‚¢ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢) / ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢
        # RMSEãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—
        importance[name] = (np.sqrt(shuffled_score) - np.sqrt(base_score)) / np.sqrt(base_score)

    return importance, base_score






# ===== uplift åˆ†æï¼ˆåºƒå‘Šã®æœ‰ç„¡ã§æ¯”è¼ƒï¼‰ =====
def uplift_by_ad(df, target_col="å£²ä¸Šæ•°", ad_lag_col="åºƒå‘Š_æœ‰ç„¡"):
    """
    åºƒå‘Šæœ‰ï¼ˆTreatmentï¼‰/ ç„¡ï¼ˆControlï¼‰ã§ uplift ã‚’ç®—å‡ºã—ã€çµæœã‚’æ•´å½¢ã—ã¦è¡¨ç¤ºã™ã‚‹ã€‚

    Args:
        df: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ ('name', ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—, åºƒå‘Šæœ‰ç„¡ãƒ©ã‚°åˆ—ã‚’å«ã‚€)
        target_col: uplift ã‚’è¨ˆç®—ã™ã‚‹ç›®çš„å¤‰æ•°ï¼ˆä¾‹: 'å£²ä¸Šæ•°', 'å£²ä¸Šé«˜'ï¼‰
        ad_lag_col: åºƒå‘Šã®æœ‰ç„¡ã‚’ç¤ºã™ãƒã‚¤ãƒŠãƒªåˆ—ï¼ˆä¾‹: 'åºƒå‘Š_æœ‰ç„¡_lag3'ï¼‰

    Returns:
        pd.Series: å•†å“åˆ¥ uplift å€¤
    """
    print(f"\n ====== {target_col} ã¸ã®åºƒå‘ŠåŠ¹æœ (Uplift) åˆ†æ ====== ")

    # 1. å…¨ä½“ uplift ã®è¨ˆç®—
    treat = df[df[ad_lag_col] == 1][target_col].mean()
    control = df[df[ad_lag_col] == 0][target_col].mean()
    uplift = treat - control

    print("\n---  å…¨ä½“ Uplift (å¹³å‡å£²ä¸Šæ•°ã®å¢—åˆ†) ---")
    print(f"   åºƒå‘Šã‚ã‚Š (Treatment) å¹³å‡: {treat:,.2f}")
    print(f"   åºƒå‘Šãªã— (Control)   å¹³å‡: {control:,.2f}")
    print(f"   **Upliftï¼ˆåºƒå‘ŠåŠ¹æœï¼‰**: {uplift:,.2f} ï¼ˆ{target_col} ã®å¢—åˆ†ï¼‰")

    # 2. å•†å“åˆ¥ uplift ã®è¨ˆç®—
    # name ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€(åºƒå‘Šã‚ã‚Šå¹³å‡ - åºƒå‘Šãªã—å¹³å‡) ã‚’è¨ˆç®—
    uplift_result = (
        df.groupby("name")
        .apply(lambda g: g[g[ad_lag_col] == 1][target_col].mean()
                         - g[g[ad_lag_col] == 0][target_col].mean())
        .sort_values(ascending=False)
        .rename("Uplift")
    )

    print("\n---  å•†å“åˆ¥ Uplift Top 5 ---")
    print(uplift_result.head(5).to_string(float_format='{:,.2f}'.format))

    print("\n--- ğŸ“‰ å•†å“åˆ¥ Uplift Bottom 5 (åŠ¹æœãŒä½ã„ãƒ»ãƒã‚¤ãƒŠã‚¹) ---")
    print(uplift_result.tail(5).to_string(float_format='{:,.2f}'.format))

    return uplift_result







pd.set_option('display.max_columns', None)  # ã™ã¹ã¦ã®åˆ—ã‚’è¡¨ç¤º
pd.set_option('display.width', None)        # æ¨ªå¹…åˆ¶é™ã‚’è§£é™¤


retail = pd.read_excel('C:/Users/23X4002/Desktop/3å¹´ç”Ÿç§‹/PBL/éœ€è¦äºˆæ¸¬(åºƒå‘Š)/å°å£²ãƒ‡ãƒ¼ã‚¿.xlsx')
weather= pd.read_csv('C:/Users/23X4002/Desktop/3å¹´ç”Ÿç§‹/PBL/éœ€è¦äºˆæ¸¬(åºƒå‘Š)/æ°—è±¡ãƒ‡ãƒ¼ã‚¿03.csv',
                     encoding='cp932', skiprows=3)


data_mm = preprocess(retail, weather)





# ===== nameã®ç‰¹å¾´é‡ä½œæˆ =====
data_api = data_mm.copy()
# data_api["bert_vec"] = data_api["name"].apply(bert_features)
data_api["bert_vec"] = list(bert_features_batch(data_api["name"].tolist(), layer=8))






# ===== æ­£è¦åŒ– =====
y_scaler = MinMaxScaler()
y_scaler.fit(data_api[['å£²ä¸Šæ•°']])
# # --- ãƒ‡ãƒãƒƒã‚°æƒ…å ± (data_api ã‚’ä½¿ç”¨) ---
# print("\n--- ãƒ‡ãƒ¼ã‚¿ (data_api) ã®ãƒ˜ãƒƒãƒ€ãƒ¼ ---")
# print(data_api.head(3))
# print("\n--- ãƒ‡ãƒ¼ã‚¿ (data_api) ã®æ¦‚è¦ ---")
# print(data_api.describe())
# print("\n--- ãƒ‡ãƒ¼ã‚¿ (data_api) ã®æƒ…å ± ---")
# print(data_api.info())





#===== bertç‰¹å¾´é‡ã®æ¬¡å…ƒå‰Šé™¤ =====
X_bert = np.vstack(data_api["bert_vec"].values)  # shape: (n_samples, 768)
scaler_pca = StandardScaler()
X_bert_scaled = scaler_pca.fit_transform(X_bert)

# PCAã®å®Ÿè¡Œï¼ˆå…¨æ¬¡å…ƒï¼‰
pca = PCA(n_components=None) # n_components=None ã§å…¨æ¬¡å…ƒã‚’è¨ˆç®—
pca.fit(X_bert_scaled)

# ç´¯ç©å¯„ä¸ç‡ã®è¨ˆç®—ã¨ãƒ—ãƒ­ãƒƒãƒˆ
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

'''plt.figure(figsize=(10, 5))
plt.plot(cumulative_variance)
plt.title('ç´¯ç©å¯„ä¸ç‡ã®æ¨ç§»')
plt.xlabel('ä¸»æˆåˆ†ã®æ•°')
plt.ylabel('ç´¯ç©å¯„ä¸ç‡')
plt.grid(True)
# ä¾‹: å¯„ä¸ç‡95%ã®ãƒ©ã‚¤ãƒ³ã‚’å¼•ã
plt.axhline(0.95, color='red', linestyle='--', label='95%')
plt.legend()
plt.show()'''

# 95%ã®å¯„ä¸ç‡ã‚’é”æˆã™ã‚‹ä¸»æˆåˆ†æ•°kã‚’æ±ºå®š
k = np.where(cumulative_variance >= 0.95)[0][0] + 1
print(f"ç´¯ç©å¯„ä¸ç‡ 95% ã‚’é”æˆã™ã‚‹ä¸»æˆåˆ†ã®æ•° k: {k}")

# æ±ºå®šã—ãŸ k æ¬¡å…ƒã«å‰Šæ¸›
pca_final = PCA(n_components=k)
X_bert_pca = pca_final.fit_transform(X_bert_scaled)

print(f"å‰Šæ¸›å¾Œã®BERTç‰¹å¾´é‡ã®å½¢çŠ¶: {X_bert_pca.shape}")





# ===== ç‰¹å¾´é‡ã®é¸æŠ =====
# select_cols = ['å®¢æ•°', 'å£²ä¸Šæ•°_rollmean_prev3', 'åº—é ­åœ¨åº«æ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±', 'name_ã€2è¾›ã€‘ç´ æã‚’ç”Ÿã‹ã—ãŸã‚«ãƒ¬ãƒ¼ 6ä»£ç›®ãƒã‚¿ãƒ¼ãƒã‚­ãƒ³',
#                'name_ã€3è¾›ã€‘ç„™ç…ã‚¹ãƒ‘ã‚¤ã‚¹ã®ã”ã‚ã‚Šç‰›è‚‰ã‚«ãƒ¬ãƒ¼', 'æ—¥ç…§æ™‚é–“(æ™‚é–“)', 'name_ã€4è¾›ã€‘ç´ æã‚’ç”Ÿã‹ã—ãŸç‰›ã°ã‚‰è‚‰ã®å¤§ç››ã‚«ãƒ¬ãƒ¼', 'é™æ°´é‡ã®åˆè¨ˆ(mm)', 'name_ã€5è¾›ã€‘ç´ æã‚’ç”Ÿã‹ã—ãŸã‚«ãƒ¬ãƒ¼ ã‚°ãƒªãƒ¼ãƒ³',
#                'yday_sin', 'å¹³å‡æ°—æ¸©(â„ƒ)', 'å¹³å‡é¢¨é€Ÿ(m/s)', 'ç´å…¥äºˆå®šæ•°', 'yday_cos',
#                'åºƒå‘Š_æœ‰ç„¡_lag2']
exclude_cols = ['name', 'å£²ä¸Šæ•°', 'å£²ä¸Šé«˜','æ—¥ä»˜',# 'åºƒå‘Š_æœ‰ç„¡','åºƒå‘Š_æœ‰ç„¡_lag1', 'åºƒå‘Š_æœ‰ç„¡_lag3',
                'Interaction_ç´å…¥äºˆå®šæ•°_åºƒå‘Š', 'Interaction_å®¢æ•°_åºƒå‘Š', 'Interaction_price_åºƒå‘Š', 'Interaction_åº—é ­åœ¨åº«æ•°_åºƒå‘Š', 'Interaction_åœ¨åº«æ‰‹æŒé€±_åºƒå‘Š',
                "bert_vec"]
target = 'å£²ä¸Šæ•°'
X_num_df = data_api.drop(columns=exclude_cols, errors='ignore')
X_num = X_num_df.values
# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
y_unscaled = data_api[target].values.reshape(-1, 1) # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
y = y_scaler.transform(y_unscaled) # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

X = np.hstack([X_num, X_bert_pca]) # bertç‰¹å¾´é‡ã‚’kæ¬¡å…ƒã«åœ§ç¸®
# X = np.hstack([X_num, X_bert_scaled])
print(f"\næœ€çµ‚çš„ãªç‰¹å¾´é‡ X ã®å½¢çŠ¶ (PCAé©ç”¨å¾Œ): {X.shape}, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ y ã®å½¢çŠ¶: {y.shape}")
# X_num_df ã®åˆ—åã‚’ç¢ºèª (ãƒ‡ãƒãƒƒã‚°ç”¨)
# print("X_num ã®åˆ—:")
# print(X_num_df.columns.tolist())






# ===== ANNã®å®Ÿè£… =====
tscv = TimeSeriesSplit(n_splits=30)
print("Start ANN CV ...")
lgbm_rmses_list = train_lgb_cv(X, y_unscaled)







# --- PFI å®Ÿè¡Œéƒ¨åˆ† ---
# æœ€çµ‚çš„ãªç‰¹å¾´é‡ã®åˆ—åãƒªã‚¹ãƒˆã‚’ä½œæˆ
num_feature_names = X_num_df.columns.tolist()
# BERTãƒ™ã‚¯ãƒˆãƒ«ã¯768æ¬¡å…ƒã‚ã‚‹ãŸã‚ã€å€‹åˆ¥ã®åˆ—åã‚’ä»˜ã‘ã‚‹
bert_feature_names = [f"bert_vec_{i}" for i in range(X_bert_pca.shape[1])]
all_feature_names = num_feature_names + bert_feature_names

# CV ã®æœ€çµ‚ Fold ã®ãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’ãƒ»è©•ä¾¡
final_train_idx, final_test_idx = list(tscv.split(X))[-1]
X_train_np, X_test_np = X[final_train_idx], X[final_test_idx]
y_train_np, y_test_np = y[final_train_idx], y[final_test_idx]

X_train_t, y_train_t = to_tensor_Xy(X_train_np, y_train_np)
X_val_t, y_val_t = to_tensor_Xy(X_test_np, y_test_np)

print("\n--- Final Model Training for PFI ---")
final_model = train_model_ann(X_train_t, X_val_t, y_train_t, y_val_t,
                              input_size=X.shape[1], title="Final Model")

sales_max = y_scaler.data_max_[0]
sales_min = y_scaler.data_min_[0]

# PFI ã®è¨ˆç®—ï¼ˆã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ MSE ã‚’ä½¿ç”¨ã€è©•ä¾¡é–¢æ•°ã«åˆã‚ã›ã¦ RMSE ã‚’è¨ˆç®—ï¼‰
importance_results, base_mse = calculate_permutation_importance(
    final_model,
    X_val_t,
    y_val_t,
    all_feature_names,
    sales_max,
    sales_min,
    loss_fn=mean_squared_error
)

# çµæœã‚’ DataFrame ã«ã—ã¦ã‚½ãƒ¼ãƒˆ
importance_df = pd.DataFrame(
    list(importance_results.items()),
    columns=['Feature', 'Importance (RMSE increase %)']
).sort_values(by='Importance (RMSE increase %)', ascending=False)

print("\n=== Permutation Feature Importance Top 30 ===")
print(importance_df.head(30))

# --- å¯è¦–åŒ– ---
plt.figure(figsize=(10, 6))
top_n = 20
df_plot = importance_df.head(top_n).sort_values(by='Importance (RMSE increase %)', ascending=True)
plt.barh(df_plot['Feature'], df_plot['Importance (RMSE increase %)'])
plt.title(f'Top {top_n} Feature Importance (Permutation)')
plt.xlabel('RMSE Increase Ratio')
plt.ylabel('Feature')
plt.grid(axis='x', linestyle='--')
plt.tight_layout()
plt.show()





# ===== uplift modeling =====
uplift_result = uplift_by_ad(data_api)
# --- çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ---
print("\n=== å•†å“åˆ¥ Uplift ã®çµ±è¨ˆæƒ…å ± ===")
print(uplift_result.describe().to_string(float_format='{:,.2f}'.format))


ad_col = 'åºƒå‘Š_æœ‰ç„¡'

# --- Treatment/Control ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ ---
X_test_treat_np = X_test_np.copy()
X_test_ctrl_np = X_test_np.copy()

ad_idx = all_feature_names.index(ad_col)
print(f"Treatment Indicator: {ad_col} (Index: {ad_idx})")


def run_two_model_uplift_cv(X, y_raw, all_feature_names, n_splits=5):
    """
    Two-Model Approach ã‚’å®Ÿè¡Œã—ã€Upliftã®å¹³å‡RMSEã‚’è¨ˆç®—ã™ã‚‹ã€‚

    Args:
        X (np.ndarray): å…¨ç‰¹å¾´é‡é…åˆ—ã€‚
        y_raw (np.ndarray): ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•° (å£²ä¸Šæ•°)ã€‚
        all_feature_names (list): Xã®åˆ—åãƒªã‚¹ãƒˆã€‚
        n_splits (int): TimeSeriesSplitã®åˆ†å‰²æ•°ã€‚

    Returns:
        tuple: (cv_rmses_mt, cv_rmses_mc, final_uplifts, final_y_pred_treat)
    """
    tscv = TimeSeriesSplit(n_splits=n_splits)
    cv_rmses_mt, cv_rmses_mc = [], []
    final_uplifts, final_y_pred_treat = None, None

    # T-Learnerã§ã¯åºƒå‘Šæœ‰ç„¡ã‚’ç‰¹å¾´é‡ã‹ã‚‰å‰Šé™¤ã—ãªã„
    X_df = pd.DataFrame(X, columns=all_feature_names)
    X_train_df = X_df.copy() # åˆæœŸåŒ–

    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        # ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
        X_train_np, X_val_np = X[train_idx], X[val_idx]
        y_train_raw, y_val_raw = y_raw[train_idx], y_raw[val_idx]

        # åºƒå‘Šæœ‰ç„¡ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ–ã‚»ãƒƒãƒˆåŒ–
        ad_train = X_train_np[:, ad_idx]
        ad_val = X_val_np[:, ad_idx] # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯ä¸¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ä½¿ç”¨

        # Treatment (T=1) ã¨ Control (T=0) ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        X_train_t = X_train_np[ad_train == 1]
        y_train_t = np.log1p(y_train_raw[ad_train == 1])

        X_train_c = X_train_np[ad_train == 0]
        y_train_c = np.log1p(y_train_raw[ad_train == 0])

        # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (èª¿æ•´å¯èƒ½)
        params = {
            "objective": "regression", "metric": "rmse", "learning_rate": 0.05,
            "num_leaves": 31, "min_data_in_leaf": 50, "verbosity": -1
        }

        # ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° (å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§å†ãƒ•ã‚£ãƒƒãƒˆ)
        scaler = StandardScaler()
        X_train_t_s = scaler.fit_transform(X_train_t)
        X_train_c_s = scaler.fit_transform(X_train_c) # Controlãƒ¢ãƒ‡ãƒ«ã¯Controlãƒ‡ãƒ¼ã‚¿ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_val_s = scaler.fit_transform(X_val_np) # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚‚ã“ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨

        # 1. Treatment Model (M_T) ã®å­¦ç¿’
        bst_t = lgb.LGBMRegressor(**params, n_estimators=5000)
        bst_t.fit(X_train_t_s, y_train_t.ravel())

        # 2. Control Model (M_C) ã®å­¦ç¿’
        bst_c = lgb.LGBMRegressor(**params, n_estimators=5000)
        bst_c.fit(X_train_c_s, y_train_c.ravel())

        # 3. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ã¨RMSEè¨ˆç®—
        # M_T ã®äºˆæ¸¬ (y_pred_t): T=1 ã®ãƒ‡ãƒ¼ã‚¿ã« M_T ã‚’é©ç”¨
        # M_C ã®äºˆæ¸¬ (y_pred_c): T=0 ã®ãƒ‡ãƒ¼ã‚¿ã« M_C ã‚’é©ç”¨
        y_pred_t_log = bst_t.predict(X_val_s)
        y_pred_c_log = bst_c.predict(X_val_s)

        y_pred_t = np.expm1(y_pred_t_log)
        y_pred_c = np.expm1(y_pred_c_log)

        # M_Tã®RMSE (T=1ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è©•ä¾¡)
        y_val_t_raw = y_val_raw[ad_val == 1]
        y_pred_t_val_subset = y_pred_t[ad_val == 1]
        rmse_t = np.sqrt(mean_squared_error(y_val_t_raw, y_pred_t_val_subset))

        # M_Cã®RMSE (T=0ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è©•ä¾¡)
        y_val_c_raw = y_val_raw[ad_val == 0]
        y_pred_c_val_subset = y_pred_c[ad_val == 0]
        rmse_c = np.sqrt(mean_squared_error(y_val_c_raw, y_pred_c_val_subset))

        cv_rmses_mt.append(rmse_t)
        cv_rmses_mc.append(rmse_c)

        print(f"Fold {fold + 1} - RMSE (M_T): {rmse_t:.4f}, RMSE (M_C): {rmse_c:.4f}")

        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®çµæœã‚’ä¿å­˜
        if fold == n_splits - 1:
            # Uplift = M_Tã®äºˆæ¸¬ - M_Cã®äºˆæ¸¬
            final_uplifts = y_pred_t - y_pred_c
            final_y_pred_treat = y_pred_t
            final_y_pred_control = y_pred_c
            final_X_val = X_val_np
            # åºƒå‘Šæœ‰ç„¡ã®ç‰¹å¾´é‡ã‚’ Uplift ã«ä½¿ç”¨ã™ã‚‹ãƒ©ã‚°ã«æˆ»ã™ (ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¤‡é›‘ãªãŸã‚ã“ã“ã§ã¯çœç•¥)
            # Uplift ã®åˆ†å¸ƒå¯è¦–åŒ–ã¯å¾Œã§è¡Œã„ã¾ã™

    print("\n=== Two-Model CV çµæœ ===")
    print(f"M_T (Treatment) ã®å¹³å‡ RMSE: {np.mean(cv_rmses_mt):.4f} (std {np.std(cv_rmses_mt):.4f})")
    print(f"M_C (Control) ã®å¹³å‡ RMSE: {np.mean(cv_rmses_mc):.4f} (std {np.std(cv_rmses_mc):.4f})")

    return final_uplifts, final_y_pred_treat, final_y_pred_control, final_X_val
'''

def run_two_model_uplift_cv_ann(X, y_raw, all_feature_names, y_scaler, n_splits=5):
    """
    Two-Model Approach (T-Learner) ã‚’ANNãƒ¢ãƒ‡ãƒ«ã§å®Ÿè¡Œã—ã€Upliftã‚’è¨ˆç®—ã™ã‚‹ã€‚

    Args:
        X (np.ndarray): å…¨ç‰¹å¾´é‡é…åˆ—ã€‚
        y_raw (np.ndarray): ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•° (å£²ä¸Šæ•°)ã€‚
        all_feature_names (list): Xã®åˆ—åãƒªã‚¹ãƒˆã€‚
        y_scaler (MinMaxScaler): y_raw ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ä½¿ç”¨ã—ãŸMinMaxScalerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        n_splits (int): TimeSeriesSplitã®åˆ†å‰²æ•°ã€‚

    Returns:
        tuple: (final_uplifts, final_y_pred_control)
    """
    tscv = TimeSeriesSplit(n_splits=n_splits)
    ad_col = 'åºƒå‘Š_æœ‰ç„¡_lag3'  # é©åˆ‡ãªåºƒå‘Šæœ‰ç„¡ã®ç‰¹å¾´é‡ã«å¤‰æ›´ã—ã¦ãã ã•ã„
    ad_idx = all_feature_names.index(ad_col)
    input_size = X.shape[1]

    final_uplifts, final_y_pred_treat, final_y_pred_control, final_X_val = None, None, None, None
    sales_max = y_scaler.data_max_[0]
    sales_min = y_scaler.data_min_[0]

    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"\n--- Fold {fold + 1} / {n_splits} ---")

        X_train_np, X_val_np = X[train_idx], X[val_idx]
        y_train_raw, y_val_raw = y_raw[train_idx], y_raw[val_idx]

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        y_train_scaled = y_scaler.transform(y_train_raw)
        y_val_scaled = y_scaler.transform(y_val_raw)

        ad_train = X_train_np[:, ad_idx]

        # ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ–ã‚»ãƒƒãƒˆåŒ–
        X_train_t_np = X_train_np[ad_train == 1]
        y_train_t_scaled = y_train_scaled[ad_train == 1]

        X_train_c_np = X_train_np[ad_train == 0]
        y_train_c_scaled = y_train_scaled[ad_train == 0]

        # PyTorch ãƒ†ãƒ³ã‚½ãƒ«åŒ–
        X_train_t, y_train_t = to_tensor_Xy(X_train_t_np, y_train_t_scaled)
        X_train_c, y_train_c = to_tensor_Xy(X_train_c_np, y_train_c_scaled)
        X_val_t, _ = to_tensor_Xy(X_val_np, y_val_scaled)  # y_val ã¯è©•ä¾¡ã«ä½¿ã‚ãªã„

        # 1. Treatment Model (M_T) ã®å­¦ç¿’
        print("Training Treatment Model (M_T)...")
        model_t = train_model_ann(X_train_t, X_val_t, y_train_t, X_val_t,
                                  input_size=input_size, title=f"M_T Fold {fold + 1}")  # X_val_t ã‚’æ¸¡ã—ã¦æ¤œè¨¼

        # 2. Control Model (M_C) ã®å­¦ç¿’
        print("Training Control Model (M_C)...")
        model_c = train_model_ann(X_train_c, X_val_t, y_train_c, X_val_t,
                                  input_size=input_size, title=f"M_C Fold {fold + 1}")  # X_val_t ã‚’æ¸¡ã—ã¦æ¤œè¨¼

        # 3. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬
        model_t.eval();
        model_c.eval()
        with torch.no_grad():
            y_pred_t_scaled = model_t(X_val_t).squeeze().numpy()
            y_pred_c_scaled = model_c(X_val_t).squeeze().numpy()

        # äºˆæ¸¬å€¤ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™ (é€†å¤‰æ›)
        y_pred_t = y_pred_t_scaled * (sales_max - sales_min) + sales_min
        y_pred_c = y_pred_c_scaled * (sales_max - sales_min) + sales_min

        # 4. Uplift ã®è¨ˆç®—
        uplifts = y_pred_t - y_pred_c

        print(f"Fold {fold + 1} - Predicted Uplift Mean: {np.mean(uplifts):.2f}")

        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®çµæœã‚’ä¿å­˜
        if fold == n_splits - 1:
            final_uplifts = uplifts
            final_y_pred_treat = y_pred_t
            final_y_pred_control = y_pred_c
            final_X_val = X_val_np

    return final_uplifts, final_y_pred_control, final_X_val


# --- å®Ÿè¡Œ ---
final_uplifts, final_y_pred_control, final_X_val = run_two_model_uplift_cv_ann(
    X, y_unscaled, all_feature_names, y_scaler, n_splits=5
)'''

# --- å®Ÿè¡Œ ---
final_uplifts, final_y_pred_treat, final_y_pred_control, final_X_val = run_two_model_uplift_cv(
    X, y_unscaled, all_feature_names, n_splits=5
)

# Uplift ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†é¡ã¨å¯è¦–åŒ–
def plot_uplift_quadrant(uplift_values, control_predictions):
    """
    Upliftã¨Controläºˆæ¸¬å€¤ã«åŸºã¥ãã€4è±¡é™æ•£å¸ƒå›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã€‚

    Args:
        uplift_values (np.ndarray): äºˆæ¸¬ã•ã‚ŒãŸ Uplift å€¤ (T - C)ã€‚
        control_predictions (np.ndarray): äºˆæ¸¬ã•ã‚ŒãŸ Control å£²ä¸Šå€¤ (M_C(X))ã€‚
    """
    plt.figure(figsize=(10, 8))

    # è±¡é™ã®é–¾å€¤ (ä¾‹: 0 or å¹³å‡)
    uplift_mean = np.mean(uplift_values)
    control_mean = np.mean(control_predictions)

    # ãƒ‡ãƒ¼ã‚¿ã®æ•£å¸ƒ
    plt.scatter(control_predictions, uplift_values, alpha=0.6, s=20)

    # è±¡é™åˆ†å‰²ç·š
    plt.axhline(0, color='red', linestyle='-', linewidth=0.8, label='Uplift = 0') # åºƒå‘ŠåŠ¹æœã®åˆ†å²ç‚¹
    plt.axvline(control_mean, color='gray', linestyle='--', linewidth=0.8, label=f'Control Mean ({control_mean:.1f})')

    # è±¡é™åã®è¿½åŠ 
    # å³ä¸Š: Sure Things (M_CãŒé«˜ã„ & Uplift > 0)
    plt.text(control_mean * 1.05, uplift_mean * 1.5, "Sure Things\n(é«˜å£²ä¸Š, åºƒå‘Šä¸è¦)", color='blue', fontsize=12, ha='left')
    # å·¦ä¸Š: Persuadables (M_CãŒä½ã„ & Uplift > 0)
    plt.text(control_mean * 0.95, uplift_mean * 1.5, "Persuadables\n(ä½å£²ä¸Š, åºƒå‘ŠåŠ¹ã)", color='green', fontsize=12, ha='right')
    # å·¦ä¸‹: Lost Causes (M_CãŒä½ã„ & Uplift < 0)
    plt.text(control_mean * 0.95, -uplift_mean * 0.5, "Lost Causes\n(ä½å£²ä¸Š, åºƒå‘ŠåŠ¹ã‹ãªã„)", color='orange', fontsize=12, ha='right')
    # å³ä¸‹: Treated Negatives (M_CãŒé«˜ã„ & Uplift < 0) - Uplift < 0 ã®ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯èª¿æ•´
    plt.text(control_mean * 1.05, -uplift_mean * 0.5, "Treated Negatives\n(é«˜å£²ä¸Š, åºƒå‘ŠãŒé€†åŠ¹æœ)", color='purple', fontsize=12, ha='left')


    plt.title("Uplift ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ4è±¡é™åˆ†æï¼‰")
    plt.xlabel(f"Predicted Sales without Ad (Control: $M_C(X)$)")
    plt.ylabel("Predicted Uplift ($M_T(X) - M_C(X)$)")
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.5)
    plt.show()

# --- å®Ÿè¡Œ ---
if final_uplifts is not None:
    # 4è±¡é™åˆ†æã‚’å®Ÿè¡Œ
    plot_uplift_quadrant(final_uplifts, final_y_pred_control)


# Upliftã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®å¯è¦–åŒ–
def plot_uplift_histogram(uplift_values):
    """
    äºˆæ¸¬ã•ã‚ŒãŸ Uplift å€¤ã®åˆ†å¸ƒã‚’ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã§ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã€‚

    Args:
        uplift_values (np.ndarray): äºˆæ¸¬ã•ã‚ŒãŸ Uplift å€¤ (T - C)ã€‚
    """
    plt.figure(figsize=(6, 4))

    # é©åˆ‡ã«ãƒ“ãƒ³ã®æ•°ã‚’èª¿æ•´
    plt.hist(uplift_values, bins=50)

    # ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³
    plt.axvline(0, color='k', linestyle="--")

    plt.title("Uplift ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ")
    plt.xlabel("Uplift ($M_T(X) - M_C(X)$)")
    plt.ylabel("ä»¶æ•°")
    plt.tight_layout()
    plt.show()

# --- å®Ÿè¡Œ ---
if 'final_uplifts' in locals() and final_uplifts is not None:
    plot_uplift_histogram(final_uplifts)
'''# ----- 1) æ•£å¸ƒå›³ï¼šäºˆæ¸¬ï¼ˆåºƒå‘Šã‚ã‚Šï¼‰ vs uplift -----
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_treatment, uplift, alpha=0.5)
plt.xlabel("Predicted Sales (Treatment = 1)")
plt.ylabel("Uplift (Treatment - Control)")
plt.title("Uplift Effect Visualization")
plt.show()

# ----- 2) ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼šuplift ã®åˆ†å¸ƒ -----
plt.figure(figsize=(6,4))
plt.hist(uplift, bins=30)
plt.axvline(0, color='k', linestyle="--")
plt.title("Uplift ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ")
plt.xlabel("Uplift")
plt.ylabel("ä»¶æ•°")
plt.tight_layout()
plt.show()'''





def analyze_bert_dimension(df: pd.DataFrame, dim_index: int, top_n: int = 10):
    """
    ç‰¹å®šã®BERTæ¬¡å…ƒã®å€¤ã«åŸºã¥ã„ã¦ã€å•†å“åï¼ˆnameï¼‰ã®Top Nã¨Bottom Nã‚’æŠ½å‡ºã—ã€è¡¨ç¤ºã™ã‚‹ã€‚

    Args:
        df (pd.DataFrame): 'name'åˆ—ã¨ 'bert_vec'åˆ—ã‚’å«ã‚€DataFrameã€‚
        dim_index (int): æŠ½å‡ºã—ãŸã„BERTãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰ã€‚
        top_n (int): è¡¨ç¤ºã™ã‚‹Top/Bottomã®æ•°ã€‚
    """
    dim_num = dim_index
    dim_col_name = f'bert_vec_{dim_num}_val'

    # 1. BERTæ¬¡å…ƒã®å€¤ã‚’æŠ½å‡º
    try:
        # ãƒ™ã‚¯ãƒˆãƒ«ã®æŒ‡å®šã•ã‚ŒãŸæ¬¡å…ƒã‚’æ–°ã—ã„åˆ—ã¨ã—ã¦è¿½åŠ 
        df[dim_col_name] = df['bert_vec'].apply(lambda x: x[dim_index])
    except IndexError:
        print(f"ã‚¨ãƒ©ãƒ¼: BERTãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {dim_index} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    # 2. çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    print(f"\n--- {dim_col_name} ã®çµ±è¨ˆæƒ…å ± ---")
    print(df[dim_col_name].describe())

    # 3. ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå•†å“åãƒ™ãƒ¼ã‚¹ã§ã‚½ãƒ¼ãƒˆã—ã€Top/Bottom Nã‚’æŠ½å‡º
    df_unique_names = df.drop_duplicates(subset=['name'])

    # Top N
    top_names = (
        df_unique_names.sort_values(by=dim_col_name, ascending=False)
        .head(top_n)['name'].tolist()
    )
    # Bottom N
    bottom_names = (
        df_unique_names.sort_values(by=dim_col_name, ascending=True)
        .head(top_n)['name'].tolist()
    )

    # 4. çµæœå‡ºåŠ›
    print(f"\n====================================")
    print(f" {dim_col_name} ãŒæœ€ã‚‚é«˜ã„å•†å“å (Top {top_n})")
    print(f"====================================")
    for i, name in enumerate(top_names, 1):
        print(f"{i}. {name}")

    print(f"\n====================================")
    print(f" {dim_col_name} ãŒæœ€ã‚‚ä½ã„å•†å“å (Bottom {top_n})")
    print(f"====================================")
    for i, name in enumerate(bottom_names, 1):
        print(f"{i}. {name}")


    # å‡¦ç†å¾Œã«ä¸€æ™‚åˆ—ã‚’å‰Šé™¤ï¼ˆä»»æ„ï¼‰
    del df[dim_col_name]
# analyze_bert_dimension(data_api.copy(), dim_index=9, top_n=5)
# analyze_bert_dimension(data_api.copy(), dim_index=6, top_n=5)
# analyze_bert_dimension(data_api.copy(), dim_index=463, top_n=5)
