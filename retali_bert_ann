from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
from collections import OrderedDict
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import japanize_matplotlib
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
import lightgbm as lgb
from sklearn.preprocessing import StandardScaler
from typing import Dict, Any, Tuple


# ---- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆä¸­é–“å±¤ã‚’è¿”ã™è¨­å®šï¼‰ ----
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased", output_hidden_states=True)


# --- å‰å‡¦ç†ã‚’é–¢æ•°åŒ– ---
def preprocess(retail_df, weather_df):
    # æ—¥ä»˜å‡¦ç†
    weather_df['æ—¥ä»˜'] = pd.to_datetime(weather_df['å¹´æœˆæ—¥.1'], errors='coerce')
    weather_df = weather_df.loc[:, ~weather_df.columns.str.contains(r'\.\d+$')]
    weather_df = weather_df.drop(columns=['æœ€æ·±ç©é›ª(cm)', 'å¹³å‡é›²é‡(10åˆ†æ¯”)', 'å¹´æœˆæ—¥'], errors='ignore')
    weather_df = weather_df.drop(index=[0, 1]).reset_index(drop=True)

    retail_df['æ—¥ä»˜'] = pd.to_datetime(retail_df['day'])
    retail_df = retail_df.drop(columns=['day'])

    merged = pd.merge(retail_df, weather_df, on='æ—¥ä»˜', how='left')

    # åºƒå‘Šçµ±åˆã¨ãƒã‚¤ãƒŠãƒªåŒ–
    ad_cols = ['SNS', 'å£²å ´æ–½ç­–', 'TVæ”¾æ˜ ', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³']
    for col in ad_cols:
        if col not in merged.columns:
            merged[col] = 0
    merged[ad_cols] = merged[ad_cols].fillna(0)
    for col in ad_cols:
        merged[col + '_æœ‰ç„¡'] = (merged[col] != 0).astype(int)
    merged['åºƒå‘Š_æœ‰ç„¡'] = merged[[c + '_æœ‰ç„¡' for c in ad_cols]].max(axis=1)
    merged = merged.drop(columns = ad_cols + [col + '_æœ‰ç„¡' for col in ad_cols])

    # ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆå•†å“ã”ã¨ï¼‰
    for lag in [1, 2, 3]:
        merged[f'åºƒå‘Š_æœ‰ç„¡_lag{lag}'] = merged.groupby('name')['åºƒå‘Š_æœ‰ç„¡'].shift(lag).fillna(0).astype(int)

    # ç§»å‹•å¹³å‡ï¼ˆ1ã¤å‰ã‹ã‚‰ã®3ç‚¹ç§»å‹•å¹³å‡ï¼‰
    merged['å£²ä¸Šæ•°_rollmean_prev3'] = (
        merged.groupby('name')['å£²ä¸Šæ•°']
              .apply(lambda s: s.shift(1).rolling(window=3, min_periods=1).mean())
              .reset_index(level=0, drop=True)
    ).fillna(0)

    # ç‰¹å®šã®æ¬ æå‡¦ç†ï¼ˆå…ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ„å›³ã‚’è¸è¥²ï¼‰
    merged = merged[merged.index != 3]  # å…ƒã‚³ãƒ¼ãƒ‰ã§é™¤å¤–ã—ã¦ã„ãŸè¡Œ
    beef_mask = (merged['name'] == 'ç„™ç…ç‰›è‚‰') & (merged.index >= 16) & (merged.index <= 94)
    for col in ['å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±']:
        if col in merged.columns:
            mean_value = merged.loc[beef_mask, col].mean()
            merged.loc[beef_mask, col] = merged.loc[beef_mask, col].fillna(mean_value)

    # å®¢æ•°ã®æ¬ æåŸ‹ã‚ï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®å€¤ã‚’å‚ç…§ï¼‰
    if 'å®¢æ•°' in merged.columns:
        try:
            costomer_count_250928 = merged.loc[merged['æ—¥ä»˜'] == pd.to_datetime('2025-09-28'), 'å®¢æ•°'].values[0]
            merged['å®¢æ•°'] = merged['å®¢æ•°'].fillna(costomer_count_250928)
        except Exception:
            merged['å®¢æ•°'] = merged['å®¢æ•°'].fillna(merged['å®¢æ•°'].mean())

    # æ•°å€¤åˆ—ã®å¹³å‡è£œå®Œ
    num_cols = ['price', 'å®¢æ•°', 'å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±',
                'å¹³å‡æ°—æ¸©(â„ƒ)', 'é™æ°´é‡ã®åˆè¨ˆ(mm)', 'æ—¥ç…§æ™‚é–“(æ™‚é–“)', 'å¹³å‡é¢¨é€Ÿ(m/s)', 'å¹³å‡è’¸æ°—åœ§(hPa)']
    num_cols = [c for c in num_cols if c in merged.columns]
    num_imputer = SimpleImputer(strategy='mean')
    merged[num_cols] = num_imputer.fit_transform(merged[num_cols])

    # IQRã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¤–ã‚Œå€¤å‡¦ç†ï¼‰
    for col in num_cols:
        Q1 = merged[col].quantile(0.25)
        Q3 = merged[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        merged[col] = merged[col].clip(lower, upper)

    # æ™‚ç³»åˆ—ç‰¹å¾´é‡
    SEASON = {1: "WINTER",2: "WINTER",3: "SPRING",4: "SPRING",5: "SPRING",
              6: "SUMMER",7: "SUMMER",8: "SUMMER",9: "FALL",10: "FALL",11: "FALL",12: "WINTER"}
    merged['æœˆ'] = merged['æ—¥ä»˜'].dt.month
    merged['é€±'] = merged['æ—¥ä»˜'].dt.isocalendar().week
    merged['å¹´'] = merged['æ—¥ä»˜'].dt.year
    merged['yday'] = merged['æ—¥ä»˜'].dt.dayofyear
    merged['season'] = merged['æœˆ'].map(SEASON)

    merged['yday_sin'] = np.sin(2 * np.pi * merged['yday'] / 365)
    merged['yday_cos'] = np.cos(2 * np.pi * merged['yday'] / 365)

    merged = merged.drop(columns=['æœˆ', 'é€±', 'å¹´', 'yday'])

    # season ã®ãƒ€ãƒŸãƒ¼åŒ–
    merged = pd.get_dummies(merged, columns=['season'], drop_first=True, dtype=int)


    ## ğŸš€ ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ (Interaction Features) ã®ç”Ÿæˆ
    lag_ad = 'åºƒå‘Š_æœ‰ç„¡_lag3'
    # 1. å½“åº—åœ¨åº«æ‰‹æŒé€± ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é …
    merged['Interaction_åœ¨åº«æ‰‹æŒé€±_åºƒå‘Š'] = merged['å½“åº—åœ¨åº«æ‰‹æŒé€±'] * merged[lag_ad]
    # 2. åº—é ­åœ¨åº«æ•° ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é …
    merged['Interaction_åº—é ­åœ¨åº«æ•°_åºƒå‘Š'] = merged['åº—é ­åœ¨åº«æ•°'] * merged[lag_ad]
    # 3. ç´å…¥äºˆå®šæ•° ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é … (Optional: ç›¸é–¢ãŒé«˜ã‹ã£ãŸãŸã‚è¿½åŠ )
    merged['Interaction_ç´å…¥äºˆå®šæ•°_åºƒå‘Š'] = merged['ç´å…¥äºˆå®šæ•°'] * merged[lag_ad]
    merged['Interaction_å®¢æ•°_åºƒå‘Š'] = merged['å®¢æ•°'] * merged[lag_ad]
    merged['Interaction_price_åºƒå‘Š'] = merged['price'] * merged[lag_ad]

    # ä½¿ã„ã‚„ã™ã„é †ã«åˆ—ã‚’æ•´ãˆã‚‹ï¼ˆä»»æ„ï¼‰
    merged = merged.reset_index(drop=True)
    return merged


# --- é€£ç¶šåºƒå‘Šæœ‰ç„¡ã®ç‰¹å¾´é‡ä½œæˆ ---
def add_continuous_ad_features(df):
    """
    'name' ã”ã¨ã« 'åºƒå‘Š_æœ‰ç„¡' ã®é€£ç¶šå›æ•°ã¨ã€ãã®ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹ã€‚
    """
    df_copy = df.copy()

    def calculate_consecutive_ad(series):
        # 1. é€£ç¶šã™ã‚‹å€¤ã®ãƒ–ãƒ­ãƒƒã‚¯IDã‚’ä½œæˆ
        # series.ne(series.shift()).cumsum() ã§ã€å€¤ãŒå¤‰ã‚ã‚‹ãŸã³ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå¢—ãˆã‚‹IDã‚’ä½œæˆ
        # ä¾‹: [1, 1, 0, 1, 1, 1, 0, 0, 1] -> [1, 1, 2, 3, 3, 3, 4, 4, 5]
        block_id = series.ne(series.shift()).cumsum()

        # 2. block_idã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€å„ãƒ–ãƒ­ãƒƒã‚¯å†…ã§ç´¯ç©ã‚«ã‚¦ãƒ³ãƒˆ (1ã‹ã‚‰é–‹å§‹)
        consecutive_count = series.groupby(block_id).cumcount() + 1

        # 3. åºƒå‘ŠãŒãªã„æœŸé–“ (å€¤ãŒ0ã®æœŸé–“) ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’0ã«ã™ã‚‹
        consecutive_count[series == 0] = 0

        return consecutive_count.astype(int)

    df_copy['åºƒå‘Š_æœ‰ç„¡_é€£ç¶š'] = (
        df_copy.groupby('name')['åºƒå‘Š_æœ‰ç„¡']
               .transform(calculate_consecutive_ad)
    )

    # 2. ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆ
    for lag in [1, 2, 3]:
        df_copy[f'åºƒå‘Š_æœ‰ç„¡_é€£ç¶š_lag{lag}'] = (
            df_copy.groupby('name')['åºƒå‘Š_æœ‰ç„¡_é€£ç¶š'].shift(lag).fillna(0).astype(int)
        )

    return df_copy


def bert_features(text, layer=8):
    # ãƒ†ã‚­ã‚¹ãƒˆ â†’ Tensor
    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)

    with torch.no_grad():
        outputs = model(**tokens)

    hidden_states = outputs.hidden_states  # å…¨å±¤ (13å€‹)

    # ---- ä½¿ã„ãŸã„å±¤ã‚’é¸æŠ ----
    h = hidden_states[layer]  # shape: (1, seq_len, 768)

    # ---- CLSãƒ™ã‚¯ãƒˆãƒ«ã‚’ç‰¹å¾´é‡ã«ã™ã‚‹ ----
    cls_vec = h[:, 0, :].squeeze().numpy()  # shape: (768,)

    return cls_vec


def bert_features_batch(text_list, layer=8, batch_size=16):

    all_vecs = []

    for i in range(0, len(text_list), batch_size):
        batch_texts = text_list[i:i+batch_size]

        # ã¾ã¨ã‚ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆpadding ã‚ã‚Šï¼‰
        tokens = tokenizer(batch_texts,
                           return_tensors='pt',
                           truncation=True,
                           padding=True,
                           max_length=128)

        with torch.no_grad():
            outputs = model(**tokens)

        hidden_states = outputs.hidden_states
        h = hidden_states[layer]       # shape: (B, seq_len, 768)
        mask = tokens["attention_mask"].unsqueeze(-1)

        # mean pooling
        h_masked = h * mask
        sum_vec = h_masked.sum(dim=1)
        len_tokens = mask.sum(dim=1)
        mean_vec = sum_vec / len_tokens  # shape: (B, 768)

        # numpy ã«å¤‰æ›
        all_vecs.append(mean_vec.numpy())

    # å…¨ãƒãƒƒãƒã‚’çµåˆ
    return np.vstack(all_vecs)


# --- BERTç‰¹å¾´é‡ã®æ¬¡å…ƒå‰Šæ¸›ï¼ˆè‡ªå·±ç¬¦å·åŒ–å™¨ï¼‰ ---
# 1. Autoencoderãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ (768 -> 512 -> 128)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, latent_dim)  # LATENT_DIM (ä½æ¬¡å…ƒã®ç‰¹å¾´é‡)
        )

        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ (128 -> 512 -> 768)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim)  # å…ƒã®æ¬¡å…ƒã«æˆ»ã™
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# 2. å­¦ç¿’ã¨æ¬¡å…ƒå‰Šæ¸›ã‚’è¡Œã†é–¢æ•°
def reduce_bert_dimension(df, input_dim=768, latent_dim=128, epochs=50, lr=1e-3):
    print("\n--- Autoencoderã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ã‚’é–‹å§‹ ---")

    bert_data_np = np.vstack(df["bert_vec"].values)

    scaler = StandardScaler()
    bert_scaled_np = scaler.fit_transform(bert_data_np)

    X_tensor = torch.from_numpy(bert_scaled_np).float()

    model = Autoencoder(input_dim, latent_dim)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    for epoch in range(epochs):
        model.train()

        # é †ä¼æ’­
        outputs = model(X_tensor)
        loss = criterion(outputs, X_tensor)  # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ X_tensor ã‚’å†ç¾ã§ãã‚‹ã‹

        # é€†ä¼æ’­ã¨æœ€é©åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}')

    model.eval()
    with torch.no_grad():
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼éƒ¨åˆ†ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡ã‚’æŠ½å‡º
        reduced_features_tensor = model.encoder(X_tensor)

    # Tensor -> NumPy
    reduced_features_np = reduced_features_tensor.numpy()

    df_reduced = df.copy()
    df_reduced['bert_vec_reduced'] = list(reduced_features_np)

    df_reduced = df_reduced.drop(columns=['bert_vec'])

    print(f"æ¬¡å…ƒå‰Šæ¸›å®Œäº†: {input_dim}æ¬¡å…ƒ -> {latent_dim}æ¬¡å…ƒ")
    return df_reduced


# --- ãƒ‡ãƒ¼ã‚¿ã®é¸æŠã€åˆ†å‰²ã€æ­£è¦åŒ– (ç›®çš„å¤‰æ•°ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œ) ---
def prepare_data_for_ann(df, target_col='å£²ä¸Šæ•°'):
    # ç›®çš„å¤‰æ•°ã®è¨­å®š
    y = df[target_col].values.reshape(-1, 1)  # MinMaxScalerã®ãŸã‚2æ¬¡å…ƒã«æ•´å½¢
    # y = np.log1p(y) # å¯¾æ•°å¤‰æ›

    # -----------------------------------------------------------
    # 1. ç‰¹å¾´é‡ã®é¸æŠã¨çµåˆ
    # -----------------------------------------------------------

    # é™¤å¤–ã™ã‚‹åˆ—
    drop_cols = ['name', 'æ—¥ä»˜', 'å£²ä¸Šé«˜', target_col]

    # BERTç‰¹å¾´é‡ã‚’ DataFrame ã®åˆ—ã«æˆ»ã™
    bert_reduced_df = pd.DataFrame(df['bert_vec_reduced'].tolist(),
                                   index=df.index,
                                   columns=[f'bert_reduced_{i}' for i in
                                            range(df['bert_vec_reduced'].iloc[0].shape[0])])

    # æœ€çµ‚çš„ãªç‰¹å¾´é‡ DataFrame ã®ä½œæˆ
    X_base = df.drop(columns=drop_cols + ['bert_vec_reduced'], errors='ignore')
    X = pd.concat([X_base, bert_reduced_df], axis=1)

    feature_names = X.columns.tolist()

    # -----------------------------------------------------------
    # 2. è¨“ç·´:æ¤œè¨¼:ãƒ†ã‚¹ãƒˆ = 8:1:1 ã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (å¤‰æ›´ãªã—)
    # -----------------------------------------------------------

    train_ratio = 0.8
    val_test_ratio = 0.2
    val_ratio_in_val_test = 0.5

    n_samples = len(X)
    n_train = int(n_samples * train_ratio)
    n_val_test = n_samples - n_train
    n_val = int(n_val_test * val_ratio_in_val_test)

    train_idx = X.index[:n_train]
    val_idx = X.index[n_train:n_train + n_val]
    test_idx = X.index[n_train + n_val:n_samples]

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²
    X_train, y_train = X.loc[train_idx], y[train_idx]
    X_val, y_val = X.loc[val_idx], y[val_idx]
    X_test, y_test = X.loc[test_idx], y[test_idx]

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_train)} ({len(X_train) / n_samples * 100:.1f}%)")

    # -----------------------------------------------------------
    # 3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ­£è¦åŒ–ï¼ˆç‰¹å¾´é‡:StandardScaler, ç›®çš„å¤‰æ•°:MinMaxScalerï¼‰
    # -----------------------------------------------------------

    # ç‰¹å¾´é‡ (X) ã®æ­£è¦åŒ–: StandardScaler
    X_scaler = StandardScaler()
    X_scaler.fit(X_train)
    X_train_scaled = X_scaler.transform(X_train)
    X_val_scaled = X_scaler.transform(X_val)
    X_test_scaled = X_scaler.transform(X_test)

    # ç›®çš„å¤‰æ•° (y) ã®æ­£è¦åŒ–: MinMaxScaler
    y_scaler = MinMaxScaler()
    y_scaler.fit(y_train)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ fit
    y_train_scaled = y_scaler.transform(y_train)
    y_val_scaled = y_scaler.transform(y_val)
    y_test_scaled = y_scaler.transform(y_test)

    print("ç‰¹å¾´é‡ãŠã‚ˆã³ç›®çš„å¤‰æ•°ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # -----------------------------------------------------------
    # 4. PyTorch Tensor ã¸ã®å¤‰æ›
    # -----------------------------------------------------------

    X_train_t, y_train_t = to_tensor_Xy(X_train_scaled, y_train_scaled.flatten())  # to_tensor_Xyã§å†åº¦æ•´å½¢ã•ã‚Œã‚‹ãŸã‚flatten()
    X_val_t, y_val_t = to_tensor_Xy(X_val_scaled, y_val_scaled.flatten())
    X_test_t, y_test_t = to_tensor_Xy(X_test_scaled, y_test_scaled.flatten())

    print(f"ANNãƒ¢ãƒ‡ãƒ«å…¥åŠ›æ¬¡å…ƒ: {X_train_t.shape[1]}")

    return {
        'X_train_t': X_train_t, 'y_train_t': y_train_t,
        'X_val_t': X_val_t, 'y_val_t': y_val_t,
        'X_test_t': X_test_t, 'y_test_t': y_test_t,
        'feature_names': feature_names,
        'y_scaler': y_scaler,
        'X_scaler': X_scaler,
        'train_idx': train_idx,
        'val_idx': val_idx,
        'test_idx': test_idx
    }


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: numpy -> torch tensor å¤‰æ› ---
def to_tensor_Xy(X_np, y_np):
    X_t = torch.from_numpy(X_np.astype(np.float32))
    y_t = torch.from_numpy(y_np.astype(np.float32)).reshape(-1, 1)
    return X_t, y_t


# --- ANN ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨å­¦ç¿’å‡¦ç† ---
# 1. ANNãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class RegressionANN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        # éš ã‚Œå±¤ã®å®šç¾©
        self.net = nn.Sequential(OrderedDict([
        ('hidden1', nn.Linear(input_dim, 300)),
        ('bn1', nn.BatchNorm1d(300)),
        ('act1', nn.ReLU()),
        ('drop1', nn.Dropout(0.3)),
        ('hidden2', nn.Linear(300, 200)),
        ('bn2', nn.BatchNorm1d(200)),
        ('act2', nn.ReLU()),
        ('drop2', nn.Dropout(0.3)),
        ('hidden3', nn.Linear(200, 100)),
        ('bn3', nn.BatchNorm1d(100)),
        ('act3', nn.ReLU()),
        ('drop3', nn.Dropout(0.3)),
        ('hidden4', nn.Linear(100, 50)),
        ('bn4', nn.BatchNorm1d(50)),
        ('act4', nn.ReLU()),
        ('drop4', nn.Dropout(0.2)),
        ('output', nn.Linear(50, 1))
    ]))

    def forward(self, x):
        return self.net(x)

# 2. ANNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡
def train_and_evaluate_ann(prepared_data, epochs=100, lr=1e-3):
    # ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®å–ã‚Šå‡ºã—
    X_train_t = prepared_data['X_train_t']
    y_train_t = prepared_data['y_train_t']
    X_val_t = prepared_data['X_val_t']
    y_val_t = prepared_data['y_val_t']
    X_test_t = prepared_data['X_test_t']
    y_test_t = prepared_data['y_test_t']
    y_scaler = prepared_data['y_scaler']

    input_dim = X_train_t.shape[1]

    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    model = RegressionANN(input_dim)

    # æå¤±é–¢æ•° (MSE) ã¨æœ€é©åŒ–æ‰‹æ³• (Adam) ã®å®šç¾©
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # æå¤±ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆ
    train_losses = []
    val_losses = []

    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print("\n--- ANNãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’é–‹å§‹ ---")
    for epoch in range(epochs):
        model.train()

        # é †ä¼æ’­
        outputs = model(X_train_t)
        loss = criterion(outputs, y_train_t)

        # é€†ä¼æ’­ã¨æœ€é©åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # è¨“ç·´æå¤±ã‚’è¨˜éŒ²
        train_losses.append(loss.item())

        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸå€¤ã§ã®è©•ä¾¡ï¼‰
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_t)
            val_loss = criterion(val_outputs, y_val_t)

        # æ¤œè¨¼æå¤±ã‚’è¨˜éŒ²
        val_losses.append(val_loss.item())

        if (epoch + 1) % 10 == 0 or epoch == 1:  # 1å›ç›®ã®æå¤±ã‚‚è¡¨ç¤º
            print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}')

    # 2. å­¦ç¿’éç¨‹ã®å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss (MSE)')
    plt.plot(val_losses, label='Validation Loss (MSE)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.title('ANN Model Learning Curve')
    plt.legend()
    plt.grid(True)
    plt.show()

    # 3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    model.eval()
    with torch.no_grad():
        y_pred_scaled_t = model(X_test_t)

    # 4. ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã™ (Inverse Transform)
    y_pred_scaled_np = y_pred_scaled_t.numpy()
    y_test_scaled_np = y_test_t.numpy()

    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ã£ã¦äºˆæ¸¬å€¤ã¨å®Ÿæ¸¬å€¤ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
    y_pred = y_scaler.inverse_transform(y_pred_scaled_np)
    y_true = y_scaler.inverse_transform(y_test_scaled_np)

    # 5. è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®— (å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã§è©•ä¾¡)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print("\n--- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡ (å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«) ---")
    print(f"Mean Absolute Error (MAE): {mae:.2f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
    print(f"R-squared (R2): {r2:.4f}")

    return y_true, y_pred, model


# --- ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã®å®Ÿè¡Œ(PI) ---
def feature_importance_analysis(model, prepared_data, y_true, y_pred):
    X_test_t = prepared_data['X_test_t']
    feature_names = prepared_data['feature_names']

    # -----------------------------------------------------------
    # Permutation Importance (PI) ã®è¨ˆç®—
    # -----------------------------------------------------------

    # PI ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ (MSE) ã®å¤‰åŒ–ã‚’è¦‹ã‚‹ãŸã‚ã€å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã§è©•ä¾¡ã—ã¾ã™ã€‚
    # ç›®çš„å¤‰æ•°ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ (y_scaler) ã‚’ä½¿ã£ã¦ã€äºˆæ¸¬é–¢æ•°ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    y_scaler = prepared_data['y_scaler']

    # ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™é–¢æ•°
    def predict_unscaled(model, X_tensor):
        model.eval()
        with torch.no_grad():
            # äºˆæ¸¬çµæœ (Scaled)
            y_pred_scaled = model(X_tensor).numpy()
            # äºˆæ¸¬çµæœã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
            y_pred_unscaled = y_scaler.inverse_transform(y_pred_scaled)
            return y_pred_unscaled.flatten()

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®Numpyé…åˆ— (Scaled)
    X_test_np = X_test_t.numpy()

    # PIè¨ˆç®—ã®ãŸã‚ã®æå¤±é–¢æ•° (ã“ã“ã§ã¯ Mean Squared Error ã‚’ä½¿ç”¨)
    def mse_loss(y_true, y_pred):
        return mean_squared_error(y_true, y_pred)

    # Permutation Importance ã®è¨ˆç®—
    # è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆNumpyï¼‰ã‚’ä½¿ç”¨
    print("\n--- 1. Permutation Importance (PI) ã®è¨ˆç®— ---")

    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢ (ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—)
    baseline_pred = predict_unscaled(model, X_test_t)
    baseline_score = mse_loss(y_true, baseline_pred)
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ MSE (ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—): {baseline_score:.2f}")

    pi_scores = {}

    for i, name in enumerate(feature_names):
        # iç•ªç›®ã®ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        X_test_shuffled = X_test_np.copy()
        X_test_shuffled[:, i] = np.random.permutation(X_test_shuffled[:, i])

        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«å¾Œã®äºˆæ¸¬
        X_shuffled_t = torch.from_numpy(X_test_shuffled).float()
        shuffled_pred = predict_unscaled(model, X_shuffled_t)

        # æå¤±ã®å¢—åŠ 
        shuffled_score = mse_loss(y_true, shuffled_pred)
        pi_scores[name] = shuffled_score - baseline_score

    # çµæœã‚’DataFrameã«æ ¼ç´ã—ã€ã‚½ãƒ¼ãƒˆ
    pi_df = pd.DataFrame(list(pi_scores.items()), columns=['Feature', 'Importance (MSE Increase)'])
    pi_df = pi_df.sort_values(by='Importance (MSE Increase)', ascending=False).reset_index(drop=True)

    # å¯è¦–åŒ–
    plt.figure(figsize=(12, pi_df.shape[0] * 0.3))
    plt.barh(pi_df['Feature'], pi_df['Importance (MSE Increase)'])
    plt.xlabel('Importance (Increase in MSE after permutation)')
    plt.title('Permutation Importance')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

    '''# -----------------------------------------------------------
    # 2. SHAP (SHapley Additive exPlanations) ã®è¨ˆç®—ã¨å¯è¦–åŒ–
    # -----------------------------------------------------------

    # SHAPã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    import shap

    print("\n--- 2. SHAP (SHapley Additive exPlanations) ã®è¨ˆç®— ---")

    # èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ä½¿ç”¨ (è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚)
    X_train_np = prepared_data['X_train_t'].numpy()
    background = X_train_np[np.random.choice(X_train_np.shape[0], 100, replace=False)]

    # DeepExplainer ã®ä½¿ç”¨ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‘ã‘ï¼‰
    explainer = shap.DeepExplainer(model, torch.from_numpy(background).float())

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹SHAPå€¤ã®è¨ˆç®— (æ³¨æ„: æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)
    shap_values = explainer.shap_values(X_test_t)

    # SHAPè¦ç´„ãƒ—ãƒ­ãƒƒãƒˆ (å…¨ä½“çš„ãªç‰¹å¾´é‡ã®é‡è¦åº¦)
    print("\nSHAP Summary Plot (ç‰¹å¾´é‡ã®å…¨ä½“çš„ãªå½±éŸ¿åº¦)")
    # ç‰¹å¾´é‡åã‚’NumPyé…åˆ—ã«å¤‰æ›ã—ã¦æ¸¡ã™
    shap.summary_plot(shap_values, X_test_np, feature_names=feature_names)
    '''
    return pi_df


# --- Uplift Modeling (åºƒå‘ŠåŠ¹æœ) ã®å¯è¦–åŒ– ---
# --- Individual Uplift Score ã®è¨ˆç®— ---
def calculate_individual_uplift(model, prepared_data):
    X_test_t = prepared_data['X_test_t']
    feature_names = prepared_data['feature_names']
    y_scaler = prepared_data['y_scaler']
    X_scaler = prepared_data['X_scaler']

    # 'åºƒå‘Š_æœ‰ç„¡' ã®ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    try:
        ad_col_index = feature_names.index('åºƒå‘Š_æœ‰ç„¡')
    except ValueError:
        print("ã‚¨ãƒ©ãƒ¼: 'åºƒå‘Š_æœ‰ç„¡' åˆ—ãŒç‰¹å¾´é‡ãƒªã‚¹ãƒˆã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Upliftè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None, None, None

    # Standard Scaler ã‚’ä½¿ç”¨ã—ã¦ã€å…ƒã®å€¤ 0 ã¨ 1 ãŒ scaled ç©ºé–“ã§ä½•ã«ãªã‚‹ã‹ã‚’è¨ˆç®—
    ad_mean = X_scaler.mean_[ad_col_index]
    ad_std = X_scaler.scale_[ad_col_index]

    scaled_ad_off = (0 - ad_mean) / ad_std
    scaled_ad_on = (1 - ad_mean) / ad_std

    print(f"\n--- S-Learner Individual Uplift è¨ˆç®— ---")
    print(f"'åºƒå‘Š_æœ‰ç„¡'=0 ã® Scaled å€¤: {scaled_ad_off:.4f}")
    print(f"'åºƒå‘Š_æœ‰ç„¡'=1 ã® Scaled å€¤: {scaled_ad_on:.4f}")

    # 2. MT (Treatment) ã¨ MC (Control) ã®äºˆæ¸¬
    model.eval()
    with torch.no_grad():
        # --- MC (Control: åºƒå‘Š_æœ‰ç„¡=0) ã®äºˆæ¸¬ ---
        X_C_t = X_test_t.clone()
        X_C_t[:, ad_col_index] = scaled_ad_off  # å…¨ã‚µãƒ³ãƒ—ãƒ«ã§ 'åºƒå‘Š_æœ‰ç„¡' ã‚’ 0 ã«å¼·åˆ¶
        M_C_scaled_t = model(X_C_t)

        # --- MT (Treatment: åºƒå‘Š_æœ‰ç„¡=1) ã®äºˆæ¸¬ ---
        X_T_t = X_test_t.clone()
        X_T_t[:, ad_col_index] = scaled_ad_on  # å…¨ã‚µãƒ³ãƒ—ãƒ«ã§ 'åºƒå‘Š_æœ‰ç„¡' ã‚’ 1 ã«å¼·åˆ¶
        M_T_scaled_t = model(X_T_t)

    # 3. Inverse Transform ã¨ Uplift è¨ˆç®—
    M_C = y_scaler.inverse_transform(M_C_scaled_t.numpy()).flatten()
    M_T = y_scaler.inverse_transform(M_T_scaled_t.numpy()).flatten()

    # Individual Uplift Score ã®è¨ˆç®—
    individual_uplift = M_T - M_C

    print(f"Individual Uplift Score ã®è¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å¹³å‡ Uplift: {individual_uplift.mean():.2f}")

    return M_C, M_T, individual_uplift

# --- Uplift Modeling (åºƒå‘ŠåŠ¹æœ) ã®å¯è¦–åŒ– (Individual Upliftå¯¾å¿œ) ---
def uplift_visualization(M_C, individual_uplift):
    print(f"\n--- Individual Uplift Visualization ã‚’é–‹å§‹ ---")

    # -----------------------------------------------------------
    # 1. äºˆæ¸¬ Uplift ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  (æ¨ªè»¸: Uplift, Uplift=0 ã«èµ¤ç·š)
    # -----------------------------------------------------------

    plt.figure(figsize=(10, 6))

    # Uplift ã®åˆ†å¸ƒ
    plt.hist(individual_uplift, bins=30, alpha=0.8, color='purple', label='Individual Uplift Score')

    # Uplift = 0 ã®ä½ç½®ã«èµ¤ã„ç·šã‚’è¿½åŠ 
    plt.axvline(0, color='red', linestyle='dashed', linewidth=2, label='Uplift = 0')

    plt.title('Individual Uplift Score ã®åˆ†å¸ƒ')
    plt.xlabel('Uplift (MT_pred - MC_pred)')
    plt.ylabel('åº¦æ•°')
    plt.legend()
    plt.grid(axis='y', linestyle='--')
    plt.show()

    # -----------------------------------------------------------
    # 2. æ•£å¸ƒå›³ (æ¨ªè»¸: M_C, ç¸¦è»¸: Uplift = M_T - M_C, 4è±¡é™)
    # -----------------------------------------------------------

    mc_avg = M_C.mean()

    plt.figure(figsize=(10, 6))

    # æ•£å¸ƒå›³ã®ãƒ—ãƒ­ãƒƒãƒˆ (å…¨ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«)
    plt.scatter(M_C, individual_uplift, alpha=0.6, s=30, color='blue')

    # Uplift = 0 ã®èµ¤ã„ç·šã‚’å¼•ã (æ¨ªç·š)
    plt.axhline(0, color='red', linestyle='-', linewidth=2, label='Uplift = 0')

    # MC_avg ã®ç‚¹ç·š (ç¸¦ç·š)
    plt.axvline(mc_avg, color='gray', linestyle='--', linewidth=1, label=f'MC_avg={mc_avg:.2f}')

    # 4è±¡é™ã®åç§°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¿½åŠ 
    y_max = individual_uplift.max() * 0.95
    y_min = individual_uplift.min() * 0.95

    # Persuadables (èª¬å¾—å¯èƒ½å±¤) - åºƒå‘Šã‚’æ‰“ã¤ã¹ãå±¤
    plt.text(M_C.min(), y_max, 'Persuadables',
             verticalalignment='top', horizontalalignment='left', color='green', fontsize=12, weight='bold')
    # Sure Things (å½“ç„¶è²·ã†å±¤) - åºƒå‘Šä¸è¦
    plt.text(mc_avg, y_max, 'Sure Things',
             verticalalignment='top', horizontalalignment='left', color='darkorange', fontsize=12, weight='bold')
    # Sleepers (ä¼‘çœ å±¤) - åºƒå‘Šä¸è¦
    plt.text(mc_avg, y_min, 'Sleepers',
             verticalalignment='bottom', horizontalalignment='left', color='purple', fontsize=12, weight='bold')
    # Lost Causes (è¦‹è¾¼ã¿è–„å±¤) - åºƒå‘Šç„¡é§„
    plt.text(M_C.min(), y_min, 'Lost Causes',
             verticalalignment='bottom', horizontalalignment='left', color='red', fontsize=12, weight='bold')

    plt.title('Individual Uplift Score vs Control Prediction ($M_C$ ã§ã®4è±¡é™)')
    plt.xlabel('äºˆæ¸¬å£²ä¸Š $M_C$ (åºƒå‘Šãªã—ã®å ´åˆ)')
    plt.ylabel('Individual Uplift ($M_T - M_C$)')
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.6)
    plt.show()


def analyze_uplift_segments(
        M_C: np.ndarray,
        individual_uplift: np.ndarray,
        prepared_data: Dict[str, Any],
        top_pi_features: list,
        uplift_threshold: float = 0.0,
        # mc_threshold: float = M_C.mean()
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    å€‹åˆ¥Upliftã‚¹ã‚³ã‚¢ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã—ã€ä¸»è¦ãªç‰¹å¾´é‡ã®å¹³å‡å€¤ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

    PIã§ä¸Šä½ã®ç‰¹å¾´é‡ã‚’ã‚­ãƒ¼ã¨ã—ã¦æ¯”è¼ƒã™ã‚‹ã€‚
    """

    print("\n--- Uplift ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ ç‰¹å¾´é‡åˆ†æã‚’é–‹å§‹ ---")

    # 1. ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒã¨çµåˆ (å¤‰æ›´ãªã—)
    X_test_scaled = prepared_data['X_test_t'].numpy()
    X_scaler = prepared_data['X_scaler']
    feature_names = prepared_data['feature_names']

    X_test_original = X_scaler.inverse_transform(X_test_scaled)
    df_results = pd.DataFrame(X_test_original, columns=feature_names)

    df_results['M_C'] = M_C
    df_results['Individual_Uplift'] = individual_uplift

    # 2. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å®šç¾©ã¨åˆ†é¡ (å¤‰æ›´ãªã—)
    PERSUADABLES_CONDITION = (df_results['Individual_Uplift'] > uplift_threshold) & \
                             (df_results['M_C'] <= M_C.mean())
    LOST_SLEEPERS_CONDITION = (df_results['Individual_Uplift'] <= 0)

    df_persuadables = df_results[PERSUADABLES_CONDITION].copy()
    df_lost_sleepers = df_results[LOST_SLEEPERS_CONDITION].copy()

    segment_counts = {
        'Persuadables': len(df_persuadables),
        'Sleepers/Lost Causes': len(df_lost_sleepers),
        'Overall Test Set': len(df_results)
    }
    df_segment_counts = pd.DataFrame(
        list(segment_counts.items()), columns=['Segment', 'Count']
    )
    print(df_segment_counts.to_markdown(index=False))

    if df_persuadables.empty or df_lost_sleepers.empty:
        print("ã‚¨ãƒ©ãƒ¼: æ¯”è¼ƒã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é–¾å€¤ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame(), df_segment_counts

    # 3. ç‰¹å¾´é‡æ¯”è¼ƒã®æ§‹ç¯‰
    # PIä¸Šä½ç‰¹å¾´é‡ã«ã€åºƒå‘Šãƒ»ç›¸äº’ä½œç”¨ãƒ»é€£ç¶šåºƒå‘Šç‰¹å¾´é‡ã‚’çµ±åˆ
    # BERTç‰¹å¾´é‡ã¯å°‘ãªãã¨ã‚‚ä¸€ã¤å«ã‚ã‚‹ (ä¾‹: bert_reduced_0)
    essential_features = [
        'åºƒå‘Š_æœ‰ç„¡_é€£ç¶š',
        'Interaction_åº—é ­åœ¨åº«æ•°_åºƒå‘Š',
        'Interaction_å®¢æ•°_åºƒå‘Š',
        'bert_reduced_0'
    ]

    # PIä¸Šä½ç‰¹å¾´é‡ã¨å¿…é ˆç‰¹å¾´é‡ã‚’çµåˆã—ã€é‡è¤‡ã‚’å‰Šé™¤
    key_features = list(set(top_pi_features + essential_features))
    # 'åºƒå‘Š_æœ‰ç„¡' ã¯ Uplift è¨ˆç®—ã®åŸºã§ã‚ã‚‹ãŸã‚é™¤å¤–ã™ã‚‹ (çµæœã®è§£é‡ˆã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚)
    if 'åºƒå‘Š_æœ‰ç„¡' in key_features:
        key_features.remove('åºƒå‘Š_æœ‰ç„¡')

    key_features = [f for f in key_features if f in df_results.columns]

    print(f"\n--- æ¯”è¼ƒå¯¾è±¡ã®å‹•çš„ key_features ({len(key_features)}å€‹) ---")
    print(key_features)

    segments = {
        'Persuadables': df_persuadables,
        'Sleepers/Lost Causes': df_lost_sleepers,
        'Overall Test Set': df_results
    }

    comparison_data = []

    for feature in key_features:
        row = {'Feature': feature}
        for name, df in segments.items():
            mean_val = df[feature].mean()
            std_val = df[feature].std()

            # æ•°å€¤ã®æ•´å½¢ãƒ­ã‚¸ãƒƒã‚¯ (å¤‰æ›´ãªã—)
            if feature.startswith('bert_reduced'):
                mean_str = f"{mean_val:.3f}"
                std_str = f"({std_val:.3f})"
            elif feature in ['å½“åº—åœ¨åº«æ‰‹æŒé€±', 'å®¢æ•°', 'åºƒå‘Š_æœ‰ç„¡_é€£ç¶š']:
                mean_str = f"{mean_val:.1f}"
                std_str = f"({std_val:.1f})"
            else:
                mean_str = f"{mean_val:.0f}"
                std_str = f"({std_val:.0f})"

            row[f'{name} (Mean)'] = mean_str
            row[f'{name} (Std)'] = std_str
        comparison_data.append(row)

    df_comparison = pd.DataFrame(comparison_data)

    return df_comparison, df_segment_counts


# ======================================================================================================================


pd.set_option('display.max_columns', None)  # ã™ã¹ã¦ã®åˆ—ã‚’è¡¨ç¤º
pd.set_option('display.width', None)        # æ¨ªå¹…åˆ¶é™ã‚’è§£é™¤


retail = pd.read_excel('C:/Users/23X4002/Desktop/3å¹´ç”Ÿç§‹/PBL/éœ€è¦äºˆæ¸¬(åºƒå‘Š)/å°å£²ãƒ‡ãƒ¼ã‚¿.xlsx')
weather= pd.read_csv('C:/Users/23X4002/Desktop/3å¹´ç”Ÿç§‹/PBL/éœ€è¦äºˆæ¸¬(åºƒå‘Š)/æ°—è±¡ãƒ‡ãƒ¼ã‚¿03.csv',
                     encoding='cp932', skiprows=3)

data_mm = preprocess(retail, weather)
data_bert = add_continuous_ad_features(data_mm)
data_bert["bert_vec"] = list(bert_features_batch(data_bert["name"].tolist(), layer=8))
data_bert_reduced = reduce_bert_dimension(data_bert.copy(),
                                          input_dim=768,
                                          latent_dim=24,
                                          epochs=100)

print("\n--- å‰Šæ¸›å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­ ---")
print(data_bert_reduced[['name', 'bert_vec_reduced']].head())
print("\nå‰Šæ¸›å¾Œã®ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°:", data_bert_reduced['bert_vec_reduced'].iloc[0].shape)

prepared_data = prepare_data_for_ann(data_bert_reduced.copy(), target_col='å£²ä¸Šæ•°')
print("\n--- PyTorch Tensor ã®å½¢çŠ¶ ---")
print(f"è¨“ç·´ã‚»ãƒƒãƒˆ X: {prepared_data['X_train_t'].shape}, y: {prepared_data['y_train_t'].shape}")
print(f"æ¤œè¨¼ã‚»ãƒƒãƒˆ X: {prepared_data['X_val_t'].shape}, y: {prepared_data['y_val_t'].shape}")
print(f"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ X: {prepared_data['X_test_t'].shape}, y: {prepared_data['y_test_t'].shape}")
# ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆANNãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›æ¬¡å…ƒæ•°ã«å¯¾å¿œï¼‰
final_feature_names = prepared_data['feature_names']
print("-- é¸æŠã—ãŸç‰¹å¾´é‡ --\n",final_feature_names)

# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡
y_true, y_pred , model= train_and_evaluate_ann(prepared_data, epochs=200)

# ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã®å®Ÿè¡Œ
pi_df = feature_importance_analysis(model, prepared_data, y_true, y_pred) # æ–°ã—ãè¿½åŠ 
# PIã®ä¸Šä½Nå€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ
N_TOP_FEATURES = 10
top_pi_features = pi_df['Feature'].head(N_TOP_FEATURES).tolist()
print(f"\n--- Permutation Importance (PI) ä¸Šä½ {N_TOP_FEATURES} ç‰¹å¾´é‡ ---")
print(top_pi_features)

# Uplift Visualizationã®å®Ÿè¡Œ
# Individual Uplift Score ã®è¨ˆç®— (S-Learner)
M_C, M_T, individual_uplift = calculate_individual_uplift(model, prepared_data)
# Uplift Visualizationã®å®Ÿè¡Œ (Individual Uplift ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨)
if M_C is not None:
    uplift_visualization(M_C, individual_uplift)
    # å‹•çš„ãªPIä¸Šä½ç‰¹å¾´é‡ã‚’ä½¿ã£ã¦åˆ†æé–¢æ•°ã®å®Ÿè¡Œ
    df_comparison_result, df_counts = analyze_uplift_segments(
        M_C,
        individual_uplift,
        prepared_data,
        top_pi_features=top_pi_features,
        uplift_threshold=0.0,
        # mc_threshold=120.0
    )
    # print(f"\n--- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ ã‚µãƒ³ãƒ—ãƒ«æ•° ---")
    # print(df_counts.to_markdown(index=False))
    print(f"\n--- ä¸»è¦ãªç‰¹å¾´é‡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥æ¯”è¼ƒ ---")
    print(df_comparison_result.to_markdown(index=False))
