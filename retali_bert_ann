from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
from collections import OrderedDict
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import japanize_matplotlib
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler


# ---- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆä¸­é–“å±¤ã‚’è¿”ã™è¨­å®šï¼‰ ----
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased", output_hidden_states=True)

def bert_features(text, layer=8):
    # ãƒ†ã‚­ã‚¹ãƒˆ â†’ Tensor
    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)

    with torch.no_grad():
        outputs = model(**tokens)

    hidden_states = outputs.hidden_states  # å…¨å±¤ (13å€‹)

    # ---- ä½¿ã„ãŸã„å±¤ã‚’é¸æŠ ----
    h = hidden_states[layer]  # shape: (1, seq_len, 768)

    # ---- CLSãƒ™ã‚¯ãƒˆãƒ«ã‚’ç‰¹å¾´é‡ã«ã™ã‚‹ ----
    cls_vec = h[:, 0, :].squeeze().numpy()  # shape: (768,)

    return cls_vec


def bert_features_batch(text_list, layer=8, batch_size=16):

    all_vecs = []

    for i in range(0, len(text_list), batch_size):
        batch_texts = text_list[i:i+batch_size]

        # ã¾ã¨ã‚ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆpadding ã‚ã‚Šï¼‰
        tokens = tokenizer(batch_texts,
                           return_tensors='pt',
                           truncation=True,
                           padding=True,
                           max_length=128)

        with torch.no_grad():
            outputs = model(**tokens)

        hidden_states = outputs.hidden_states
        h = hidden_states[layer]       # shape: (B, seq_len, 768)
        mask = tokens["attention_mask"].unsqueeze(-1)

        # mean pooling
        h_masked = h * mask
        sum_vec = h_masked.sum(dim=1)
        len_tokens = mask.sum(dim=1)
        mean_vec = sum_vec / len_tokens  # shape: (B, 768)

        # numpy ã«å¤‰æ›
        all_vecs.append(mean_vec.numpy())

    # å…¨ãƒãƒƒãƒã‚’çµåˆ
    return np.vstack(all_vecs)


# --- å‰å‡¦ç†ã‚’é–¢æ•°åŒ– ---
def preprocess(retail_df, weather_df):
    # æ—¥ä»˜å‡¦ç†
    weather_df['æ—¥ä»˜'] = pd.to_datetime(weather_df['å¹´æœˆæ—¥.1'], errors='coerce')
    weather_df = weather_df.loc[:, ~weather_df.columns.str.contains(r'\.\d+$')]
    weather_df = weather_df.drop(columns=['æœ€æ·±ç©é›ª(cm)', 'å¹³å‡é›²é‡(10åˆ†æ¯”)', 'å¹´æœˆæ—¥'], errors='ignore')
    weather_df = weather_df.drop(index=[0, 1]).reset_index(drop=True)

    retail_df['æ—¥ä»˜'] = pd.to_datetime(retail_df['day'])
    retail_df = retail_df.drop(columns=['day'])

    merged = pd.merge(retail_df, weather_df, on='æ—¥ä»˜', how='left')

    # åºƒå‘Šçµ±åˆã¨ãƒã‚¤ãƒŠãƒªåŒ–
    ad_cols = ['SNS', 'å£²å ´æ–½ç­–', 'TVæ”¾æ˜ ', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³']
    for col in ad_cols:
        if col not in merged.columns:
            merged[col] = 0
    merged[ad_cols] = merged[ad_cols].fillna(0)
    for col in ad_cols:
        merged[col + '_æœ‰ç„¡'] = (merged[col] != 0).astype(int)
    merged['åºƒå‘Š_æœ‰ç„¡'] = merged[[c + '_æœ‰ç„¡' for c in ad_cols]].max(axis=1)
    merged = merged.drop(columns = ad_cols + [col + '_æœ‰ç„¡' for col in ad_cols])

    # ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆå•†å“ã”ã¨ï¼‰
    for lag in [1, 2, 3]:
        merged[f'åºƒå‘Š_æœ‰ç„¡_lag{lag}'] = merged.groupby('name')['åºƒå‘Š_æœ‰ç„¡'].shift(lag).fillna(0).astype(int)

    # ç§»å‹•å¹³å‡ï¼ˆ1ã¤å‰ã‹ã‚‰ã®3ç‚¹ç§»å‹•å¹³å‡ï¼‰
    merged['å£²ä¸Šæ•°_rollmean_prev3'] = (
        merged.groupby('name')['å£²ä¸Šæ•°']
              .apply(lambda s: s.shift(1).rolling(window=3, min_periods=1).mean())
              .reset_index(level=0, drop=True)
    ).fillna(0)

    # ç‰¹å®šã®æ¬ æå‡¦ç†ï¼ˆå…ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ„å›³ã‚’è¸è¥²ï¼‰
    merged = merged[merged.index != 3]  # å…ƒã‚³ãƒ¼ãƒ‰ã§é™¤å¤–ã—ã¦ã„ãŸè¡Œ
    beef_mask = (merged['name'] == 'ç„™ç…ç‰›è‚‰') & (merged.index >= 16) & (merged.index <= 94)
    for col in ['å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±']:
        if col in merged.columns:
            mean_value = merged.loc[beef_mask, col].mean()
            merged.loc[beef_mask, col] = merged.loc[beef_mask, col].fillna(mean_value)

    # å®¢æ•°ã®æ¬ æåŸ‹ã‚ï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®å€¤ã‚’å‚ç…§ï¼‰
    if 'å®¢æ•°' in merged.columns:
        try:
            costomer_count_250928 = merged.loc[merged['æ—¥ä»˜'] == pd.to_datetime('2025-09-28'), 'å®¢æ•°'].values[0]
            merged['å®¢æ•°'] = merged['å®¢æ•°'].fillna(costomer_count_250928)
        except Exception:
            merged['å®¢æ•°'] = merged['å®¢æ•°'].fillna(merged['å®¢æ•°'].mean())

    # æ•°å€¤åˆ—ã®å¹³å‡è£œå®Œ
    num_cols = ['price', 'å®¢æ•°', 'å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±',
                'å¹³å‡æ°—æ¸©(â„ƒ)', 'é™æ°´é‡ã®åˆè¨ˆ(mm)', 'æ—¥ç…§æ™‚é–“(æ™‚é–“)', 'å¹³å‡é¢¨é€Ÿ(m/s)', 'å¹³å‡è’¸æ°—åœ§(hPa)']
    num_cols = [c for c in num_cols if c in merged.columns]
    num_imputer = SimpleImputer(strategy='mean')
    merged[num_cols] = num_imputer.fit_transform(merged[num_cols])

    # IQRã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¤–ã‚Œå€¤å‡¦ç†ï¼‰
    for col in num_cols:
        Q1 = merged[col].quantile(0.25)
        Q3 = merged[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        merged[col] = merged[col].clip(lower, upper)

    # æ™‚ç³»åˆ—ç‰¹å¾´é‡
    SEASON = {1: "WINTER",2: "WINTER",3: "SPRING",4: "SPRING",5: "SPRING",
              6: "SUMMER",7: "SUMMER",8: "SUMMER",9: "FALL",10: "FALL",11: "FALL",12: "WINTER"}
    merged['æœˆ'] = merged['æ—¥ä»˜'].dt.month
    merged['é€±'] = merged['æ—¥ä»˜'].dt.isocalendar().week
    merged['å¹´'] = merged['æ—¥ä»˜'].dt.year
    merged['yday'] = merged['æ—¥ä»˜'].dt.dayofyear
    merged['season'] = merged['æœˆ'].map(SEASON)

    merged['yday_sin'] = np.sin(2 * np.pi * merged['yday'] / 365)
    merged['yday_cos'] = np.cos(2 * np.pi * merged['yday'] / 365)

    merged = merged.drop(columns=['æœˆ', 'é€±', 'å¹´', 'yday'])

    # season ã®ãƒ€ãƒŸãƒ¼åŒ–
    merged = pd.get_dummies(merged, columns=['season'], drop_first=True, dtype=int)

    # æ­£è¦åŒ–ï¼ˆmin-maxï¼‰ --- æ³¨æ„: ã‚¹ã‚±ãƒ¼ãƒ«ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ fit ã™ã‚‹ã®ãŒç†æƒ³ã ãŒã€ã“ã“ã¯ç°¡æ˜“å‡¦ç†
    scale_cols = ['price', 'å®¢æ•°', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±',
                  'å¹³å‡æ°—æ¸©(â„ƒ)', 'é™æ°´é‡ã®åˆè¨ˆ(mm)', 'æ—¥ç…§æ™‚é–“(æ™‚é–“)', 'å¹³å‡é¢¨é€Ÿ(m/s)', 'å¹³å‡è’¸æ°—åœ§(hPa)']
    sales_max = merged['å£²ä¸Šæ•°'].max()
    sales_min = merged['å£²ä¸Šæ•°'].min()
    scale_cols = [c for c in scale_cols if c in merged.columns]
    mm = MinMaxScaler()
    merged[scale_cols] = mm.fit_transform(merged[scale_cols])

    ## ğŸš€ ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ (Interaction Features) ã®ç”Ÿæˆ
    # 1. å½“åº—åœ¨åº«æ‰‹æŒé€± ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é …
    merged['Interaction_åœ¨åº«æ‰‹æŒé€±_åºƒå‘Š'] = merged['å½“åº—åœ¨åº«æ‰‹æŒé€±'] * merged['åºƒå‘Š_æœ‰ç„¡_lag2']
    # 2. åº—é ­åœ¨åº«æ•° ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é …
    merged['Interaction_åº—é ­åœ¨åº«æ•°_åºƒå‘Š'] = merged['åº—é ­åœ¨åº«æ•°'] * merged['åºƒå‘Š_æœ‰ç„¡_lag2']
    # 3. ç´å…¥äºˆå®šæ•° ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é … (Optional: ç›¸é–¢ãŒé«˜ã‹ã£ãŸãŸã‚è¿½åŠ )
    merged['Interaction_ç´å…¥äºˆå®šæ•°_åºƒå‘Š'] = merged['ç´å…¥äºˆå®šæ•°'] * merged['åºƒå‘Š_æœ‰ç„¡_lag2']
    merged['Interaction_å®¢æ•°_åºƒå‘Š'] = merged['å®¢æ•°'] * merged['åºƒå‘Š_æœ‰ç„¡_lag2']
    merged['Interaction_price_åºƒå‘Š'] = merged['price'] * merged['åºƒå‘Š_æœ‰ç„¡_lag2']

    # ä½¿ã„ã‚„ã™ã„é †ã«åˆ—ã‚’æ•´ãˆã‚‹ï¼ˆä»»æ„ï¼‰
    merged = merged.reset_index(drop=True)
    return merged


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: numpy -> torch tensor å¤‰æ› ---
def to_tensor_Xy(X_np, y_np):
    X_t = torch.from_numpy(X_np.astype(np.float32))
    y_t = torch.from_numpy(y_np.astype(np.float32)).reshape(-1, 1)
    return X_t, y_t


# ===== ANNã®é–¢æ•° =====
def training_loop_ann(n_epochs, optimizer, scheduler, model, loss_fn, X_train, X_test, y_train, y_test):
    train_losses = []
    val_losses = []
    for epoch in range(1, n_epochs + 1):
        model.train()
        y_p_train = model(X_train)
        loss_train = loss_fn(y_p_train, y_train)

        model.eval()
        with torch.no_grad():
            y_p_test = model(X_test)
            loss_test = loss_fn(y_p_test, y_test)

        optimizer.zero_grad()
        loss_train.backward()
        optimizer.step()
        scheduler.step()

        train_losses.append(loss_train.item())
        val_losses.append(loss_test.item())

        # if epoch == 1 or epoch % 1000 == 0:
        #     print(f"Epoch {epoch:5d} | Training loss {loss_train.item():.4f} | Validation loss {loss_test.item():.4f}")

    return train_losses, val_losses


def train_model_ann(X_train, X_val, y_train, y_val, input_size, title):
    model = nn.Sequential(OrderedDict([
        ('hidden1', nn.Linear(input_size, 100)),
        ('bn1', nn.BatchNorm1d(100)),
        ('act1', nn.ReLU()),
        ('drop1', nn.Dropout(0.3)),
        ('hidden2', nn.Linear(100, 50)),
        ('bn2', nn.BatchNorm1d(50)),
        ('act2', nn.ReLU()),
        ('drop2', nn.Dropout(0.3)),
        ('hidden3', nn.Linear(50, 25)),
        ('bn3', nn.BatchNorm1d(25)),
        ('act3', nn.ReLU()),
        ('drop3', nn.Dropout(0.2)),
        ('output', nn.Linear(25, 1))
    ]))

    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)
    loss_fn = nn.MSELoss()

    train_losses, val_losses = training_loop_ann(
        n_epochs=5000,
        optimizer=optimizer,
        scheduler=scheduler,
        model=model,
        loss_fn=loss_fn,
        X_train=X_train,
        X_test=X_val,
        y_train=y_train,
        y_test=y_val
    )


    '''

    # ----- å­¦ç¿’æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ -----
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title(f'{title} - Training & Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.show()
    '''

    return model


def evaluate_model_ann(model, X_val, y_val, title, sales_max, sales_min):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val).squeeze().numpy()
        y_true = y_val.squeeze().numpy()


    y_true_orig = y_true * (sales_max - sales_min) + sales_min
    y_pred_orig = y_pred * (sales_max - sales_min) + sales_min
    mse = mean_squared_error(y_true_orig, y_pred_orig)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true_orig, y_pred_orig)
    r2 = r2_score(y_true_orig, y_pred_orig)
    ''' # è©•ä¾¡æŒ‡æ•°ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™

    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    '''

    # print(f"\n=== {title} ã®è©•ä¾¡çµæœ ===")
    # print(f"RMSEï¼ˆå¹³å‡äºŒä¹—èª¤å·®ã®å¹³æ–¹æ ¹ï¼‰: {rmse:.4f}")
    # print(f"MAEï¼ˆå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰: {mae:.4f}")
    # print(f"RÂ²ï¼ˆæ±ºå®šä¿‚æ•°ï¼‰: {r2:.4f}\n")


    '''
    # === 2. äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ ===
    plt.figure(figsize=(6, 6))
    plt.scatter(y_true, y_pred, alpha=0.6)
    min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='y=x')
    plt.title(f'{title}ï¼šäºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.legend()
    plt.grid(True)
    plt.show()
    '''

    return rmse, mae, r2

# --- ANN ã‚’ TimeSeriesSplit ã§ CV å®Ÿè¡Œ ---
def run_ann_cv(X, y, tscv, verbose=True):
    rmse_list, mae_list, r2_list = [], [], []
    input_size = X.shape[1]

    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        X_train_np, X_test_np = X[train_idx], X[test_idx]
        y_train_np, y_test_np = y[train_idx], y[test_idx]

        # torch ãƒ†ãƒ³ã‚½ãƒ«åŒ–
        X_train_t, y_train_t = to_tensor_Xy(X_train_np, y_train_np)
        X_val_t, y_val_t = to_tensor_Xy(X_test_np, y_test_np)

        # å­¦ç¿’ï¼ˆtrain_model_ann ã‚’åˆ©ç”¨ï¼‰
        title = f"ANN Fold {fold}"
        model_ann = train_model_ann(X_train_t, X_val_t, y_train_t, y_val_t, input_size=input_size, title=title)

        # è©•ä¾¡ï¼ˆy_scaler ã® min/max ã‚’æ¸¡ã™ï¼‰
        sales_max = y_scaler.data_max_[0]
        sales_min = y_scaler.data_min_[0]
        rmse, mae, r2 = evaluate_model_ann(model_ann, X_val_t, y_val_t, title, sales_max, sales_min)
        rmse_list.append(rmse); mae_list.append(mae); r2_list.append(r2)

        if verbose:
            print(f"[ANN] Fold {fold} -> RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}")

    print("\n=== ANN CV çµæœ ===")
    print(f"å¹³å‡ RMSE: {np.mean(rmse_list):.4f} (std {np.std(rmse_list):.4f})")
    print(f"å¹³å‡ MAE : {np.mean(mae_list):.4f}")
    print(f"å¹³å‡ R2  : {np.mean(r2_list):.4f}")
    return rmse_list, mae_list, r2_list





pd.set_option('display.max_columns', None)  # ã™ã¹ã¦ã®åˆ—ã‚’è¡¨ç¤º
pd.set_option('display.width', None)        # æ¨ªå¹…åˆ¶é™ã‚’è§£é™¤


retail = pd.read_excel('C:/Users/23X4002/Desktop/3å¹´ç”Ÿç§‹/PBL/éœ€è¦äºˆæ¸¬(åºƒå‘Š)/å°å£²ãƒ‡ãƒ¼ã‚¿.xlsx')
weather= pd.read_csv('C:/Users/23X4002/Desktop/3å¹´ç”Ÿç§‹/PBL/éœ€è¦äºˆæ¸¬(åºƒå‘Š)/æ°—è±¡ãƒ‡ãƒ¼ã‚¿03.csv',
                     encoding='cp932', skiprows=3)


data_mm = preprocess(retail, weather)





# ===== nameã®ç‰¹å¾´é‡ä½œæˆ =====
data_api = data_mm.copy()
# data_api["bert_vec"] = data_api["name"].apply(bert_features)
data_api["bert_vec"] = list(bert_features_batch(data_api["name"].tolist(), layer=8))






# ===== åˆ†æã®çµæœã‹ã‚‰bertã®463åˆ—ç›®ã®è¦ç´ ã«ã¤ã„ã¦å•†å“ã®ç‰¹å¾´ã‚’è€ƒå¯Ÿ =====
def analyze_bert_dimension(df: pd.DataFrame, dim_index: int, top_n: int = 10):
    """
    ç‰¹å®šã®BERTæ¬¡å…ƒã®å€¤ã«åŸºã¥ã„ã¦ã€å•†å“åï¼ˆnameï¼‰ã®Top Nã¨Bottom Nã‚’æŠ½å‡ºã—ã€è¡¨ç¤ºã™ã‚‹ã€‚

    Args:
        df (pd.DataFrame): 'name'åˆ—ã¨ 'bert_vec'åˆ—ã‚’å«ã‚€DataFrameã€‚
        dim_index (int): æŠ½å‡ºã—ãŸã„BERTãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰ã€‚
        top_n (int): è¡¨ç¤ºã™ã‚‹Top/Bottomã®æ•°ã€‚
    """
    dim_num = dim_index + 1
    dim_col_name = f'bert_vec_{dim_num}_val'

    # 1. BERTæ¬¡å…ƒã®å€¤ã‚’æŠ½å‡º
    try:
        # ãƒ™ã‚¯ãƒˆãƒ«ã®æŒ‡å®šã•ã‚ŒãŸæ¬¡å…ƒã‚’æ–°ã—ã„åˆ—ã¨ã—ã¦è¿½åŠ 
        df[dim_col_name] = df['bert_vec'].apply(lambda x: x[dim_index])
    except IndexError:
        print(f"ã‚¨ãƒ©ãƒ¼: BERTãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {dim_index} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    # 2. çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    print(f"\n--- {dim_col_name} ã®çµ±è¨ˆæƒ…å ± ---")
    print(df[dim_col_name].describe())

    # 3. ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå•†å“åãƒ™ãƒ¼ã‚¹ã§ã‚½ãƒ¼ãƒˆã—ã€Top/Bottom Nã‚’æŠ½å‡º
    df_unique_names = df.drop_duplicates(subset=['name'])

    # Top N
    top_names = (
        df_unique_names.sort_values(by=dim_col_name, ascending=False)
        .head(top_n)['name'].tolist()
    )
    # Bottom N
    bottom_names = (
        df_unique_names.sort_values(by=dim_col_name, ascending=True)
        .head(top_n)['name'].tolist()
    )

    # 4. çµæœå‡ºåŠ›
    print(f"\n====================================")
    print(f"ğŸ“ˆ {dim_col_name} ãŒæœ€ã‚‚é«˜ã„å•†å“å (Top {top_n})")
    print(f"====================================")
    for i, name in enumerate(top_names, 1):
        print(f"{i}. {name}")

    print(f"\n====================================")
    print(f"ğŸ“‰ {dim_col_name} ãŒæœ€ã‚‚ä½ã„å•†å“å (Bottom {top_n})")
    print(f"====================================")
    for i, name in enumerate(bottom_names, 1):
        print(f"{i}. {name}")

    # è€ƒå¯Ÿã®ãƒ’ãƒ³ãƒˆã¯ä»Šå›ã¯çœç•¥ (é–¢æ•°å†…ã§ã¯è‡ªå‹•å‡ºåŠ›ã—ãªã„)

    # å‡¦ç†å¾Œã«ä¸€æ™‚åˆ—ã‚’å‰Šé™¤ï¼ˆä»»æ„ï¼‰
    del df[dim_col_name]
analyze_bert_dimension(data_api.copy(), dim_index=462, top_n=5)
analyze_bert_dimension(data_api.copy(), dim_index=571, top_n=5)
analyze_bert_dimension(data_api.copy(), dim_index=463, top_n=5)





# ===== æ­£è¦åŒ– =====
y_scaler = MinMaxScaler()
y_scaler.fit(data_api[['å£²ä¸Šæ•°']])
# # --- ãƒ‡ãƒãƒƒã‚°æƒ…å ± (data_api ã‚’ä½¿ç”¨) ---
# print("\n--- ãƒ‡ãƒ¼ã‚¿ (data_api) ã®ãƒ˜ãƒƒãƒ€ãƒ¼ ---")
# print(data_api.head(3))
# print("\n--- ãƒ‡ãƒ¼ã‚¿ (data_api) ã®æ¦‚è¦ ---")
# print(data_api.describe())
# print("\n--- ãƒ‡ãƒ¼ã‚¿ (data_api) ã®æƒ…å ± ---")
# print(data_api.info())





# ===== ç‰¹å¾´é‡ã®é¸æŠ =====
# select_cols = ['å®¢æ•°', 'å£²ä¸Šæ•°_rollmean_prev3', 'åº—é ­åœ¨åº«æ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±', 'name_ã€2è¾›ã€‘ç´ æã‚’ç”Ÿã‹ã—ãŸã‚«ãƒ¬ãƒ¼ 6ä»£ç›®ãƒã‚¿ãƒ¼ãƒã‚­ãƒ³',
#                'name_ã€3è¾›ã€‘ç„™ç…ã‚¹ãƒ‘ã‚¤ã‚¹ã®ã”ã‚ã‚Šç‰›è‚‰ã‚«ãƒ¬ãƒ¼', 'æ—¥ç…§æ™‚é–“(æ™‚é–“)', 'name_ã€4è¾›ã€‘ç´ æã‚’ç”Ÿã‹ã—ãŸç‰›ã°ã‚‰è‚‰ã®å¤§ç››ã‚«ãƒ¬ãƒ¼', 'é™æ°´é‡ã®åˆè¨ˆ(mm)', 'name_ã€5è¾›ã€‘ç´ æã‚’ç”Ÿã‹ã—ãŸã‚«ãƒ¬ãƒ¼ ã‚°ãƒªãƒ¼ãƒ³',
#                'yday_sin', 'å¹³å‡æ°—æ¸©(â„ƒ)', 'å¹³å‡é¢¨é€Ÿ(m/s)', 'ç´å…¥äºˆå®šæ•°', 'yday_cos',
#                'åºƒå‘Š_æœ‰ç„¡_lag2']
exclude_cols = ['name', 'å£²ä¸Šæ•°', 'å£²ä¸Šé«˜','æ—¥ä»˜', 'åºƒå‘Š_æœ‰ç„¡','åºƒå‘Š_æœ‰ç„¡_lag1', 'åºƒå‘Š_æœ‰ç„¡_lag3',
                'Interaction_ç´å…¥äºˆå®šæ•°_åºƒå‘Š', 'Interaction_å®¢æ•°_åºƒå‘Š', 'Interaction_price_åºƒå‘Š', "bert_vec"]
target = 'å£²ä¸Šæ•°'
X_bert = np.vstack(data_api["bert_vec"].values)  # shape: (n_samples, 768)
X_num_df = data_api.drop(columns=exclude_cols, errors='ignore')
X_num = X_num_df.values
# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
y_unscaled = data_api[target].values.reshape(-1, 1) # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
y = y_scaler.transform(y_unscaled) # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

X = np.hstack([X_num, X_bert])
print(f"\næœ€çµ‚çš„ãªç‰¹å¾´é‡ X ã®å½¢çŠ¶: {X.shape}, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ y ã®å½¢çŠ¶: {y.shape}")
# X_num_df ã®åˆ—åã‚’ç¢ºèª (ãƒ‡ãƒãƒƒã‚°ç”¨)
# print("X_num ã®åˆ—:")
# print(X_num_df.columns.tolist())






# ===== ANNã®å®Ÿè£… =====
tscv = TimeSeriesSplit(n_splits=30)
print("Start ANN CV ...")
ann_rmse, ann_mae, ann_r2 = run_ann_cv(X, y, tscv=tscv)





# ===== PIã‚’åˆ©ç”¨ã—ã¦ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å¯è¦–åŒ– =====
# --- PFI ã®ãŸã‚ã®é–¢æ•° (scikit-learn ã® permutation_importance ã‚’åˆ©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ãŒã€ä»Šå›ã¯æ‰‹å‹•ã§å®Ÿè£…) ---

def calculate_permutation_importance(model, X_val_t, y_val_t, feature_names, sales_max, sales_min,
                                     loss_fn=mean_squared_error):
    model.eval()

    # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾— (ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—)
    with torch.no_grad():
        y_pred_base = model(X_val_t).squeeze().numpy()
        y_true = y_val_t.squeeze().numpy()

    # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã™
    y_true_orig = y_true * (sales_max - sales_min) + sales_min
    y_pred_base_orig = y_pred_base * (sales_max - sales_min) + sales_min
    base_score = loss_fn(y_true_orig, y_pred_base_orig)

    importance = {}
    X_val_np = X_val_t.numpy()

    # 2. å„ç‰¹å¾´é‡ã‚’é †ç•ªã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    for i, name in enumerate(feature_names):
        # å…ƒã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
        X_shuffled_np = X_val_np.copy()

        # iç•ªç›®ã®ç‰¹å¾´é‡ã ã‘ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        X_shuffled_np[:, i] = np.random.permutation(X_shuffled_np[:, i])

        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã§äºˆæ¸¬
        X_shuffled_t = torch.from_numpy(X_shuffled_np.astype(np.float32))

        with torch.no_grad():
            y_pred_shuffled = model(X_shuffled_t).squeeze().numpy()

        # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã—ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        y_pred_shuffled_orig = y_pred_shuffled * (sales_max - sales_min) + sales_min
        shuffled_score = loss_fn(y_true_orig, y_pred_shuffled_orig)

        # é‡è¦åº¦: (ã‚·ãƒ£ãƒƒãƒ•ãƒ«å¾Œã®ã‚¹ã‚³ã‚¢ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢) / ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢
        # RMSEãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—
        importance[name] = (np.sqrt(shuffled_score) - np.sqrt(base_score)) / np.sqrt(base_score)

    return importance, base_score


# --- PFI å®Ÿè¡Œéƒ¨åˆ† ---
# æœ€çµ‚çš„ãªç‰¹å¾´é‡ã®åˆ—åãƒªã‚¹ãƒˆã‚’ä½œæˆ
num_feature_names = X_num_df.columns.tolist()
# BERTãƒ™ã‚¯ãƒˆãƒ«ã¯768æ¬¡å…ƒã‚ã‚‹ãŸã‚ã€å€‹åˆ¥ã®åˆ—åã‚’ä»˜ã‘ã‚‹
bert_feature_names = [f"bert_vec_{i}" for i in range(X_bert.shape[1])]
all_feature_names = num_feature_names + bert_feature_names

# CV ã®æœ€çµ‚ Fold ã®ãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’ãƒ»è©•ä¾¡
final_train_idx, final_test_idx = list(tscv.split(X))[-1]
X_train_np, X_test_np = X[final_train_idx], X[final_test_idx]
y_train_np, y_test_np = y[final_train_idx], y[final_test_idx]

X_train_t, y_train_t = to_tensor_Xy(X_train_np, y_train_np)
X_val_t, y_val_t = to_tensor_Xy(X_test_np, y_test_np)

print("\n--- Final Model Training for PFI ---")
final_model = train_model_ann(X_train_t, X_val_t, y_train_t, y_val_t,
                              input_size=X.shape[1], title="Final Model")

sales_max = y_scaler.data_max_[0]
sales_min = y_scaler.data_min_[0]

# PFI ã®è¨ˆç®—ï¼ˆã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ MSE ã‚’ä½¿ç”¨ã€è©•ä¾¡é–¢æ•°ã«åˆã‚ã›ã¦ RMSE ã‚’è¨ˆç®—ï¼‰
importance_results, base_mse = calculate_permutation_importance(
    final_model,
    X_val_t,
    y_val_t,
    all_feature_names,
    sales_max,
    sales_min,
    loss_fn=mean_squared_error
)

# çµæœã‚’ DataFrame ã«ã—ã¦ã‚½ãƒ¼ãƒˆ
importance_df = pd.DataFrame(
    list(importance_results.items()),
    columns=['Feature', 'Importance (RMSE increase %)']
).sort_values(by='Importance (RMSE increase %)', ascending=False)

print("\n=== Permutation Feature Importance Top 30 ===")
print(importance_df.head(30))

# --- å¯è¦–åŒ– ---
plt.figure(figsize=(10, 6))
top_n = 20
df_plot = importance_df.head(top_n).sort_values(by='Importance (RMSE increase %)', ascending=True)
plt.barh(df_plot['Feature'], df_plot['Importance (RMSE increase %)'])
plt.title(f'Top {top_n} Feature Importance (Permutation)')
plt.xlabel('RMSE Increase Ratio')
plt.ylabel('Feature')
plt.grid(axis='x', linestyle='--')
plt.tight_layout()
plt.show()
