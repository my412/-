from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
from collections import OrderedDict
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import japanize_matplotlib
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
import lightgbm as lgb
from sklearn.preprocessing import StandardScaler
from typing import Dict, Any, Tuple
import re
import optuna
from optuna.integration import PyTorchLightningPruningCallback


# ---- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆä¸­é–“å±¤ã‚’è¿”ã™è¨­å®šï¼‰ ----
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased", output_hidden_states=True)


# --- å‰å‡¦ç†ã‚’é–¢æ•°åŒ– ---
def preprocess(retail_df, weather_df, correlation_threshold=0.9):
    # æ—¥ä»˜å‡¦ç†
    weather_df['æ—¥ä»˜'] = pd.to_datetime(weather_df['å¹´æœˆæ—¥.1'], errors='coerce')
    weather_df = weather_df.loc[:, ~weather_df.columns.str.contains(r'\.\d+$')]
    weather_df = weather_df.drop(columns=['æœ€æ·±ç©é›ª(cm)', 'å¹³å‡é›²é‡(10åˆ†æ¯”)', 'å¹´æœˆæ—¥'], errors='ignore')
    weather_df = weather_df.drop(index=[0, 1]).reset_index(drop=True)

    retail_df['æ—¥ä»˜'] = pd.to_datetime(retail_df['day'])
    retail_df = retail_df.drop(columns=['day'])

    merged = pd.merge(retail_df, weather_df, on='æ—¥ä»˜', how='left')

    # åºƒå‘Šçµ±åˆã¨ãƒã‚¤ãƒŠãƒªåŒ–
    ad_cols = ['SNS', 'å£²å ´æ–½ç­–', 'TVæ”¾æ˜ ', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³']
    for col in ad_cols:
        if col not in merged.columns:
            merged[col] = 0
    merged[ad_cols] = merged[ad_cols].fillna(0)
    for col in ad_cols:
        merged[col + '_æœ‰ç„¡'] = (merged[col] != 0).astype(int)
    merged['åºƒå‘Š_æœ‰ç„¡'] = merged[[c + '_æœ‰ç„¡' for c in ad_cols]].max(axis=1)
    merged = merged.drop(columns = ad_cols + [col + '_æœ‰ç„¡' for col in ad_cols])

    # ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆå•†å“ã”ã¨ï¼‰
    for lag in [1, 2, 3]:
        merged[f'åºƒå‘Š_æœ‰ç„¡_lag{lag}'] = merged.groupby('name')['åºƒå‘Š_æœ‰ç„¡'].shift(lag).fillna(0).astype(int)

    # ç§»å‹•å¹³å‡ï¼ˆ1ã¤å‰ã‹ã‚‰ã®3ç‚¹ç§»å‹•å¹³å‡ï¼‰
    merged['å£²ä¸Šæ•°_rollmean_prev3'] = (
        merged.groupby('name')['å£²ä¸Šæ•°']
              .apply(lambda s: s.shift(1).rolling(window=3, min_periods=1).mean())
              .reset_index(level=0, drop=True)
    ).fillna(0)

    # ç‰¹å®šã®æ¬ æå‡¦ç†ï¼ˆå…ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ„å›³ã‚’è¸è¥²ï¼‰
    merged = merged[merged.index != 3]  # å…ƒã‚³ãƒ¼ãƒ‰ã§é™¤å¤–ã—ã¦ã„ãŸè¡Œ
    beef_mask = (merged['name'] == 'ç„™ç…ç‰›è‚‰') & (merged.index >= 16) & (merged.index <= 94)
    for col in ['å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±']:
        if col in merged.columns:
            mean_value = merged.loc[beef_mask, col].mean()
            merged.loc[beef_mask, col] = merged.loc[beef_mask, col].fillna(mean_value)

    # å®¢æ•°ã®æ¬ æåŸ‹ã‚ï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®å€¤ã‚’å‚ç…§ï¼‰
    if 'å®¢æ•°' in merged.columns:
        try:
            costomer_count_250928 = merged.loc[merged['æ—¥ä»˜'] == pd.to_datetime('2025-09-28'), 'å®¢æ•°'].values[0]
            merged['å®¢æ•°'] = merged['å®¢æ•°'].fillna(costomer_count_250928)
        except Exception:
            merged['å®¢æ•°'] = merged['å®¢æ•°'].fillna(merged['å®¢æ•°'].mean())

    # æ•°å€¤åˆ—ã®å¹³å‡è£œå®Œ
    num_cols = ['price', 'å®¢æ•°', 'å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'åº—é ­åœ¨åº«æ•°', 'ç´å…¥äºˆå®šæ•°', 'å½“åº—åœ¨åº«æ‰‹æŒé€±',
                'å¹³å‡æ°—æ¸©(â„ƒ)', 'é™æ°´é‡ã®åˆè¨ˆ(mm)', 'æ—¥ç…§æ™‚é–“(æ™‚é–“)', 'å¹³å‡é¢¨é€Ÿ(m/s)', 'å¹³å‡è’¸æ°—åœ§(hPa)']
    num_cols = [c for c in num_cols if c in merged.columns]
    num_imputer = SimpleImputer(strategy='mean')
    merged[num_cols] = num_imputer.fit_transform(merged[num_cols])

    # IQRã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¤–ã‚Œå€¤å‡¦ç†ï¼‰
    for col in num_cols:
        Q1 = merged[col].quantile(0.25)
        Q3 = merged[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        merged[col] = merged[col].clip(lower, upper)

    # æ™‚ç³»åˆ—ç‰¹å¾´é‡
    SEASON = {1: "WINTER",2: "WINTER",3: "SPRING",4: "SPRING",5: "SPRING",
              6: "SUMMER",7: "SUMMER",8: "SUMMER",9: "FALL",10: "FALL",11: "FALL",12: "WINTER"}
    merged['æœˆ'] = merged['æ—¥ä»˜'].dt.month
    merged['é€±'] = merged['æ—¥ä»˜'].dt.isocalendar().week
    merged['å¹´'] = merged['æ—¥ä»˜'].dt.year
    merged['yday'] = merged['æ—¥ä»˜'].dt.dayofyear
    merged['season'] = merged['æœˆ'].map(SEASON)

    merged['yday_sin'] = np.sin(2 * np.pi * merged['yday'] / 365)
    merged['yday_cos'] = np.cos(2 * np.pi * merged['yday'] / 365)

    merged = merged.drop(columns=['æœˆ', 'é€±', 'å¹´', 'yday'])

    # season ã®ãƒ€ãƒŸãƒ¼åŒ–
    merged = pd.get_dummies(merged, columns=['season'], drop_first=True, dtype=int)


    ## ğŸš€ ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ (Interaction Features) ã®ç”Ÿæˆ
    lag_ad = 'åºƒå‘Š_æœ‰ç„¡_lag3'
    # 1. å½“åº—åœ¨åº«æ‰‹æŒé€± ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é …
    merged['Interaction_åœ¨åº«æ‰‹æŒé€±_åºƒå‘Š'] = merged['å½“åº—åœ¨åº«æ‰‹æŒé€±'] * merged[lag_ad]
    # 2. åº—é ­åœ¨åº«æ•° ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é …
    merged['Interaction_åº—é ­åœ¨åº«æ•°_åºƒå‘Š'] = merged['åº—é ­åœ¨åº«æ•°'] * merged[lag_ad]
    # 3. ç´å…¥äºˆå®šæ•° ã¨ åºƒå‘Šæœ‰ç„¡_lag2 ã®ç›¸äº’ä½œç”¨é … (Optional: ç›¸é–¢ãŒé«˜ã‹ã£ãŸãŸã‚è¿½åŠ )
    merged['Interaction_ç´å…¥äºˆå®šæ•°_åºƒå‘Š'] = merged['ç´å…¥äºˆå®šæ•°'] * merged[lag_ad]
    merged['Interaction_å®¢æ•°_åºƒå‘Š'] = merged['å®¢æ•°'] * merged[lag_ad]
    merged['Interaction_price_åºƒå‘Š'] = merged['price'] * merged[lag_ad]

    # ä½¿ã„ã‚„ã™ã„é †ã«åˆ—ã‚’æ•´ãˆã‚‹ï¼ˆä»»æ„ï¼‰
    merged = merged.reset_index(drop=True)

    # é«˜ç›¸é–¢ç‰¹å¾´é‡ã®è‡ªå‹•å‰Šé™¤
    print(f"\n--- ç›¸é–¢åˆ†æã«ã‚ˆã‚‹ç‰¹å¾´é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (é–¾å€¤: {correlation_threshold}) ã‚’é–‹å§‹ ---")

    # BERTç‰¹å¾´é‡ä»¥å¤–ã®æ•°å€¤åˆ—ã®ã¿ã‚’é¸æŠ (bert_vec_reducedã¯å¾Œã§çµåˆã•ã‚Œã‚‹ãŸã‚ã“ã“ã§ã¯é™¤å¤–)
    # name, æ—¥ä»˜, å£²ä¸Šé«˜, å£²ä¸Šæ•° ã‚’é™¤ãã€æœ€çµ‚çš„ãªæ•°å€¤ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
    final_num_cols = merged.select_dtypes(include=np.number).columns.tolist()

    # ç›®çš„å¤‰æ•°ã‚„IDçš„ãªåˆ—ã‚’é™¤å¤–
    cols_to_exclude = ['å£²ä¸Šæ•°', 'å£²ä¸Šé«˜', 'æ—¥ä»˜']
    feature_cols = [c for c in final_num_cols if c not in cols_to_exclude]

    corr_matrix = merged[feature_cols].corr().abs()

    # ä¸Šä¸‰è§’è¡Œåˆ—ã‚’å–å¾— (é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚)
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    # é–¾å€¤ã‚’è¶…ãˆã‚‹ç‰¹å¾´é‡ã‚’ç‰¹å®š
    cols_to_drop_corr = set()
    for col in upper.columns:
        # é–¾å€¤ã‚’è¶…ãˆã‚‹ç›¸é–¢ã‚’æŒã¤ç‰¹å¾´é‡ã‚’è¦‹ã¤ã‘ã‚‹
        high_corr_features = upper.index[upper[col] > correlation_threshold]

        # å‰Šé™¤ãƒªã‚¹ãƒˆã«è¿½åŠ  (ã“ã“ã§ã¯ã€ãƒšã‚¢ã®ã†ã¡å¾Œã«ã‚ã‚‹åˆ— 'col' ã‚’å„ªå…ˆçš„ã«å‰Šé™¤)
        # ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸæ–¹æ³•ã¨ã—ã¦ã¯ã€å¹³å‡PIãŒä½ã„æ–¹ã‚’å‰Šé™¤ã™ã¹ãã§ã™ãŒã€å‰å‡¦ç†ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ã€‚
        for feature_to_drop in high_corr_features:
            # ãƒšã‚¢ã®ã†ã¡ã€ã™ã§ã«å‰Šé™¤ãƒªã‚¹ãƒˆã«ãªã„æ–¹ã®ç‰¹å¾´é‡ã‚’é¸ã¶ (åˆ— 'col' ã‚’æ®‹ã™)
            # NOTE: 'col' ã‚’æ®‹ã—ã€'feature_to_drop' ã‚’å‰Šé™¤ãƒªã‚¹ãƒˆã«å…¥ã‚Œã‚‹
            if feature_to_drop not in cols_to_drop_corr:
                cols_to_drop_corr.add(feature_to_drop)

    cols_to_drop_corr = list(cols_to_drop_corr)

    if cols_to_drop_corr:
        merged = merged.drop(columns=cols_to_drop_corr, errors='ignore')
        print(f" é«˜ç›¸é–¢ ({correlation_threshold}ä»¥ä¸Š) ã®ãŸã‚ {len(cols_to_drop_corr)} å€‹ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤ã—ã¾ã—ãŸ:")
        print(cols_to_drop_corr)
    else:
        print(" å‰Šé™¤å¯¾è±¡ã¨ãªã‚‹é«˜ç›¸é–¢ãªç‰¹å¾´é‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    print(f"æœ€çµ‚çš„ãªç‰¹å¾´é‡å€™è£œæ•° (BERTç‰¹å¾´é‡ã‚’é™¤ã): {len(merged.columns) - 2} å€‹")  # name, æ—¥ä»˜ ã‚’é™¤ãæ¦‚ç®—
    return merged


# --- é€£ç¶šåºƒå‘Šæœ‰ç„¡ã®ç‰¹å¾´é‡ä½œæˆ ---
def add_continuous_ad_features(df):
    """
    'name' ã”ã¨ã« 'åºƒå‘Š_æœ‰ç„¡' ã®é€£ç¶šå›æ•°ã¨ã€ãã®ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹ã€‚
    """
    df_copy = df.copy()

    def calculate_consecutive_ad(series):
        # 1. é€£ç¶šã™ã‚‹å€¤ã®ãƒ–ãƒ­ãƒƒã‚¯IDã‚’ä½œæˆ
        # series.ne(series.shift()).cumsum() ã§ã€å€¤ãŒå¤‰ã‚ã‚‹ãŸã³ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå¢—ãˆã‚‹IDã‚’ä½œæˆ
        # ä¾‹: [1, 1, 0, 1, 1, 1, 0, 0, 1] -> [1, 1, 2, 3, 3, 3, 4, 4, 5]
        block_id = series.ne(series.shift()).cumsum()

        # 2. block_idã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€å„ãƒ–ãƒ­ãƒƒã‚¯å†…ã§ç´¯ç©ã‚«ã‚¦ãƒ³ãƒˆ (1ã‹ã‚‰é–‹å§‹)
        consecutive_count = series.groupby(block_id).cumcount() + 1

        # 3. åºƒå‘ŠãŒãªã„æœŸé–“ (å€¤ãŒ0ã®æœŸé–“) ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’0ã«ã™ã‚‹
        consecutive_count[series == 0] = 0

        return consecutive_count.astype(int)

    df_copy['åºƒå‘Š_æœ‰ç„¡_é€£ç¶š'] = (
        df_copy.groupby('name')['åºƒå‘Š_æœ‰ç„¡']
               .transform(calculate_consecutive_ad)
    )

    # 2. ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆ
    for lag in [1, 2, 3]:
        df_copy[f'åºƒå‘Š_æœ‰ç„¡_é€£ç¶š_lag{lag}'] = (
            df_copy.groupby('name')['åºƒå‘Š_æœ‰ç„¡_é€£ç¶š'].shift(lag).fillna(0).astype(int)
        )

    return df_copy


def bert_features(text, layer=8):
    # ãƒ†ã‚­ã‚¹ãƒˆ â†’ Tensor
    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)

    with torch.no_grad():
        outputs = model(**tokens)

    hidden_states = outputs.hidden_states  # å…¨å±¤ (13å€‹)

    # ---- ä½¿ã„ãŸã„å±¤ã‚’é¸æŠ ----
    h = hidden_states[layer]  # shape: (1, seq_len, 768)

    # ---- CLSãƒ™ã‚¯ãƒˆãƒ«ã‚’ç‰¹å¾´é‡ã«ã™ã‚‹ ----
    cls_vec = h[:, 0, :].squeeze().numpy()  # shape: (768,)

    return cls_vec


def bert_features_batch(text_list, layer=8, batch_size=16):

    all_vecs = []

    for i in range(0, len(text_list), batch_size):
        batch_texts = text_list[i:i+batch_size]

        # ã¾ã¨ã‚ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆpadding ã‚ã‚Šï¼‰
        tokens = tokenizer(batch_texts,
                           return_tensors='pt',
                           truncation=True,
                           padding=True,
                           max_length=128)

        with torch.no_grad():
            outputs = model(**tokens)

        hidden_states = outputs.hidden_states
        h = hidden_states[layer]       # shape: (B, seq_len, 768)
        mask = tokens["attention_mask"].unsqueeze(-1)

        # mean pooling
        h_masked = h * mask
        sum_vec = h_masked.sum(dim=1)
        len_tokens = mask.sum(dim=1)
        mean_vec = sum_vec / len_tokens  # shape: (B, 768)

        # numpy ã«å¤‰æ›
        all_vecs.append(mean_vec.numpy())

    # å…¨ãƒãƒƒãƒã‚’çµåˆ
    return np.vstack(all_vecs)


# --- BERTç‰¹å¾´é‡ã®æ¬¡å…ƒå‰Šæ¸›ï¼ˆè‡ªå·±ç¬¦å·åŒ–å™¨ï¼‰ ---
# 1. Autoencoderãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ (768 -> 512 -> 128)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, latent_dim)  # LATENT_DIM (ä½æ¬¡å…ƒã®ç‰¹å¾´é‡)
        )

        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ (128 -> 512 -> 768)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim)  # å…ƒã®æ¬¡å…ƒã«æˆ»ã™
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# 2. å­¦ç¿’ã¨æ¬¡å…ƒå‰Šæ¸›ã‚’è¡Œã†é–¢æ•°
def reduce_bert_dimension(df, input_dim=768, latent_dim=128, epochs=50, lr=1e-3):
    print("\n--- Autoencoderã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ã‚’é–‹å§‹ ---")

    bert_data_np = np.vstack(df["bert_vec"].values)

    scaler = StandardScaler()
    bert_scaled_np = scaler.fit_transform(bert_data_np)

    X_tensor = torch.from_numpy(bert_scaled_np).float()

    model = Autoencoder(input_dim, latent_dim)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    for epoch in range(epochs):
        model.train()

        # é †ä¼æ’­
        outputs = model(X_tensor)
        loss = criterion(outputs, X_tensor)  # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ X_tensor ã‚’å†ç¾ã§ãã‚‹ã‹

        # é€†ä¼æ’­ã¨æœ€é©åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}')

    model.eval()
    with torch.no_grad():
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼éƒ¨åˆ†ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡ã‚’æŠ½å‡º
        reduced_features_tensor = model.encoder(X_tensor)

    # Tensor -> NumPy
    reduced_features_np = reduced_features_tensor.numpy()

    df_reduced = df.copy()
    df_reduced['bert_vec_reduced'] = list(reduced_features_np)

    df_reduced = df_reduced.drop(columns=['bert_vec'])

    print(f"æ¬¡å…ƒå‰Šæ¸›å®Œäº†: {input_dim}æ¬¡å…ƒ -> {latent_dim}æ¬¡å…ƒ")
    return df_reduced


# --- ãƒ‡ãƒ¼ã‚¿ã®é¸æŠã€åˆ†å‰²ã€æ­£è¦åŒ– (ç›®çš„å¤‰æ•°ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œ) ---
def prepare_data_for_ann(df, target_col='å£²ä¸Šæ•°'):
    # ç›®çš„å¤‰æ•°ã®è¨­å®š
    y = df[target_col].values.reshape(-1, 1)  # MinMaxScalerã®ãŸã‚2æ¬¡å…ƒã«æ•´å½¢
    # y = np.log1p(y) # å¯¾æ•°å¤‰æ›

    # -----------------------------------------------------------
    # 1. ç‰¹å¾´é‡ã®é¸æŠã¨çµåˆ
    # -----------------------------------------------------------

    # é™¤å¤–ã™ã‚‹åˆ—
    drop_cols = ['name', 'æ—¥ä»˜', 'å£²ä¸Šé«˜', target_col]

    # BERTç‰¹å¾´é‡ã‚’ DataFrame ã®åˆ—ã«æˆ»ã™
    bert_reduced_df = pd.DataFrame(df['bert_vec_reduced'].tolist(),
                                   index=df.index,
                                   columns=[f'bert_reduced_{i}' for i in
                                            range(df['bert_vec_reduced'].iloc[0].shape[0])])

    # æœ€çµ‚çš„ãªç‰¹å¾´é‡ DataFrame ã®ä½œæˆ
    X_base = df.drop(columns=drop_cols + ['bert_vec_reduced'], errors='ignore')
    X = pd.concat([X_base, bert_reduced_df], axis=1)

    feature_names = X.columns.tolist()

    # -----------------------------------------------------------
    # 2. è¨“ç·´:æ¤œè¨¼:ãƒ†ã‚¹ãƒˆ = 8:1:1 ã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (å¤‰æ›´ãªã—)
    # -----------------------------------------------------------

    train_ratio = 0.8
    val_test_ratio = 0.2
    val_ratio_in_val_test = 0.5

    n_samples = len(X)
    n_train = int(n_samples * train_ratio)
    n_val_test = n_samples - n_train
    n_val = int(n_val_test * val_ratio_in_val_test)

    train_idx = X.index[:n_train]
    val_idx = X.index[n_train:n_train + n_val]
    test_idx = X.index[n_train + n_val:n_samples]

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²
    X_train, y_train = X.loc[train_idx], y[train_idx]
    X_val, y_val = X.loc[val_idx], y[val_idx]
    X_test, y_test = X.loc[test_idx], y[test_idx]

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_train)} ({len(X_train) / n_samples * 100:.1f}%)")

    # -----------------------------------------------------------
    # 3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ­£è¦åŒ–ï¼ˆç‰¹å¾´é‡:StandardScaler, ç›®çš„å¤‰æ•°:MinMaxScalerï¼‰
    # -----------------------------------------------------------

    # ç‰¹å¾´é‡ (X) ã®æ­£è¦åŒ–: StandardScaler
    X_scaler = StandardScaler()
    X_scaler.fit(X_train)
    X_train_scaled = X_scaler.transform(X_train)
    X_val_scaled = X_scaler.transform(X_val)
    X_test_scaled = X_scaler.transform(X_test)

    # ç›®çš„å¤‰æ•° (y) ã®æ­£è¦åŒ–: MinMaxScaler
    y_scaler = MinMaxScaler()
    y_scaler.fit(y_train)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ fit
    y_train_scaled = y_scaler.transform(y_train)
    y_val_scaled = y_scaler.transform(y_val)
    y_test_scaled = y_scaler.transform(y_test)

    print("ç‰¹å¾´é‡ãŠã‚ˆã³ç›®çš„å¤‰æ•°ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # -----------------------------------------------------------
    # 4. PyTorch Tensor ã¸ã®å¤‰æ›
    # -----------------------------------------------------------

    X_train_t, y_train_t = to_tensor_Xy(X_train_scaled, y_train_scaled.flatten())  # to_tensor_Xyã§å†åº¦æ•´å½¢ã•ã‚Œã‚‹ãŸã‚flatten()
    X_val_t, y_val_t = to_tensor_Xy(X_val_scaled, y_val_scaled.flatten())
    X_test_t, y_test_t = to_tensor_Xy(X_test_scaled, y_test_scaled.flatten())

    print(f"ANNãƒ¢ãƒ‡ãƒ«å…¥åŠ›æ¬¡å…ƒ: {X_train_t.shape[1]}")

    return {
        'X_train_t': X_train_t, 'y_train_t': y_train_t,
        'X_val_t': X_val_t, 'y_val_t': y_val_t,
        'X_test_t': X_test_t, 'y_test_t': y_test_t,
        'feature_names': feature_names,
        'y_scaler': y_scaler,
        'X_scaler': X_scaler,
        'train_idx': train_idx,
        'val_idx': val_idx,
        'test_idx': test_idx
    }


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: numpy -> torch tensor å¤‰æ› ---
def to_tensor_Xy(X_np, y_np):
    X_t = torch.from_numpy(X_np.astype(np.float32))
    y_t = torch.from_numpy(y_np.astype(np.float32)).reshape(-1, 1)
    return X_t, y_t


def prepare_data_for_ann_filtered(df, target_col='å£²ä¸Šæ•°', features_to_drop=[]):
    y = df[target_col].values.reshape(-1, 1)

    drop_cols_base = ['name', 'æ—¥ä»˜', 'å£²ä¸Šé«˜', target_col]

    # BERTç‰¹å¾´é‡ã‚’ DataFrame ã®åˆ—ã«æˆ»ã™
    bert_reduced_df = pd.DataFrame(df['bert_vec_reduced'].tolist(),
                                   index=df.index,
                                   columns=[f'bert_reduced_{i}' for i in
                                            range(df['bert_vec_reduced'].iloc[0].shape[0])])

    # æœ€çµ‚çš„ãªç‰¹å¾´é‡ DataFrame ã®ä½œæˆ
    X_base = df.drop(columns=drop_cols_base + ['bert_vec_reduced'], errors='ignore')
    X_combined = pd.concat([X_base, bert_reduced_df], axis=1)

    # PIã§ãƒã‚¤ãƒŠã‚¹ã ã£ãŸç‰¹å¾´é‡ã‚’å‰Šé™¤
    X = X_combined.drop(columns=features_to_drop, errors='ignore')

    feature_names = X.columns.tolist()

    # è¨“ç·´:æ¤œè¨¼:ãƒ†ã‚¹ãƒˆ åˆ†å‰²
    train_ratio = 0.8;
    val_test_ratio = 0.2;
    val_ratio_in_val_test = 0.5
    n_samples = len(X)
    n_train = int(n_samples * train_ratio)
    n_val_test = n_samples - n_train
    n_val = int(n_val_test * val_ratio_in_val_test)

    train_idx = X.index[:n_train];
    val_idx = X.index[n_train:n_train + n_val];
    test_idx = X.index[n_train + n_val:n_samples]
    X_train, y_train = X.loc[train_idx], y[train_idx]
    X_val, y_val = X.loc[val_idx], y[val_idx]
    X_test, y_test = X.loc[test_idx], y[test_idx]

    # ç‰¹å¾´é‡ (X) ã®æ­£è¦åŒ–: StandardScaler
    X_scaler = StandardScaler()
    X_scaler.fit(X_train)
    X_train_scaled = X_scaler.transform(X_train)
    X_val_scaled = X_scaler.transform(X_val)
    X_test_scaled = X_scaler.transform(X_test)

    # ç›®çš„å¤‰æ•° (y) ã®æ­£è¦åŒ–: MinMaxScaler
    y_scaler = MinMaxScaler()
    y_scaler.fit(y_train)
    y_train_scaled = y_scaler.transform(y_train)
    y_val_scaled = y_scaler.transform(y_val)
    y_test_scaled = y_scaler.transform(y_test)

    # PyTorch Tensor ã¸ã®å¤‰æ›
    X_train_t, y_train_t = to_tensor_Xy(X_train_scaled, y_train_scaled.flatten())
    X_val_t, y_val_t = to_tensor_Xy(X_val_scaled, y_val_scaled.flatten())
    X_test_t, y_test_t = to_tensor_Xy(X_test_scaled, y_test_scaled.flatten())

    return {
        'X_train_t': X_train_t, 'y_train_t': y_train_t, 'X_val_t': X_val_t, 'y_val_t': y_val_t,
        'X_test_t': X_test_t, 'y_test_t': y_test_t, 'feature_names': feature_names,
        'y_scaler': y_scaler, 'X_scaler': X_scaler, 'train_idx': train_idx, 'val_idx': val_idx, 'test_idx': test_idx
    }

# --- ANN ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨å­¦ç¿’å‡¦ç† ---
# ANNãƒ¢ãƒ‡ãƒ«ã®å†å®šç¾©ï¼ˆOptunaã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å…¥ã‚Œã‚‹ã‚ˆã†ã«ä¿®æ­£ï¼‰
class OptimizedANN(nn.Module):
    def __init__(self, input_dim, trial):
        super().__init__()

        # éš ã‚Œå±¤ã®æ•° (Optunaã§æ¢ç´¢)
        n_layers = trial.suggest_int('n_layers', 2, 5)

        layers = OrderedDict()
        input_size = input_dim

        for i in range(1, n_layers + 1):
            # ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã®æ¢ç´¢
            output_size = trial.suggest_int(f'n_units_l{i}', 32, 512, log=True)
            # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã®æ¢ç´¢
            dropout_rate = trial.suggest_float(f'dropout_l{i}', 0.1, 0.5)

            layers[f'hidden{i}'] = nn.Linear(input_size, output_size)
            layers[f'bn{i}'] = nn.BatchNorm1d(output_size)
            layers[f'act{i}'] = nn.ReLU()
            layers[f'drop{i}'] = nn.Dropout(dropout_rate)

            input_size = output_size  # æ¬¡ã®å±¤ã®å…¥åŠ›ã‚µã‚¤ã‚ºã‚’æ›´æ–°

        # æœ€çµ‚å‡ºåŠ›å±¤
        layers['output'] = nn.Linear(input_size, 1)

        self.net = nn.Sequential(layers)

    def forward(self, x):
        return self.net(x)

# Optunaã®ç›®çš„é–¢æ•°
'''def objective_ann(trial, prepared_data):
    # ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®å–ã‚Šå‡ºã—
    X_train_t = prepared_data['X_train_t']
    y_train_t = prepared_data['y_train_t']
    X_val_t = prepared_data['X_val_t']
    y_val_t = prepared_data['y_val_t']
    y_scaler = prepared_data['y_scaler']

    input_dim = X_train_t.shape[1]

    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ– (Optunaã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚’æ¸¡ã™)
    model = OptimizedANN(input_dim, trial)

    # å­¦ç¿’ç‡ã®æ¢ç´¢
    lr = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    epochs = 100  # Optunaã§ã¯çŸ­ã„ã‚¨ãƒãƒƒã‚¯æ•°ã«è¨­å®šï¼ˆæ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰

    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_t)
        loss = criterion(outputs, y_train_t)
        loss.backward()
        optimizer.step()

        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡ï¼ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã«ä½¿ç”¨ï¼‰
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_t)
            val_loss = criterion(val_outputs, y_val_t)


    # æœ€çµ‚è©•ä¾¡: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®RMSE (å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«)
    model.eval()
    with torch.no_grad():
        y_val_pred_scaled_t = model(X_val_t)

    # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã™
    y_val_pred = y_scaler.inverse_transform(y_val_pred_scaled_t.numpy())
    y_val_true = y_scaler.inverse_transform(y_val_t.numpy())

    # è©•ä¾¡æŒ‡æ¨™ (RMSEã‚’è¿”ã™)
    rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))

    return rmse
'''
def objective_ann(trial, prepared_data):
    # -----------------------------
    # ãƒ‡ãƒ¼ã‚¿å–ã‚Šå‡ºã—
    # -----------------------------
    X_train_t = prepared_data['X_train_t']
    y_train_t = prepared_data['y_train_t']
    X_val_t = prepared_data['X_val_t']
    y_val_t = prepared_data['y_val_t']
    y_scaler = prepared_data['y_scaler']

    input_dim = X_train_t.shape[1]

    # -----------------------------
    # ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆï¼ˆæ§‹é€ ã¯ Optuna ã«ä»»ã›ã‚‹ï¼‰
    # -----------------------------
    model = OptimizedANN(input_dim, trial)

    # -----------------------------
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    # -----------------------------
    lr = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # -----------------------------
    # æœ€å¤§ã‚¨ãƒãƒƒã‚¯ï¼ˆä¸Šé™ã ã‘æ±ºã‚ã‚‹ï¼‰
    # -----------------------------
    max_epochs = 1000

    # -----------------------------
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ— + Pruning
    # -----------------------------
    for epoch in range(max_epochs):
        # ---- train ----
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_t)
        loss = criterion(outputs, y_train_t)
        loss.backward()
        optimizer.step()

        # ---- validation ----
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_t)
            val_loss = criterion(val_outputs, y_val_t)

        # ---- Optuna ã«å ±å‘Š ----
        trial.report(val_loss.item(), step=epoch)

        # ---- Pruning åˆ¤å®š ----
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

    # -----------------------------
    # æœ€çµ‚è©•ä¾¡ï¼ˆRMSEï¼šå…ƒã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    # -----------------------------
    model.eval()
    with torch.no_grad():
        y_val_pred_scaled_t = model(X_val_t)

    y_val_pred = y_scaler.inverse_transform(
        y_val_pred_scaled_t.cpu().numpy()
    )
    y_val_true = y_scaler.inverse_transform(
        y_val_t.cpu().numpy()
    )

    rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))
    return rmse

# æœ€é©åŒ–ã•ã‚ŒãŸæ§‹é€ ã‚’æŒã¤ ANN ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©
class FinalOptimizedANN(nn.Module):
    def __init__(self, input_dim, params):
        super().__init__()

        n_layers = params['n_layers']
        layers = OrderedDict()
        input_size = input_dim

        for i in range(1, n_layers + 1):
            output_size = params[f'n_units_l{i}']
            dropout_rate = params[f'dropout_l{i}']

            layers[f'hidden{i}'] = nn.Linear(input_size, output_size)
            layers[f'bn{i}'] = nn.BatchNorm1d(output_size)
            layers[f'act{i}'] = nn.ReLU()
            layers[f'drop{i}'] = nn.Dropout(dropout_rate)

            input_size = output_size

        layers['output'] = nn.Linear(input_size, 1)
        self.net = nn.Sequential(layers)

    def forward(self, x):
        return self.net(x)
# æœ€çµ‚å­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆè©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
def final_train_and_evaluate(prepared_data, best_params, epochs, lr):
    # ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®å–ã‚Šå‡ºã—
    X_train_t = prepared_data['X_train_t']
    y_train_t = prepared_data['y_train_t']
    X_val_t = prepared_data['X_val_t']
    y_val_t = prepared_data['y_val_t']
    X_test_t = prepared_data['X_test_t']
    y_test_t = prepared_data['y_test_t']
    y_scaler = prepared_data['y_scaler']

    input_dim = X_train_t.shape[1]

    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ– (æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨)
    model = FinalOptimizedANN(input_dim, best_params)

    # æå¤±é–¢æ•° (MSE) ã¨æœ€é©åŒ–æ‰‹æ³• (Adam) ã®å®šç¾©
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # å­¦ç¿’ã¨æ¤œè¨¼ã®æå¤±ã‚’è¨˜éŒ²
    train_losses = []
    val_losses = []

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã€è¨“ç·´ã‚»ãƒƒãƒˆã‚’æ‹¡å¤§ã™ã‚‹æˆ¦ç•¥ã‚‚æœ‰åŠ¹ã§ã™ãŒã€
    # ã“ã“ã§ã¯æ—¢å­˜ã® train_val_test åˆ†å‰²ã‚’ç¶­æŒã—ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ç¢ºèªã—ã¾ã™ã€‚
    X_train_full = torch.cat([X_train_t, X_val_t], dim=0)
    y_train_full = torch.cat([y_train_t, y_val_t], dim=0)

    print("\n--- æœ€çµ‚ ANN ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’é–‹å§‹ ---")
    print(f"æ§‹é€ : {best_params['n_layers']}å±¤, å­¦ç¿’ç‡: {lr:.5f}, ã‚¨ãƒãƒƒã‚¯æ•°: {epochs}")

    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    for epoch in range(epochs):
        model.train()

        # é †ä¼æ’­
        outputs = model(X_train_full)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’åˆã‚ã›ãŸãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
        loss = criterion(outputs, y_train_full)

        # é€†ä¼æ’­ã¨æœ€é©åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_losses.append(loss.item())

        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡ï¼ˆéå­¦ç¿’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼‰
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_t)
            val_loss = criterion(val_outputs, y_val_t)
        val_losses.append(val_loss.item())

        if (epoch + 1) % 20 == 0 or epoch == 0:
            print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}')

    # 1. å­¦ç¿’éç¨‹ã®å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss (MSE)')
    plt.plot(val_losses, label='Validation Loss (MSE)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.title('Final ANN Model Learning Curve')
    plt.legend()
    plt.grid(True)
    plt.show()

    # 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    model.eval()
    with torch.no_grad():
        y_pred_scaled_t = model(X_test_t)

    # 3. ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã™ (Inverse Transform)
    y_pred_scaled_np = y_pred_scaled_t.numpy()
    y_test_scaled_np = y_test_t.numpy()

    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ã£ã¦äºˆæ¸¬å€¤ã¨å®Ÿæ¸¬å€¤ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
    y_pred = y_scaler.inverse_transform(y_pred_scaled_np)
    y_true = y_scaler.inverse_transform(y_test_scaled_np)

    # 4. è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®— (å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã§è©•ä¾¡)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print("\n--- æœ€çµ‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡ (å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«) ---")
    print(f"Mean Absolute Error (MAE): {mae:.2f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
    print(f"R-squared (R2): {r2:.4f}")

    return y_true, y_pred, model

# --- ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã®å®Ÿè¡Œ(PI) ---
def feature_importance_analysis_robust(model, prepared_data, y_true, y_pred, n_repeats=5):
    """
    Permutation Importance (PI) ã‚’è¤‡æ•°å›ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã“ã¨ã§ãƒ­ãƒã‚¹ãƒˆã«è¨ˆç®—ã™ã‚‹é–¢æ•°ã€‚

    Args:
        model (nn.Module): è¨“ç·´æ¸ˆã¿ã®ANNãƒ¢ãƒ‡ãƒ«ã€‚
        prepared_data (Dict): prepare_data_for_annã‹ã‚‰è¿”ã•ã‚ŒãŸè¾æ›¸ã€‚
        y_true (np.ndarray): ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿæ¸¬å€¤ï¼ˆå…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ã€‚
        y_pred (np.ndarray): ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬å€¤ï¼ˆå…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ã€‚
        n_repeats (int): ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚’ç¹°ã‚Šè¿”ã™å›æ•°ã€‚

    Returns:
        pd.DataFrame: ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆå¹³å‡ï¼‰ã‚’æ ¼ç´ã—ãŸDataFrameã€‚
    """
    X_test_t = prepared_data['X_test_t']
    feature_names = prepared_data['feature_names']
    y_scaler = prepared_data['y_scaler']

    # ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™é–¢æ•° (å¤‰æ›´ãªã—)
    def predict_unscaled(model, X_tensor):
        model.eval()
        with torch.no_grad():
            y_pred_scaled = model(X_tensor).numpy()
            y_pred_unscaled = y_scaler.inverse_transform(y_pred_scaled)
            return y_pred_unscaled.flatten()

    # æå¤±é–¢æ•° (MSE) (å¤‰æ›´ãªã—)
    def mse_loss(y_true, y_pred):
        return mean_squared_error(y_true, y_pred)

    X_test_np = X_test_t.numpy()

    print("\n--- 1. Permutation Importance (PI) ã®è¨ˆç®— ---")
    print(f"--- ã‚·ãƒ£ãƒƒãƒ•ãƒ«å›æ•°: {n_repeats} å› ---")

    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢ (ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—) ã¯ä¸€åº¦ã ã‘è¨ˆç®—
    baseline_pred = predict_unscaled(model, X_test_t)
    baseline_score = mse_loss(y_true, baseline_pred)
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ MSE (ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—): {baseline_score:.2f}")

    # PIã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã®DataFrame (ç‰¹å¾´é‡ Ã— ç¹°ã‚Šè¿”ã—å›æ•°)
    all_pi_scores = pd.DataFrame(index=feature_names)

    for repeat in range(n_repeats):
        current_pi_scores = {}

        # ç¹°ã‚Šè¿”ã—å›æ•°ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã—ã¦å†ç¾æ€§ã‚’é«˜ã‚ã‚‹
        np.random.seed(42 + repeat)

        for i, name in enumerate(feature_names):
            # iç•ªç›®ã®ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            X_test_shuffled = X_test_np.copy()
            # np.random.permutation ã§ i åˆ—ç›®ã®å€¤ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            X_test_shuffled[:, i] = np.random.permutation(X_test_shuffled[:, i])

            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«å¾Œã®äºˆæ¸¬ã¨æå¤±ã®è¨ˆç®—
            X_shuffled_t = torch.from_numpy(X_test_shuffled).float()
            shuffled_pred = predict_unscaled(model, X_shuffled_t)
            shuffled_score = mse_loss(y_true, shuffled_pred)

            # æå¤±ã®å¢—åŠ ã‚’è¨˜éŒ²
            current_pi_scores[name] = shuffled_score - baseline_score

        # ä»Šå›ã®è©¦è¡Œçµæœã‚’ DataFrame ã«è¿½åŠ 
        all_pi_scores[f'Repeat_{repeat + 1}'] = pd.Series(current_pi_scores)

    # å…¨ã¦ã®è©¦è¡Œã®å¹³å‡ã‚’æœ€çµ‚çš„ãªé‡è¦åº¦ã¨ã™ã‚‹
    pi_df = pd.DataFrame()
    pi_df['Importance (MSE Increase)'] = all_pi_scores.mean(axis=1)
    pi_df = pi_df.sort_values(by='Importance (MSE Increase)', ascending=False).reset_index()
    pi_df = pi_df.rename(columns={'index': 'Feature'})

    # å¯è¦–åŒ–
    plt.figure(figsize=(12, pi_df.shape[0] * 0.3))
    plt.barh(pi_df['Feature'], pi_df['Importance (MSE Increase)'])
    plt.xlabel(f'Average Importance (Increase in MSE after {n_repeats} repeats)')
    plt.title('Permutation Importance (Robust)')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

    print("\n--- ãƒ­ãƒã‚¹ãƒˆ PI è¨ˆç®—å®Œäº† ---")
    print(pi_df.head(10))

    return pi_df


def iterative_feature_selection_and_hpo_integrated_optimized(
        data_bert_reduced,
        prepared_data_initial,
        best_params_initial,
        epochs_final,
        final_lr,
        max_iterations=5,
        pi_repeats=100,
        n_trials_hpo=100
):
    """
    PIãŒã‚¼ãƒ­ä»¥ä¸‹ã®ç‰¹å¾´é‡ãŒãªããªã‚‹ã¾ã§ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨
    Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†æœ€é©åŒ–ã€ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´ã‚’ç¹°ã‚Šè¿”ã™ã€‚

    ã€æ”¹è‰¯ç‚¹ã€‘å…¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸­ã§æœ€ã‚‚ãƒ†ã‚¹ãƒˆRMSEãŒè‰¯ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨ã™ã‚‹ï¼ˆæ—©æœŸåœæ­¢ã®åŸå‰‡ï¼‰ã€‚
    """

    # Upliftåˆ†æã«å¿…è¦ãªç‰¹å¾´é‡ (ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«)
    GUARDRAIL_FEATURES = ['åºƒå‘Š_æœ‰ç„¡']

    all_features_dropped = []
    prepared_data_current = prepared_data_initial

    # åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ ¼ç´
    current_best_params = best_params_initial
    current_final_lr = final_lr

    # ğŸ† ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«è¿½è·¡ç”¨ã®å¤‰æ•°ã‚’åˆæœŸåŒ– ğŸ†
    best_rmse = float('inf')  # æœ€å°RMSEã‚’ç„¡é™å¤§ã§åˆæœŸåŒ–
    best_model = None
    best_feature_names = prepared_data_initial['feature_names']
    best_evaluation_results = None

    for iteration in range(max_iterations):
        print(f"\n====================== ğŸš€ ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {iteration + 1} ã‚’é–‹å§‹ ğŸš€ ======================")

        # ----------------------------------------------------
        # I. Optuna HPO ã®å†æœ€é©åŒ– (2å›ç›®ä»¥é™)
        # ----------------------------------------------------
        # ... (HPOã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—) ...
        if iteration > 0:
            print(
                f"--- 1. Optuna HPO ({n_trials_hpo} trials) ã‚’é–‹å§‹ (ç‰¹å¾´é‡æ•°: {len(prepared_data_current['feature_names'])}) ---")
            study_ann = optuna.create_study(direction='minimize', study_name=f'ann_hpo_iter_{iteration + 1}')
            study_ann.optimize(lambda trial: objective_ann(trial, prepared_data_current),
                               n_trials=n_trials_hpo,
                               show_progress_bar=True)

            current_best_params = study_ann.best_params
            current_final_lr = current_best_params['learning_rate']

            print(f"Optunaçµæœ: ãƒ™ã‚¹ãƒˆæ¤œè¨¼RMSE: {study_ann.best_value:.4f}, ãƒ™ã‚¹ãƒˆLR: {current_final_lr:.5f}")
            # ... (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤ºã¯çœç•¥)
        else:
            print(f"--- 1. åˆæœŸOptunaçµæœã‚’ä½¿ç”¨ (ç‰¹å¾´é‡æ•°: {len(prepared_data_current['feature_names'])}) ---")

        # ----------------------------------------------------
        # II. æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆè©•ä¾¡
        # ----------------------------------------------------
        print(f"--- 2. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ ({epochs_final} epochs) ã¨ PI è¨ˆç®— ---")

        y_true, y_pred, final_model_current = final_train_and_evaluate(  # å¤‰æ•°åã‚’å¤‰æ›´
            prepared_data=prepared_data_current,
            best_params=current_best_params,
            epochs=epochs_final,
            lr=current_final_lr
        )

        if final_model_current is None:
            print("ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸã€‚åå¾©ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break

        # ----------------------------------------------------
        # III. ğŸ† ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®šã¨æ›´æ–° ğŸ†
        # ----------------------------------------------------
        current_rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        print(f"ç¾åœ¨ã®ãƒ†ã‚¹ãƒˆè©•ä¾¡ RMSE: {current_rmse:.4f}")

        if current_rmse < best_rmse:
            print(f"  âœ¨ RMSEãŒ {best_rmse:.4f} ã‹ã‚‰ {current_rmse:.4f} ã«æ”¹å–„ï¼ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨å€™è£œã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚")
            best_rmse = current_rmse
            best_model = final_model_current
            best_feature_names = prepared_data_current['feature_names']
            best_evaluation_results = (y_true, y_pred)
        else:
            print(f"  âš ï¸ RMSEã¯æ”¹å–„ã—ã¾ã›ã‚“ã§ã—ãŸ (ç¾åœ¨ã®æœ€è‰¯ RMSE: {best_rmse:.4f})ã€‚")

        # ----------------------------------------------------
        # IV. Permutation Importance (PI) ã®è¨ˆç®—ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        # ----------------------------------------------------
        # PIã®è¨ˆç®—ã¯ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®åˆ¤æ–­ã®ãŸã‚ã«å¸¸ã«å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        pi_result_df = feature_importance_analysis_robust(
            model=final_model_current,
            prepared_data=prepared_data_current,
            y_true=y_true,
            y_pred=y_pred,
            n_repeats=pi_repeats
        )

        # ... (PIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«å‡¦ç†ã¯å¤‰æ›´ãªã—) ...

        # PIãŒã‚¼ãƒ­ä»¥ä¸‹ã®ç‰¹å¾´é‡ï¼ˆè‡ªå‹•å‰Šé™¤å€™è£œï¼‰
        negative_pi_features_auto = pi_result_df[
            pi_result_df['Importance (MSE Increase)'] <= 0
            ]['Feature'].tolist()

        # ==========  ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«å‡¦ç†ã®è¿½åŠ   ==========
        initial_num_negative = len(negative_pi_features_auto)

        features_to_drop = [
            f for f in negative_pi_features_auto if f not in GUARDRAIL_FEATURES
        ]

        num_negative = len(features_to_drop)

        if initial_num_negative > num_negative:
            print(
                f" ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«èµ·å‹•: {initial_num_negative - num_negative} å€‹ã®ç‰¹å¾´é‡ (åºƒå‘Š_æœ‰ç„¡ãªã©) ã‚’å‰Šé™¤ãƒªã‚¹ãƒˆã‹ã‚‰é™¤å¤–ã—ã¾ã—ãŸã€‚")
        # =================================================

        print(f"\n--- PI ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ (ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {iteration + 1}) ---")
        print(f"å‰Šé™¤å¯¾è±¡ã®ç‰¹å¾´é‡æ•°: {num_negative} å€‹")
        print([f for f in features_to_drop if f not in GUARDRAIL_FEATURES])

        # ----------------------------------------------------
        # V. çµ‚äº†æ¡ä»¶ã®ç¢ºèªã¨æ¬¡ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
        # ----------------------------------------------------
        if num_negative == 0 or iteration == max_iterations - 1:
            print("\nâœ… ç‰¹å¾´é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚åå¾©ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break

        # æ¬¡ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã«ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’æ›´æ–°
        all_features_dropped.extend(features_to_drop)
        all_features_dropped = list(set(all_features_dropped))  # é‡è¤‡å‰Šé™¤

        # æ–°ã—ã„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™
        prepared_data_current = prepare_data_for_ann_filtered(
            data_bert_reduced.copy(),
            target_col='å£²ä¸Šæ•°',
            features_to_drop=all_features_dropped
        )

    print("\n====================== âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ âœ… ======================")
    print("ğŸ† æœ€çµ‚çš„ãªæ¡ç”¨ãƒ¢ãƒ‡ãƒ«ã¯ã€å…¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸­ã§æœ€ã‚‚ç²¾åº¦ãŒè‰¯ã‹ã£ãŸã‚‚ã®ã§ã™ã€‚")
    print(f"æ¡ç”¨ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡æ•°: {len(best_feature_names)}")
    print(f"æœ€çµ‚æ¡ç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆè©•ä¾¡ RMSE: {best_rmse:.4f}")

    # æœ€çµ‚çš„ãªè¿”ã‚Šå€¤ã¯ã€æœ€ã‚‚ç²¾åº¦ã®è‰¯ã‹ã£ãŸæ™‚ç‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¨ç‰¹å¾´é‡ã¨ã™ã‚‹
    return best_feature_names, best_model, best_evaluation_results




# --- Uplift Modeling (åºƒå‘ŠåŠ¹æœ) ã®å¯è¦–åŒ– ---
# --- Individual Uplift Score ã®è¨ˆç®— ---
def calculate_individual_uplift(model, prepared_data):
    X_test_t = prepared_data['X_test_t']
    feature_names = prepared_data['feature_names']
    y_scaler = prepared_data['y_scaler']
    X_scaler = prepared_data['X_scaler']

    # 'åºƒå‘Š_æœ‰ç„¡' ã®ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    try:
        ad_col_index = feature_names.index('åºƒå‘Š_æœ‰ç„¡')
    except ValueError:
        print("ã‚¨ãƒ©ãƒ¼: 'åºƒå‘Š_æœ‰ç„¡' åˆ—ãŒç‰¹å¾´é‡ãƒªã‚¹ãƒˆã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Upliftè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None, None, None

    # Standard Scaler ã‚’ä½¿ç”¨ã—ã¦ã€å…ƒã®å€¤ 0 ã¨ 1 ãŒ scaled ç©ºé–“ã§ä½•ã«ãªã‚‹ã‹ã‚’è¨ˆç®—
    ad_mean = X_scaler.mean_[ad_col_index]
    ad_std = X_scaler.scale_[ad_col_index]

    scaled_ad_off = (0 - ad_mean) / ad_std
    scaled_ad_on = (1 - ad_mean) / ad_std

    print(f"\n--- S-Learner Individual Uplift è¨ˆç®— ---")
    print(f"'åºƒå‘Š_æœ‰ç„¡'=0 ã® Scaled å€¤: {scaled_ad_off:.4f}")
    print(f"'åºƒå‘Š_æœ‰ç„¡'=1 ã® Scaled å€¤: {scaled_ad_on:.4f}")

    # 2. MT (Treatment) ã¨ MC (Control) ã®äºˆæ¸¬
    model.eval()
    with torch.no_grad():
        # --- MC (Control: åºƒå‘Š_æœ‰ç„¡=0) ã®äºˆæ¸¬ ---
        X_C_t = X_test_t.clone()
        X_C_t[:, ad_col_index] = scaled_ad_off  # å…¨ã‚µãƒ³ãƒ—ãƒ«ã§ 'åºƒå‘Š_æœ‰ç„¡' ã‚’ 0 ã«å¼·åˆ¶
        M_C_scaled_t = model(X_C_t)

        # --- MT (Treatment: åºƒå‘Š_æœ‰ç„¡=1) ã®äºˆæ¸¬ ---
        X_T_t = X_test_t.clone()
        X_T_t[:, ad_col_index] = scaled_ad_on  # å…¨ã‚µãƒ³ãƒ—ãƒ«ã§ 'åºƒå‘Š_æœ‰ç„¡' ã‚’ 1 ã«å¼·åˆ¶
        M_T_scaled_t = model(X_T_t)

    # 3. Inverse Transform ã¨ Uplift è¨ˆç®—
    M_C = y_scaler.inverse_transform(M_C_scaled_t.numpy()).flatten()
    M_T = y_scaler.inverse_transform(M_T_scaled_t.numpy()).flatten()

    # Individual Uplift Score ã®è¨ˆç®—
    individual_uplift = M_T - M_C

    print(f"Individual Uplift Score ã®è¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å¹³å‡ Uplift: {individual_uplift.mean():.2f}")

    return M_C, M_T, individual_uplift

# --- Uplift Modeling (åºƒå‘ŠåŠ¹æœ) ã®å¯è¦–åŒ– (Individual Upliftå¯¾å¿œ) ---
def uplift_visualization(M_C, individual_uplift):
    print(f"\n--- Individual Uplift Visualization ã‚’é–‹å§‹ ---")

    # -----------------------------------------------------------
    # 1. äºˆæ¸¬ Uplift ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  (æ¨ªè»¸: Uplift, Uplift=0 ã«èµ¤ç·š)
    # -----------------------------------------------------------

    plt.figure(figsize=(10, 6))

    # Uplift ã®åˆ†å¸ƒ
    plt.hist(individual_uplift, bins=30, alpha=0.8, color='purple', label='Individual Uplift Score')

    # Uplift = 0 ã®ä½ç½®ã«èµ¤ã„ç·šã‚’è¿½åŠ 
    plt.axvline(0, color='red', linestyle='dashed', linewidth=2, label='Uplift = 0')

    plt.title('Individual Uplift Score ã®åˆ†å¸ƒ')
    plt.xlabel('Uplift (MT_pred - MC_pred)')
    plt.ylabel('åº¦æ•°')
    plt.legend()
    plt.grid(axis='y', linestyle='--')
    plt.show()

    # -----------------------------------------------------------
    # 2. æ•£å¸ƒå›³ (æ¨ªè»¸: M_C, ç¸¦è»¸: Uplift = M_T - M_C, 4è±¡é™)
    # -----------------------------------------------------------

    mc_avg = M_C.mean()

    plt.figure(figsize=(10, 6))

    # æ•£å¸ƒå›³ã®ãƒ—ãƒ­ãƒƒãƒˆ (å…¨ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«)
    plt.scatter(M_C, individual_uplift, alpha=0.6, s=30, color='blue')

    # Uplift = 0 ã®èµ¤ã„ç·šã‚’å¼•ã (æ¨ªç·š)
    plt.axhline(0, color='red', linestyle='-', linewidth=2, label='Uplift = 0')

    # MC_avg ã®ç‚¹ç·š (ç¸¦ç·š)
    plt.axvline(mc_avg, color='gray', linestyle='--', linewidth=1, label=f'MC_avg={mc_avg:.2f}')

    # 4è±¡é™ã®åç§°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¿½åŠ 
    y_max = individual_uplift.max() * 0.95
    y_min = individual_uplift.min() * 0.95

    # Persuadables (èª¬å¾—å¯èƒ½å±¤) - åºƒå‘Šã‚’æ‰“ã¤ã¹ãå±¤
    plt.text(M_C.min(), y_max, 'Persuadables',
             verticalalignment='top', horizontalalignment='left', color='green', fontsize=12, weight='bold')
    # Sure Things (å½“ç„¶è²·ã†å±¤) - åºƒå‘Šä¸è¦
    plt.text(mc_avg, y_max, 'Sure Things',
             verticalalignment='top', horizontalalignment='left', color='darkorange', fontsize=12, weight='bold')
    # Sleepers (ä¼‘çœ å±¤) - åºƒå‘Šä¸è¦
    plt.text(mc_avg, y_min, 'Sleepers',
             verticalalignment='bottom', horizontalalignment='left', color='purple', fontsize=12, weight='bold')
    # Lost Causes (è¦‹è¾¼ã¿è–„å±¤) - åºƒå‘Šç„¡é§„
    plt.text(M_C.min(), y_min, 'Lost Causes',
             verticalalignment='bottom', horizontalalignment='left', color='red', fontsize=12, weight='bold')

    plt.title('Individual Uplift Score vs Control Prediction ($M_C$ ã§ã®4è±¡é™)')
    plt.xlabel('äºˆæ¸¬å£²ä¸Š $M_C$ (åºƒå‘Šãªã—ã®å ´åˆ)')
    plt.ylabel('Individual Uplift ($M_T - M_C$)')
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.6)
    plt.show()


def analyze_uplift_segments(
        M_C: np.ndarray,
        individual_uplift: np.ndarray,
        prepared_data: Dict[str, Any],
        top_pi_features: list,
        uplift_threshold: float = 0.0,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    å€‹åˆ¥Upliftã‚¹ã‚³ã‚¢ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã—ã€ä¸»è¦ãªç‰¹å¾´é‡ã®å¹³å‡å€¤ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

    PIã§ä¸Šä½ã®ç‰¹å¾´é‡ã‚’ã‚­ãƒ¼ã¨ã—ã¦æ¯”è¼ƒã™ã‚‹ã€‚
    """

    print("\n--- Uplift ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ ç‰¹å¾´é‡åˆ†æã‚’é–‹å§‹ ---")

    # 1. ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒã¨çµåˆ (å¤‰æ›´ãªã—)
    X_test_scaled = prepared_data['X_test_t'].numpy()
    X_scaler = prepared_data['X_scaler']
    feature_names = prepared_data['feature_names']

    X_test_original = X_scaler.inverse_transform(X_test_scaled)
    df_results = pd.DataFrame(X_test_original, columns=feature_names)

    df_results['M_C'] = M_C
    df_results['Individual_Uplift'] = individual_uplift

    # 2. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å®šç¾©ã¨åˆ†é¡ (å¤‰æ›´ãªã—)
    PERSUADABLES_CONDITION = (df_results['Individual_Uplift'] > uplift_threshold) & \
                             (df_results['M_C'] <= M_C.mean())
    LOST_SLEEPERS_CONDITION = (df_results['Individual_Uplift'] <= 0)

    df_persuadables = df_results[PERSUADABLES_CONDITION].copy()
    df_lost_sleepers = df_results[LOST_SLEEPERS_CONDITION].copy()

    segment_counts = {
        'Persuadables': len(df_persuadables),
        'Sleepers/Lost Causes': len(df_lost_sleepers),
        'Overall Test Set': len(df_results)
    }
    df_segment_counts = pd.DataFrame(
        list(segment_counts.items()), columns=['Segment', 'Count']
    )
    print(df_segment_counts.to_markdown(index=False))

    if df_persuadables.empty or df_lost_sleepers.empty:
        print("ã‚¨ãƒ©ãƒ¼: æ¯”è¼ƒã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é–¾å€¤ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame(), df_segment_counts

    # ç‰¹å¾´é‡æ¯”è¼ƒã®æ§‹ç¯‰
    # PIä¸Šä½ç‰¹å¾´é‡ã«ã€åºƒå‘Šãƒ»ç›¸äº’ä½œç”¨ãƒ»é€£ç¶šåºƒå‘Šç‰¹å¾´é‡ã‚’çµ±åˆ
    # BERTç‰¹å¾´é‡ã¯å°‘ãªãã¨ã‚‚ä¸€ã¤å«ã‚ã‚‹ (ä¾‹: bert_reduced_0)
    essential_features = [
        'åºƒå‘Š_æœ‰ç„¡_é€£ç¶š',
        'Interaction_åº—é ­åœ¨åº«æ•°_åºƒå‘Š',
        'Interaction_å®¢æ•°_åºƒå‘Š',
        'bert_reduced_0'
    ]

    # PIä¸Šä½ç‰¹å¾´é‡ã¨å¿…é ˆç‰¹å¾´é‡ã‚’çµåˆã—ã€é‡è¤‡ã‚’å‰Šé™¤
    key_features = list(set(top_pi_features + essential_features))
    # 'åºƒå‘Š_æœ‰ç„¡' ã¯ Uplift è¨ˆç®—ã®åŸºã§ã‚ã‚‹ãŸã‚é™¤å¤–ã™ã‚‹ (çµæœã®è§£é‡ˆã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚)
    if 'åºƒå‘Š_æœ‰ç„¡' in key_features:
        key_features.remove('åºƒå‘Š_æœ‰ç„¡')

    key_features = [f for f in key_features if f in df_results.columns]

    print(f"\n--- æ¯”è¼ƒå¯¾è±¡ã®å‹•çš„ key_features ({len(key_features)}å€‹) ---")
    print(key_features)

    segments = {
        'Persuadables': df_persuadables,
        'Sleepers/Lost Causes': df_lost_sleepers,
        'Overall Test Set': df_results
    }

    comparison_data = []

    for feature in key_features:
        row = {'Feature': feature}
        for name, df in segments.items():
            mean_val = df[feature].mean()
            std_val = df[feature].std()

            # æ•°å€¤ã®æ•´å½¢ãƒ­ã‚¸ãƒƒã‚¯ (å¤‰æ›´ãªã—)
            if feature.startswith('bert_reduced'):
                mean_str = f"{mean_val:.3f}"
                std_str = f"({std_val:.3f})"
            elif feature in ['å½“åº—åœ¨åº«æ‰‹æŒé€±', 'å®¢æ•°', 'åºƒå‘Š_æœ‰ç„¡_é€£ç¶š']:
                mean_str = f"{mean_val:.1f}"
                std_str = f"({std_val:.1f})"
            else:
                mean_str = f"{mean_val:.0f}"
                std_str = f"({std_val:.0f})"

            row[f'{name} (Mean)'] = mean_str
            row[f'{name} (Std)'] = std_str
        comparison_data.append(row)

    df_comparison = pd.DataFrame(comparison_data)

    # å¯è¦–åŒ– (ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ )
    # BERTç‰¹å¾´é‡ã‚’é™¤å¤–ã—ãŸå¯è¦–åŒ–å¯¾è±¡ã®ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    features_to_plot = [
        f for f in key_features if not f.startswith('bert_reduced')
    ]

    print(f"\n--- ğŸ“Š éBERTç‰¹å¾´é‡ ({len(features_to_plot)}å€‹) ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  (å€‹åˆ¥å‡ºåŠ›) ---")

    if features_to_plot:

        for feature in features_to_plot:
            # ç‰¹å¾´é‡ã”ã¨ã«æ–°ã—ã„å›³ã‚’ä½œæˆ
            plt.figure(figsize=(10, 6), dpi=100)

            # Persuadables
            plt.hist(
                df_persuadables[feature].dropna(),
                bins=15,
                alpha=0.6,
                label=f'Persuadables (N={len(df_persuadables)})',
                color='green',
                density=True
            )

            # Sleepers/Lost Causes
            plt.hist(
                df_lost_sleepers[feature].dropna(),
                bins=15,
                alpha=0.6,
                label=f'Sleepers/Lost Causes (N={len(df_lost_sleepers)})',
                color='red',
                density=True
            )

            plt.title(f'åˆ†å¸ƒæ¯”è¼ƒ: {feature}', fontsize=16)
            plt.xlabel(feature, fontsize=14)
            plt.ylabel('å¯†åº¦', fontsize=14)

            # å‡¡ä¾‹ã‚’é©åˆ‡ã«é…ç½®
            plt.legend(
                loc='upper right',
                fontsize=10
            )
            plt.grid(axis='y', linestyle=':', alpha=0.5)

            # å›³ã‚’è¡¨ç¤º
            plt.tight_layout()
            plt.show()
    else:
        print("è­¦å‘Š: å¯è¦–åŒ–ã™ã‚‹éBERTç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    return df_comparison, df_segment_counts


def analyze_bert_dimension(df: pd.DataFrame, dim_index: int, top_n: int = 13):
    """
    ç‰¹å®šã®BERTæ¬¡å…ƒã®å€¤ã«åŸºã¥ã„ã¦ã€å•†å“åï¼ˆnameï¼‰ã®Top Nã¨Bottom Nã‚’æŠ½å‡ºã—ã€è¡¨ç¤ºã™ã‚‹ã€‚

    Args:
        df (pd.DataFrame): 'name'åˆ—ã¨ 'bert_vec'åˆ—ã‚’å«ã‚€DataFrameã€‚
        dim_index (int): æŠ½å‡ºã—ãŸã„BERTãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰ã€‚
        top_n (int): è¡¨ç¤ºã™ã‚‹Top/Bottomã®æ•°ã€‚
    """
    dim_num = dim_index
    dim_col_name = f'bert_vec_{dim_num}'  # å…ƒã®BERTãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒåã‚’å®šç¾©

    # 1. BERTæ¬¡å…ƒã®å€¤ã‚’æŠ½å‡º
    try:
        # ãƒ™ã‚¯ãƒˆãƒ«ã®æŒ‡å®šã•ã‚ŒãŸæ¬¡å…ƒã‚’æ–°ã—ã„åˆ—ã¨ã—ã¦è¿½åŠ 
        # ã“ã“ã§ã¯ã€å…ƒã®768æ¬¡å…ƒã®bert_vecã‹ã‚‰å€¤ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ã‚’æƒ³å®š
        df[dim_col_name] = df['bert_vec'].apply(lambda x: x[dim_index])
    except IndexError:
        print(f"ã‚¨ãƒ©ãƒ¼: BERTãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {dim_index} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    # 2. çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    print(f"\n--- {dim_col_name} ã®çµ±è¨ˆæƒ…å ± ---")
    print(df[dim_col_name].describe())

    # 3. ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå•†å“åãƒ™ãƒ¼ã‚¹ã§ã‚½ãƒ¼ãƒˆã—ã€Top/Bottom Nã‚’æŠ½å‡º
    # åŒã˜å•†å“åã«è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼ˆ=åŒã˜å•†å“ã ãŒç•°ãªã‚‹åº—èˆ—/æ—¥ä»˜ï¼‰ã‚’è€ƒæ…®ã—ã€
    # å„å•†å“åã«ã¤ã„ã¦ã€ãã®æ¬¡å…ƒã®å¹³å‡å€¤ã‚’ä½¿ç”¨ã—ã¦ä»£è¡¨å€¤ã¨ã™ã‚‹
    df_unique_names = df.groupby('name')[dim_col_name].mean().reset_index()

    # Top N
    top_names = (
        df_unique_names.sort_values(by=dim_col_name, ascending=False)
        .head(top_n)['name'].tolist()
    )

    # ä»£è¡¨çš„ãªå€¤ï¼ˆTop/Bottomã®å¹³å‡å€¤ï¼‰ã‚‚è¨ˆç®—
    top_mean_val = df_unique_names.sort_values(by=dim_col_name, ascending=False).head(top_n)[dim_col_name].mean()

    # 4. çµæœå‡ºåŠ›
    print(f"\n====================================")
    print(f"ğŸ“ˆ {dim_col_name} ãŒæœ€ã‚‚é«˜ã„å•†å“å (Top {top_n}, å¹³å‡å€¤: {top_mean_val:.4f})")
    print(f"====================================")
    for i, name in enumerate(top_names, 1):
        print(f"{i}. {name}")


def analyze_top_bert_features_automatically(
        pi_result_df: pd.DataFrame,
        full_data_df: pd.DataFrame,
        n_top: int = 2
):
    """
    PIçµæœã‹ã‚‰é‡è¦åº¦ä¸Šä½ã®BERTç‰¹å¾´é‡ã‚’è‡ªå‹•é¸æŠã—ã€analyze_bert_dimensioné–¢æ•°ã§åˆ†æã™ã‚‹ã€‚

    Args:
        pi_result_df (pd.DataFrame): Permutation Importanceã®çµæœDataFrame (Feature, Importance (MSE Increase)ã‚’å«ã‚€)ã€‚
        full_data_df (pd.DataFrame): 'name'åˆ—ã¨ 'bert_vec'åˆ—ã‚’å«ã‚€å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  (analyze_bert_dimensionç”¨)ã€‚
        n_top (int): åˆ†æã™ã‚‹ä¸Šä½BERTç‰¹å¾´é‡ã®æ•°ã€‚
    """
    print("\n======================  è‡ªå‹• BERT ç‰¹å¾´é‡åˆ†æã‚’é–‹å§‹  ======================")

    # 1. PIçµæœã‚’é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ—¢ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã¨ä»®å®šï¼‰
    df_sorted = pi_result_df.copy()

    # 2. BERTç‰¹å¾´é‡ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    bert_features = df_sorted[
        df_sorted['Feature'].str.startswith('bert_reduced_')
    ].head(n_top)

    if bert_features.empty:
        print("ã‚¨ãƒ©ãƒ¼: PIçµæœã« 'bert_reduced_' ã§å§‹ã¾ã‚‹ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    print(f"--- PIä¸Šä½ {n_top} ã® BERT ç‰¹å¾´é‡ ---")
    print(bert_features[['Feature', 'Importance (MSE Increase)']].to_markdown(index=False))

    # 3. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æŠ½å‡ºã¨åˆ†æã®å®Ÿè¡Œ
    for index, row in bert_features.iterrows():
        feature_name = row['Feature']
        # æ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡åã‹ã‚‰æ•°å­—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ã‚’æŠ½å‡º
        match = re.search(r'bert_reduced_(\d+)', feature_name)

        if match:
            # æŠ½å‡ºã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ã€å…ƒã®BERTãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨
            dim_index = int(match.group(1))

            print(f"\n--- ğŸ’¡ ç‰¹å¾´é‡: {feature_name} (å…ƒã®BERTæ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {dim_index}) ã®åˆ†æ ---")

            # analyze_bert_dimension é–¢æ•°ã‚’å®Ÿè¡Œ
            analyze_bert_dimension(full_data_df.copy(), dim_index=dim_index)
        else:
            print(f"è­¦å‘Š: ç‰¹å¾´é‡å {feature_name} ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    print("\n====================== âœ… è‡ªå‹• BERT ç‰¹å¾´é‡åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ âœ… ======================")

# ======================================================================================================================

# ===== ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ»å‰å‡¦ç†ãƒ»æ¬¡å…ƒå‰Šæ¸› =====
pd.set_option('display.max_columns', None)  # ã™ã¹ã¦ã®åˆ—ã‚’è¡¨ç¤º
pd.set_option('display.width', None)        # æ¨ªå¹…åˆ¶é™ã‚’è§£é™¤


retail = pd.read_excel('C:/Users/23X4002/Desktop/3å¹´ç”Ÿç§‹/PBL/éœ€è¦äºˆæ¸¬(åºƒå‘Š)/å°å£²ãƒ‡ãƒ¼ã‚¿.xlsx')
weather= pd.read_csv('C:/Users/23X4002/Desktop/3å¹´ç”Ÿç§‹/PBL/éœ€è¦äºˆæ¸¬(åºƒå‘Š)/æ°—è±¡ãƒ‡ãƒ¼ã‚¿03.csv',
                     encoding='cp932', skiprows=3)

data_mm = preprocess(retail, weather)
data_bert = add_continuous_ad_features(data_mm)
data_bert["bert_vec"] = list(bert_features_batch(data_bert["name"].tolist(), layer=8))
data_bert_reduced = reduce_bert_dimension(data_bert.copy(),
                                          input_dim=768,
                                          latent_dim=15,
                                          epochs=150)

print("\n--- å‰Šæ¸›å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­ ---")
print(data_bert_reduced[['name', 'bert_vec_reduced']].head())
print("\nå‰Šæ¸›å¾Œã®ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°:", data_bert_reduced['bert_vec_reduced'].iloc[0].shape)

prepared_data = prepare_data_for_ann(data_bert_reduced.copy(), target_col='å£²ä¸Šæ•°')
print("\n--- PyTorch Tensor ã®å½¢çŠ¶ ---")
print(f"è¨“ç·´ã‚»ãƒƒãƒˆ X: {prepared_data['X_train_t'].shape}, y: {prepared_data['y_train_t'].shape}")
print(f"æ¤œè¨¼ã‚»ãƒƒãƒˆ X: {prepared_data['X_val_t'].shape}, y: {prepared_data['y_val_t'].shape}")
print(f"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ X: {prepared_data['X_test_t'].shape}, y: {prepared_data['y_test_t'].shape}")
# ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆANNãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›æ¬¡å…ƒæ•°ã«å¯¾å¿œï¼‰
final_feature_names = prepared_data['feature_names']
print("-- é¸æŠã—ãŸç‰¹å¾´é‡ --\n",final_feature_names)


# ===== åˆæœŸ Optuna æœ€é©åŒ– (best_params_initial ã®å®šç¾©) =====
'''study_ann = optuna.create_study(direction='minimize')
N_TRIALS_INITIAL = 100

print("\n--- åˆæœŸ Optuna æœ€é©åŒ–ã‚’é–‹å§‹ (best_params_initial ã‚’å–å¾—) ---")
# ç›®çš„é–¢æ•°ã‚’å®Ÿè¡Œ
study_ann.optimize(lambda trial: objective_ann(trial, prepared_data),
                   n_trials=N_TRIALS_INITIAL,
                   show_progress_bar=True)'''
study_ann = optuna.create_study(
    direction="minimize",
    sampler=optuna.samplers.TPESampler(seed=42),
    pruner=optuna.pruners.MedianPruner(
        n_startup_trials=5,   # æœ€åˆã¯ prune ã—ãªã„
        n_warmup_steps=50     # æœ€ä½ 50 epoch ã¯è¦‹ã‚‹
    )
)
N_TRIALS_INITIAL = 100
study_ann.optimize(lambda trial: objective_ann(trial, prepared_data),
                   n_trials=N_TRIALS_INITIAL,
                   show_progress_bar=True)
best_trial = study_ann.best_trial
print("best epoch:", best_trial.last_step)

# --- best_params_initial ã¨ final_lr ã®å–å¾— ---
best_params_initial = study_ann.best_params
final_lr_initial = best_params_initial['learning_rate']

print("\n--- Optuna æœ€é©åŒ–çµæœ (åˆæœŸ) ---")
print(f"ãƒ™ã‚¹ãƒˆæ¤œè¨¼RMSE (å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«): {study_ann.best_value:.4f}")
print("ãƒ™ã‚¹ãƒˆãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(best_params_initial)



# ===== æœ€åˆã®è¨“ç·´ãƒ»è©•ä¾¡ï¼ˆPIè¨ˆç®—ã®ãŸã‚ã«å¿…é ˆ) =====
epochs_final = 250 # æœ€çµ‚çš„ãªå­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°

print(f"\n--- æœ€åˆã®æœ€çµ‚è¨“ç·´ã‚’å®Ÿè¡Œ (PIè¨ˆç®—ãŠã‚ˆã³åå¾©é–¢æ•°ã¸ã®å¼•æ¸¡ã—ã®ãŸã‚) ---")
y_true_initial, y_pred_initial, final_model_initial = final_train_and_evaluate(
    prepared_data,
    best_params_initial,
    epochs_final,
    final_lr_initial
)


# ===== åå¾©çš„ãªç‰¹å¾´é‡é¸æŠã¨HPOã®å®Ÿè¡Œ =====
print("\n======================  åå¾©çš„ãªæœ€é©åŒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é–‹å§‹  ======================")
# çµ±åˆé–¢æ•°ã‚’å®Ÿè¡Œ
final_features, final_model_iterative, (y_true_final, y_pred_final) = iterative_feature_selection_and_hpo_integrated_optimized(
    data_bert_reduced=data_bert_reduced,
    prepared_data_initial=prepared_data,       # åˆæœŸãƒ‡ãƒ¼ã‚¿æº–å‚™ã®çµæœ
    best_params_initial=best_params_initial,   # åˆæœŸOptunaã®çµæœ
    epochs_final=epochs_final,
    final_lr=final_lr_initial,                 # åˆæœŸã®å­¦ç¿’ç‡
    max_iterations=5,
    pi_repeats=20,
    n_trials_hpo=50  # åå¾©ä¸­ã®Optunaè©¦è¡Œå›æ•°
)

print("\n--- æœ€çµ‚çš„ãªæœ€é©åŒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ ---")
print(f"æœ€çµ‚ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ ({len(final_features)} å€‹): {final_features}")



# ===== ã‚¢ãƒƒãƒ—ãƒªãƒ•ãƒˆåˆ†æ (æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¨æœ€çµ‚çµæœã‚’ä½¿ç”¨) =====
# æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ï¼ˆã‚¢ãƒƒãƒ—ãƒªãƒ•ãƒˆåˆ†æç”¨ï¼‰
prepared_data_final = prepare_data_for_ann_filtered(
    data_bert_reduced.copy(),
    target_col='å£²ä¸Šæ•°',
    features_to_drop=[f for f in prepared_data['feature_names'] if f not in final_features]
)

# --- æœ€çµ‚PIè¨ˆç®— (ã‚¢ãƒƒãƒ—ãƒªãƒ•ãƒˆåˆ†æã®å‰ã«) ---
pi_result_df_final = feature_importance_analysis_robust(
    model=final_model_iterative,
    prepared_data=prepared_data_final,
    y_true=y_true_final,
    y_pred=y_pred_final,
    n_repeats=10
)
N_TOP_FEATURES = 20
top_pi_features = pi_result_df_final['Feature'].head(N_TOP_FEATURES).tolist()

# Uplift Visualizationã®å®Ÿè¡Œ
print("\n---  Uplift Analysisã‚’é–‹å§‹ ---")
M_C, M_T, individual_uplift = calculate_individual_uplift(final_model_iterative, prepared_data_final)

if M_C is not None:
    uplift_visualization(M_C, individual_uplift)

    df_comparison_result, df_counts = analyze_uplift_segments(
        M_C,
        individual_uplift,
        prepared_data_final,  # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
        top_pi_features=top_pi_features,
        uplift_threshold=0.0,
    )

    print(f"\n--- ä¸»è¦ãªç‰¹å¾´é‡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥æ¯”è¼ƒ (æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ) ---")
    print(df_comparison_result.to_markdown(index=False))


if 'pi_result_df_final' in locals() and 'data_bert' in locals():
    # 'data_bert' ã¯å…ƒã®768æ¬¡å…ƒã®bert_vecã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŒ‡ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    # æœ€çµ‚çš„ãªPIçµæœã¨å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  (name, bert_vecã‚’æŒã¤) ã‚’æ¸¡ã™ã€‚
    analyze_top_bert_features_automatically(
        pi_result_df=pi_result_df_final,
        full_data_df=data_bert, # analyze_bert_dimensionã¯å…ƒã® 'bert_vec' ãŒå¿…è¦
        n_top=2
    )
else:
    print("\n[è­¦å‘Š] analyze_top_bert_features_automaticallyã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚pi_result_df_final ã¾ãŸã¯ data_bert ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
